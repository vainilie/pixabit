-------- START OF FILE api/client.py --------
# pixabit/habitica/client.py

# SECTION: MODULE DOCSTRING
"""Defines the main HabiticaClient integrating all API functionalities via mixins."""

# SECTION: IMPORTS
from .exception import HabiticaAPIError  # Import Exception for potential use
from .habitica_api import HabiticaAPI
from .mixin.challenge_mixin import ChallengesMixin
from .mixin.message_mixin import MessageMixin
from .mixin.party_mixin import PartyMixin
from .mixin.tag_mixin import TagMixin
from .mixin.task_mixin import TasksMixin
from .mixin.user_mixin import UserMixin

# SECTION: CLIENT CLASS


# KLASS: HabiticaClient
class HabiticaClient(
    # Order influences Method Resolution Order (MRO), place base API first or last typically
    HabiticaAPI,  # Base API functionality
    ChallengesMixin,
    MessageMixin,
    PartyMixin,
    TagMixin,
    TasksMixin,
    UserMixin,
):
    """A full Habitica API Client combining base API access with specific endpoint methods.

    Inherits authentication, request logic, and rate limiting from HabiticaAPI,
    and specific API call methods (like get_tasks, get_user_data, etc.) from the Mixin classes.
    """

    # FUNC: __init__ (Optional)
    # Inherits __init__ from HabiticaAPI. Add a custom one if needed.
    # def __init__(self, *args, **kwargs):
    #     super().__init__(*args, **kwargs)
    #     # Add any client-specific initialization here
    #     log.info("HabiticaClient fully initialized.")

    # Methods from mixins and HabiticaAPI are directly available on instances
    # e.g., client = HabiticaClient(...)
    # await client.get_user_data()
    # await client.get_tasks()
    # await client.get_party_data()
    # ... etc ...

    pass  # No additional methods needed here currently
-e 
-------- END OF FILE api/client.py --------

-------- START OF FILE api/exception.py --------
# pixabit/habitica/exception.py

# SECTION: MODULE DOCSTRING
"""Defines a custom exception class for Habitica API errors."""

# SECTION: IMPORTS
from typing import Any

# SECTION: EXCEPTION CLASS


# KLASS: HabiticaAPIError
class HabiticaAPIError(Exception):
    """Custom exception for Habitica API errors with detailed information."""

    # FUNC: __init__
    def __init__(
        self,
        message: str,
        status_code: int | None = None,
        error_type: str | None = None,
        response_data: Any | None = None,
    ):
        """Initialize the API error with detailed context.

        Args:
            message: The main error message.
            status_code: The HTTP status code, if available.
            error_type: The Habitica-specific error type (e.g., 'NotFound'), if available.
            response_data: The raw response data (dict/list) from the API, if available.
        """
        super().__init__(message)
        self.status_code = status_code
        self.error_type = error_type
        self.response_data = response_data

    # FUNC: __str__
    def __str__(self) -> str:
        """Format the error message with available details."""
        details: list[str] = []
        if self.status_code is not None:
            details.append(f"Status={self.status_code}")
        if self.error_type:
            details.append(f"Type='{self.error_type}'")
        # Optionally add response data preview, be careful with large responses
        # if self.response_data:
        #     details.append(f"Data={str(self.response_data)[:50]}...")

        base_msg = super().__str__()
        details_str = f" ({', '.join(details)})" if details else ""
        return f"HabiticaAPIError: {base_msg}{details_str}"
-e 
-------- END OF FILE api/exception.py --------

-------- START OF FILE api/habitica_api.py --------
# pixabit/habitica/api.py

# SECTION: MODULE DOCSTRING
"""Habitica API client with async support and proper error handling."""

# SECTION: IMPORTS
from __future__ import annotations

import asyncio
import json
import time
from pathlib import Path
from typing import Any, Literal, TypeAlias, TypeVar, cast

import httpx
from pydantic import Field, SecretStr
from pydantic_settings import BaseSettings, SettingsConfigDict

from pixabit.api.exception import HabiticaAPIError

# Assuming logger helper is in helpers now
from pixabit.helpers._logger import log

# SECTION: TYPE ALIASES
HabiticaApiSuccessData: TypeAlias = dict[str, Any] | list[dict[str, Any]] | None
HabiticaApiResponsePayload: TypeAlias = dict[str, Any] | list[Any] | None

# SECTION: CONSTANTS
DEFAULT_BASE_URL: str = "https://habitica.com/api/v3/"
REQUESTS_PER_MINUTE: int = 29  # Habitica rate limit (30/min, use 29 for safety)
MIN_REQUEST_INTERVAL: float = 60.0 / REQUESTS_PER_MINUTE

# SECTION: CONFIGURATION MODEL


# KLASS: HabiticaConfig
class HabiticaConfig(BaseSettings):
    """Pydantic settings model for Habitica API configuration."""

    habitica_user_id: str = Field(..., description="Habitica User ID")
    habitica_api_token: SecretStr = Field(..., description="Habitica API Token")
    habitica_base_url: str = Field(DEFAULT_BASE_URL, description="Habitica API Base URL")

    model_config = SettingsConfigDict(
        env_file=".env",
        env_file_encoding="utf-8",
        case_sensitive=False,
        extra="ignore",
    )


# SECTION: API CLIENT CLASS


# KLASS: HabiticaAPI
class HabiticaAPI:
    """Asynchronous client for interacting with the Habitica API."""

    # FUNC: __init__
    def __init__(
        self,
        config: HabiticaConfig | None = None,
        user_id: str | None = None,
        api_token: str | None = None,
        base_url: str | None = None,
    ):
        """Initialize the API client with authentication and configuration.

        Args:
            config: Pydantic config object (preferred way to configure).
            user_id: Override user ID from config.
            api_token: Override API token from config.
            base_url: Override base URL from config.
        """
        log.debug("Initializing HabiticaAPI client...")
        # Load config from .env if not provided
        if config is None:
            try:
                config = HabiticaConfig()
                log.debug("Loaded HabiticaConfig from environment.")
            except Exception as e:
                log.error(f"Failed to load HabiticaConfig from environment: {e}")
                raise ValueError("Habitica configuration not provided and failed to load from .env") from e

        # Set credentials with priority to explicit parameters
        self.user_id = user_id or config.habitica_user_id
        self.api_token = api_token or config.habitica_api_token.get_secret_value()
        self.base_url = base_url or config.habitica_base_url

        # Validate credentials
        if not self.user_id or not self.api_token:
            log.error("Habitica User ID and API Token are required.")
            raise ValueError("Habitica User ID and API Token are required")

        # Set up request headers
        self.headers = {
            "x-api-user": self.user_id,
            "x-api-key": self.api_token,
            "Content-Type": "application/json",
        }
        log.debug(f"API Headers set for user: {self.user_id[:6]}...")

        # Rate limiting tracking
        self._last_request_time: float = 0.0
        self._request_interval: float = MIN_REQUEST_INTERVAL
        self._async_client: httpx.AsyncClient | None = None
        log.info("HabiticaAPI client initialized successfully.")

    # FUNC: get_async_client (Lazy initialization of httpx client)
    def get_async_client(self) -> httpx.AsyncClient:
        """Returns the httpx.AsyncClient instance, creating it if necessary."""
        if self._async_client is None or self._async_client.is_closed:
            log.debug("Creating new httpx.AsyncClient instance.")
            self._async_client = httpx.AsyncClient(headers=self.headers, base_url=self.base_url, timeout=120.0)  # Set base_url and headers here
        return self._async_client

    # FUNC: close (To close the client when done)
    async def close(self) -> None:
        """Closes the underlying httpx client."""
        if self._async_client and not self._async_client.is_closed:
            log.debug("Closing httpx.AsyncClient.")
            await self._async_client.aclose()
            self._async_client = None

    # FUNC: _wait_for_rate_limit
    async def _wait_for_rate_limit(self) -> None:
        """Enforce rate limiting by waiting if necessary."""
        current_time = time.monotonic()
        time_since_last = current_time - self._last_request_time

        if time_since_last < self._request_interval:
            wait_time = self._request_interval - time_since_last
            log.debug(f"Rate limit: waiting for {wait_time:.2f} seconds.")
            await asyncio.sleep(wait_time)

        self._last_request_time = time.monotonic()

    # FUNC: _request
    async def _request(self, method: str, endpoint: str, **kwargs: Any) -> HabiticaApiSuccessData:
        """Make an API request with proper error handling and rate limiting.

        Args:
            method: HTTP method (GET, POST, etc.).
            endpoint: API endpoint path (relative to base_url).
            **kwargs: Additional parameters for the httpx request (e.g., json, params).

        Returns:
            The 'data' part of a successful Habitica API response, or the full
            response body if the standard structure isn't found but the request
            was successful (status 2xx). Returns None for 204 No Content.

        Raises:
            HabiticaAPIError: For API-related errors (non-2xx status codes) or
                              Habitica explicit errors (success: false).
            ValueError: For data parsing issues (invalid JSON).
        """
        await self._wait_for_rate_limit()

        # Construct URL (httpx client handles base_url)
        relative_url = endpoint.lstrip("/")
        client = self.get_async_client()
        log.debug(f"Request: {method} {relative_url}, args: {kwargs}")

        try:
            response = await client.request(method, relative_url, **kwargs)
            log.debug(f"Response: {response.status_code} {response.reason_phrase}")
            # Check for HTTP errors first
            response.raise_for_status()

            # Handle empty responses (common for DELETE or success without data)
            if response.status_code == 204 or not response.content:
                log.debug("Received 204 No Content or empty body.")
                return None

            # Parse JSON response
            try:
                response_data = response.json()
            except json.JSONDecodeError as json_err:
                log.error(f"Invalid JSON received from {method} {relative_url}")
                raise ValueError(f"Invalid JSON received from {method} {relative_url}") from json_err

            # Handle standard Habitica V3 response format
            if isinstance(response_data, dict) and "success" in response_data:
                if response_data["success"]:
                    # Return the 'data' field if present, otherwise the whole dict?
                    # Habitica usually has 'data', but let's be safe.
                    return cast(HabiticaApiSuccessData, response_data.get("data"))
                else:
                    # API returned success: false
                    error_type = response_data.get("error", "Unknown Habitica Error")
                    message = response_data.get("message", "No message provided")
                    log.warning(f"Habitica API Error: {error_type} - {message}")
                    raise HabiticaAPIError(
                        f"{error_type} - {message}",
                        status_code=response.status_code,
                        error_type=error_type,
                        response_data=response_data,
                    )
            # Handle non-standard but valid JSON responses (e.g., /content)
            elif isinstance(response_data, (dict, list)):
                log.debug("Received non-standard JSON response (dict or list).")
                return cast(HabiticaApiSuccessData, response_data)

            # Handle unexpected response format
            else:
                log.error(f"Unexpected JSON response structure: {type(response_data).__name__}")
                raise ValueError(f"Unexpected response structure: {type(response_data).__name__}")

        except httpx.TimeoutException as err:
            log.error(f"Request timed out for {method} {relative_url}")
            raise HabiticaAPIError(
                f"Request timed out for {method} {relative_url}",
                status_code=408,
            ) from err

        except httpx.HTTPStatusError as err:
            response = err.response
            status_code = response.status_code
            error_type = f"HTTP{status_code}"
            message = f"HTTP {status_code} Error for {method} {relative_url}"
            response_data = None
            try:
                # Try to extract error details from JSON response
                err_data = response.json()
                if isinstance(err_data, dict):
                    response_data = err_data
                    error_type = err_data.get("error", error_type)
                    message = err_data.get("message", message)
                    log.warning(f"HTTP Error {status_code} with JSON body: {error_type} - {message}")
                else:
                    log.warning(f"HTTP Error {status_code} with non-dict JSON body: {err_data}")

            except json.JSONDecodeError:
                # Handle non-JSON error responses
                log.warning(f"HTTP Error {status_code} with non-JSON response body: {response.text}")
                message = f"HTTP {status_code} Error (non-JSON response)"

            raise HabiticaAPIError(
                message,
                status_code=status_code,
                error_type=error_type,
                response_data=response_data,
            ) from err

        except httpx.RequestError as err:
            # Network-related errors
            log.error(f"Network error for {method} {relative_url}: {err}")
            raise HabiticaAPIError(f"Network error for {method} {relative_url}: {err}") from err

        except Exception as err:
            # Catch any other unexpected errors
            log.exception(f"Unexpected error during request {method} {relative_url}: {err}")
            raise HabiticaAPIError(f"Unexpected error: {err}") from err

    # --- HTTP Method Helpers ---

    # FUNC: get
    async def get(
        self,
        endpoint: str,
        params: dict[str, Any] | None = None,
    ) -> HabiticaApiSuccessData:
        """Make a GET request to the Habitica API.

        Args:
            endpoint: API endpoint path.
            params: Optional query parameters.

        Returns:
            API response payload or None.
        """
        return await self._request("GET", endpoint, params=params)

    # FUNC: post
    async def post(
        self,
        endpoint: str,
        data: dict[str, Any] | None = None,
        params: dict[str, Any] | None = None,
    ) -> HabiticaApiSuccessData:
        """Make a POST request to the Habitica API.

        Args:
            endpoint: API endpoint path.
            data: Optional JSON body data.
            params: Optional query parameters.

        Returns:
            API response payload or None.
        """
        return await self._request("POST", endpoint, json=data, params=params)

    # FUNC: put
    async def put(
        self,
        endpoint: str,
        data: dict[str, Any] | None = None,
        params: dict[str, Any] | None = None,
    ) -> HabiticaApiSuccessData:
        """Make a PUT request to the Habitica API.

        Args:
            endpoint: API endpoint path.
            data: Optional JSON body data.
            params: Optional query parameters.

        Returns:
            API response payload or None.
        """
        return await self._request("PUT", endpoint, json=data, params=params)

    # FUNC: delete
    async def delete(self, endpoint: str, params: dict[str, Any] | None = None) -> HabiticaApiSuccessData:
        """Make a DELETE request to the Habitica API.

        Args:
            endpoint: API endpoint path.
            params: Optional query parameters.

        Returns:
            API response payload or None (often None for successful DELETE).
        """
        return await self._request("DELETE", endpoint, params=params)

    # --- Specific Endpoint Methods (Examples moved to Mixins/Client) ---
    # Example:
    # async def get_user_data(self) -> dict[str, Any] | None:
    #     """Get current user data."""
    #     result = await self.get("user")
    #     return cast(dict[str, Any], result) if isinstance(result, dict) else None

    async def get_content(self) -> dict[str, Any] | None:
        """Get the main game content object."""
        result = await self.get("/content")
        # Basic validation
        return cast(dict[str, Any], result) if isinstance(result, dict) else None
-e 
-------- END OF FILE api/habitica_api.py --------

-------- START OF FILE api/mixin/challenge_mixin.py --------
# pixabit/habitica/mixin/challenge_mixin.py

# SECTION: MODULE DOCSTRING
"""Mixin class providing Habitica Challenge related API methods."""

# SECTION: IMPORTS
import asyncio
from enum import Enum
from typing import TYPE_CHECKING, Any, Literal, cast

# Use TYPE_CHECKING to avoid circular import issues if API uses models
if TYPE_CHECKING:
    from pixabit.api.habitica_api import HabiticaAPI

# SECTION: ENUMS


# ENUM: TaskKeepOption
class TaskKeepOption(str, Enum):
    """Options for keeping tasks when unlinking."""

    KEEP = "keep"
    REMOVE = "remove"


# ENUM: ChallengeKeepOption
class ChallengeKeepOption(str, Enum):
    """Options for keeping challenge tasks when leaving."""

    KEEP_ALL = "keep-all"
    REMOVE_ALL = "remove-all"


# SECTION: MIXIN CLASS


# KLASS: ChallengesMixin
class ChallengesMixin:
    """Mixin containing methods for interacting with Habitica Challenges."""

    # Assert self is HabiticaAPI for type hinting internal methods like self.get
    if TYPE_CHECKING:
        _request: Callable[..., Coroutine[Any, Any, HabiticaApiSuccessData]]
        get: Callable[..., Coroutine[Any, Any, HabiticaApiSuccessData]]
        post: Callable[..., Coroutine[Any, Any, HabiticaApiSuccessData]]
        put: Callable[..., Coroutine[Any, Any, HabiticaApiSuccessData]]
        delete: Callable[..., Coroutine[Any, Any, HabiticaApiSuccessData]]

    # FUNC: get_challenges
    async def get_challenges(self, member_only: bool = True, page: int = 0) -> list[dict[str, Any]]:
        """Fetches a single page of user challenges.

        Args:
            member_only: If True, only return challenges the user is a member of.
            page: The page number to retrieve (0-indexed).

        Returns:
            A list of challenge dictionaries for the requested page, or empty list.
        """
        params: dict[str, Any] = {"page": page}
        # API uses string "true"/"false" for boolean params
        if member_only is True:
            params["member"] = "true"
            result = await self.get("/challenges/user", params=params)
        else:
            result = await self.get("/challenges/user", params=params)
        # Ensure result is a list, return empty list otherwise
        return cast(list[dict[str, Any]], result) if isinstance(result, list) else []

    # FUNC: get_all_challenges_paginated (Renamed for clarity)
    async def get_all_challenges_paginated(self, member_only: bool = True, page_delay: float = 0.5) -> list[dict[str, Any]]:
        """Fetches all challenges the user is associated with, handling pagination.

        Args:
            member_only: If True, only return challenges user is a member of.
            page_delay: Delay in seconds between fetching pages to respect rate limits.

        Returns:
            A list containing all challenge dictionaries across all pages.
        """
        all_challenges: list[dict[str, Any]] = []
        current_page = 0
        while True:
            page_data = await self.get_challenges(member_only=member_only, page=current_page)
            if not page_data:  # Empty list indicates end of pagination
                break
            all_challenges.extend(page_data)
            current_page += 1
            await asyncio.sleep(page_delay)  # Prevent hitting rate limits too quickly
        return all_challenges

    # FUNC: get_challenge_tasks
    async def get_challenge_tasks(self, challenge_id: str) -> list[dict[str, Any]]:
        """Fetches the tasks associated with a specific challenge.

        Args:
            challenge_id: The ID of the challenge.

        Returns:
            A list of task dictionaries within the challenge, or empty list.

        Raises:
            ValueError: If challenge_id is empty.
        """
        if not challenge_id:
            raise ValueError("challenge_id cannot be empty.")
        result = await self.get(f"/tasks/challenge/{challenge_id}")
        return cast(list[dict[str, Any]], result) if isinstance(result, list) else []

    # FUNC: join_challenge
    async def join_challenge(self, challenge_id: str) -> list[dict[str, Any]]:

        if not challenge_id:
            raise ValueError("challenge_id cannot be empty.")
        result = await self.post(f"/challenges/{challenge_id}/join")
        return cast(dict[str, Any], result) if isinstance(result, dict) else {}

    # FUNC: leave_challenge
    async def leave_challenge(
        self,
        challenge_id: str,
        keep: ChallengeKeepOption | Literal["keep-all", "remove-all"] = ChallengeKeepOption.KEEP_ALL,
    ) -> bool:
        """Leaves a specific challenge.

        Args:
            challenge_id: The ID of the challenge to leave.
            keep: Option whether to keep tasks ('keep-all', 'remove-all', or enum).

        Returns:
            True if the operation was successful (API returned no data), False otherwise.

        Raises:
            ValueError: If challenge_id is empty or keep option is invalid.
        """
        if not challenge_id:
            raise ValueError("challenge_id cannot be empty.")

        keep_value = keep.value if isinstance(keep, Enum) else keep
        if keep_value not in ["keep-all", "remove-all"]:
            raise ValueError("keep must be 'keep-all' or 'remove-all'")

        result = await self.post(f"/challenges/{challenge_id}/leave", params={"keep": keep_value})
        # Successful leave often returns 204 No Content (result is None)
        return result is None

    # FUNC: unlink_task_from_challenge
    async def unlink_task_from_challenge(
        self,
        task_id: str,
        keep: TaskKeepOption | Literal["keep", "remove"] = TaskKeepOption.KEEP,
    ) -> bool:
        """Unlinks a specific task from its challenge.

        Args:
            task_id: The ID of the task to unlink.
            keep: Option whether to keep the task as a personal task ('keep', 'remove', or enum).

        Returns:
            True if the operation was successful (API returned no data), False otherwise.

        Raises:
            ValueError: If task_id is empty or keep option is invalid.
        """
        if not task_id:
            raise ValueError("task_id cannot be empty.")

        keep_value = keep.value if isinstance(keep, Enum) else keep
        if keep_value not in ["keep", "remove"]:
            raise ValueError("keep must be 'keep' or 'remove'")

        result = await self.post(f"/tasks/unlink-one/{task_id}", params={"keep": keep_value})
        return result is None

    # FUNC: unlink_all_challenge_tasks
    async def unlink_all_challenge_tasks(
        self,
        challenge_id: str,
        keep: ChallengeKeepOption | Literal["keep-all", "remove-all"] = ChallengeKeepOption.KEEP_ALL,
    ) -> bool:
        """Unlinks all tasks belonging to a specific challenge.

        Args:
            challenge_id: The ID of the challenge whose tasks should be unlinked.
            keep: Option whether to keep tasks ('keep-all', 'remove-all', or enum).

        Returns:
            True if the operation was successful (API returned no data), False otherwise.

        Raises:
            ValueError: If challenge_id is empty or keep option is invalid.
        """
        if not challenge_id:
            raise ValueError("challenge_id cannot be empty.")

        keep_value = keep.value if isinstance(keep, Enum) else keep
        if keep_value not in ["keep-all", "remove-all"]:
            raise ValueError("keep must be 'keep-all' or 'remove-all'")

        result = await self.post(f"/tasks/unlink-all/{challenge_id}", params={"keep": keep_value})
        return result is None

    # FUNC: create_challenge
    async def create_challenge(self, data: dict[str, Any]) -> dict[str, Any] | None:
        """Creates a new challenge.

        Args:
            data: A dictionary containing challenge data (requires 'name', 'shortName', 'group').

        Returns:
            A dictionary representing the newly created challenge, or None on failure.

        Raises:
            ValueError: If required fields ('name', 'shortName', 'group') are missing.
        """
        if not data.get("name") or not data.get("shortName") or not data.get("group"):  # Group ID is required
            raise ValueError("Challenge creation requires at least 'name', 'shortName', and 'group' ID.")
        result = await self.post("/challenges", data=data)
        return cast(dict[str, Any], result) if isinstance(result, dict) else None

    # FUNC: clone_challenge
    async def clone_challenge(self, challenge_id: str) -> dict[str, Any] | None:
        """Clones an existing challenge.

        Args:
            challenge_id: The ID of the challenge to clone.

        Returns:
            A dictionary representing the cloned challenge, or None on failure.

        Raises:
            ValueError: If challenge_id is empty.
        """
        if not challenge_id:
            raise ValueError("challenge_id cannot be empty.")
        result = await self.post(f"/challenges/{challenge_id}/clone")
        return cast(dict[str, Any], result) if isinstance(result, dict) else None

    # FUNC: update_challenge
    async def update_challenge(self, challenge_id: str, data: dict[str, Any]) -> dict[str, Any] | None:
        """Updates an existing challenge.

        Args:
            challenge_id: The ID of the challenge to update.
            data: A dictionary containing the fields to update.

        Returns:
            A dictionary representing the updated challenge, or None on failure.

        Raises:
            ValueError: If challenge_id or data is empty.
        """
        if not challenge_id:
            raise ValueError("challenge_id cannot be empty.")
        if not data:
            raise ValueError("Update data cannot be empty.")
        result = await self.put(f"/challenges/{challenge_id}", data=data)
        return cast(dict[str, Any], result) if isinstance(result, dict) else None

    # FUNC: create_challenge_task
    async def create_challenge_task(self, challenge_id: str, task_data: dict[str, Any]) -> dict[str, Any] | None:
        """Creates a new task within a specific challenge.

        Args:
            challenge_id: The ID of the challenge to add the task to.
            task_data: A dictionary containing task data (requires 'text', 'type').

        Returns:
            A dictionary representing the newly created task, or None on failure.

        Raises:
            ValueError: If challenge_id is empty, required task fields are missing, or type is invalid.
        """
        if not challenge_id:
            raise ValueError("challenge_id cannot be empty.")
        if not task_data.get("text") or not task_data.get("type"):
            raise ValueError("Task data requires 'text' and 'type'.")
        if task_data["type"] not in {"habit", "daily", "todo", "reward"}:
            raise ValueError("Invalid task type. Must be 'habit', 'daily', 'todo', or 'reward'.")

        result = await self.post(f"/tasks/challenge/{challenge_id}", data=task_data)
        return cast(dict[str, Any], result) if isinstance(result, dict) else None
-e 
-------- END OF FILE api/mixin/challenge_mixin.py --------

-------- START OF FILE api/mixin/message_mixin.py --------
# pixabit/habitica/mixin/message_mixin.py

# SECTION: MODULE DOCSTRING
"""Mixin class providing Habitica Messaging (Inbox/PM) related API methods."""

# SECTION: IMPORTS
from __future__ import annotations

from typing import TYPE_CHECKING, Any, cast

# Use TYPE_CHECKING to avoid circular import issues if API uses models
if TYPE_CHECKING:
    from collections.abc import Callable, Coroutine  # For hinting self methods

    from pixabit.api.habitica_api import HabiticaAPI, HabiticaApiSuccessData

# SECTION: MIXIN CLASS


# KLASS: MessageMixin
class MessageMixin:
    """Mixin containing methods for interacting with Habitica Messages."""

    # Assert self is HabiticaAPI for type hinting internal methods
    if TYPE_CHECKING:
        _request: Callable[..., Coroutine[Any, Any, HabiticaApiSuccessData]]
        get: Callable[..., Coroutine[Any, Any, HabiticaApiSuccessData]]
        post: Callable[..., Coroutine[Any, Any, HabiticaApiSuccessData]]
        put: Callable[..., Coroutine[Any, Any, HabiticaApiSuccessData]]
        delete: Callable[..., Coroutine[Any, Any, HabiticaApiSuccessData]]

    # FUNC: get_inbox_messages
    async def get_inbox_messages(self, page: int = 0, conversation_id: str | None = None) -> list[dict[str, Any]]:
        """Fetches inbox messages, optionally filtered by conversation.

        Args:
            page: The page number to retrieve (0-indexed).
            conversation_id: Optional UUID of the other user in the conversation.

        Returns:
            A list of message dictionaries, or an empty list.
        """
        params: dict[str, Any] = {"page": page}
        if conversation_id:
            # API parameter is 'conversation' according to docs
            params["conversation"] = conversation_id
        result = await self.get("/inbox/messages", params=params)
        return cast(list[dict[str, Any]], result) if isinstance(result, list) else []

    # FUNC: send_private_message
    async def send_private_message(self, recipient_id: str, message_text: str) -> dict[str, Any] | None:
        """Sends a private message to another user.

        Args:
            recipient_id: The UUID of the recipient user.
            message_text: The content of the message.

        Returns:
            A dictionary representing the sent message confirmation, or None on failure.

        Raises:
            ValueError: If recipient_id or message_text is empty.
        """
        if not recipient_id:
            raise ValueError("recipient_id is required.")
        message_text_stripped = message_text.strip()
        if not message_text_stripped:
            raise ValueError("message_text cannot be empty.")

        payload = {"toUserId": recipient_id, "message": message_text_stripped}
        # Endpoint is documented as /members/send-private-message
        result = await self.post("/members/send-private-message", data=payload)
        return cast(dict[str, Any], result) if isinstance(result, dict) else None

    # FUNC: mark_pms_read
    async def mark_pms_read(self) -> bool:
        """Marks all private messages as read for the current user.

        Returns:
            True if the operation was successful (API returned no data), False otherwise.
        """
        # Endpoint is /user/mark-pms-read
        result = await self.post("/user/mark-pms-read")
        return result is None

    # FUNC: delete_private_message
    async def delete_private_message(self, message_id: str) -> bool:
        """Deletes a specific private message.

        Args:
            message_id: The ID (_id) of the message to delete.

        Returns:
            True if the operation was successful (API returned no data), False otherwise.

        Raises:
            ValueError: If message_id is empty.
        """
        if not message_id:
            raise ValueError("message_id is required.")
        # Endpoint includes user prefix based on some API patterns, double-check docs if issues arise
        result = await self.delete(f"/user/messages/{message_id}")
        return result is None
-e 
-------- END OF FILE api/mixin/message_mixin.py --------

-------- START OF FILE api/mixin/party_mixin.py --------
# pixabit/habitica/mixin/party_mixin.py

# SECTION: MODULE DOCSTRING
"""Mixin class providing Habitica Party and Group related API methods."""

# SECTION: IMPORTS
from __future__ import annotations

from typing import TYPE_CHECKING, Any, cast

from pixabit.api.exception import HabiticaAPIError

# Assuming logger helper is in helpers
from pixabit.helpers._logger import log

# Use TYPE_CHECKING to avoid circular import issues if API uses models
if TYPE_CHECKING:
    from collections.abc import Callable, Coroutine  # For hinting self methods

    from pixabit.api.habitica_api import HabiticaAPI, HabiticaApiSuccessData

# SECTION: MIXIN CLASS


# KLASS: PartyMixin
class PartyMixin:
    """Mixin containing methods for interacting with Habitica Parties and Groups."""

    # Assert self is HabiticaAPI for type hinting internal methods
    if TYPE_CHECKING:
        _request: Callable[..., Coroutine[Any, Any, HabiticaApiSuccessData]]
        get: Callable[..., Coroutine[Any, Any, HabiticaApiSuccessData]]
        post: Callable[..., Coroutine[Any, Any, HabiticaApiSuccessData]]
        put: Callable[..., Coroutine[Any, Any, HabiticaApiSuccessData]]
        delete: Callable[..., Coroutine[Any, Any, HabiticaApiSuccessData]]
        # Removed _ensure_type

    # FUNC: get_party_data
    async def get_party_data(self) -> dict[str, Any] | None:
        """Fetches data for the user's current party.

        Returns:
            A dictionary containing party data, or None if not in a party or on error.
        """
        # Endpoint is /groups/party for the user's party
        result = await self.get("/groups/party")
        return cast(dict[str, Any], result) if isinstance(result, dict) else None

    # FUNC: get_group_chat_messages
    async def get_group_chat_messages(self, group_id: str) -> list[dict[str, Any]]:
        """Fetches chat messages for a specific group (party, guild, tavern).

        Args:
            group_id: The ID of the group ('party', guild ID, or 'tavern').

        Returns:
            A list of chat message dictionaries, or an empty list.

        Raises:
            ValueError: If group_id is empty.
        """
        if not group_id:
            raise ValueError("group_id cannot be empty.")

        params: dict[str, Any] = {}

        result = await self.get(f"/groups/{group_id}/chat", params=params)
        return cast(list[dict[str, Any]], result) if isinstance(result, list) else []

    # FUNC: like_group_chat_message
    async def like_group_chat_message(self, group_id: str, chat_id: str) -> dict[str, Any] | None:
        """Likes a specific chat message within a group.

        Args:
            group_id: The ID of the group containing the message.
            chat_id: The ID (_id) of the chat message to like.

        Returns:
            A dictionary confirming the like action, or None on failure.

        Raises:
            ValueError: If group_id or chat_id is empty.
        """
        if not group_id or not chat_id:
            raise ValueError("group_id and chat_id are required.")
        # Note: API endpoint requires POST, not GET
        result = await self.post(f"/groups/{group_id}/chat/{chat_id}/like")
        return cast(dict[str, Any], result) if isinstance(result, dict) else None

    # FUNC: mark_group_chat_seen
    async def mark_group_chat_seen(self, group_id: str = "party") -> bool:
        """Marks messages in a group chat as seen by the user.

        Args:
            group_id: The ID of the group ('party', guild ID, 'tavern').

        Returns:
            True if the operation was successful (API returned no data), False otherwise.

        Raises:
            ValueError: If group_id is empty.
        """
        if not group_id:
            raise ValueError("group_id cannot be empty.")
        result = await self.post(f"/groups/{group_id}/chat/seen")
        return result is None

    # FUNC: post_group_chat_message
    async def post_group_chat_message(self, group_id: str = "party", message_text: str = "") -> dict[str, Any] | None:
        """Posts a message to a group chat.

        Args:
            group_id: The ID of the group ('party', guild ID, 'tavern').
            message_text: The content of the message to post.

        Returns:
            A dictionary representing the posted message, or None on failure.

        Raises:
            ValueError: If group_id is empty or message_text is empty/whitespace.
        """
        if not group_id:
            raise ValueError("group_id is required.")
        message_text_stripped = message_text.strip()
        if not message_text_stripped:
            raise ValueError("message_text cannot be empty.")

        payload = {"message": message_text_stripped}
        result = await self.post(f"/groups/{group_id}/chat", data=payload)
        return cast(dict[str, Any], result) if isinstance(result, dict) else None

    # FUNC: get_quest_status (Checks if party quest is active)
    async def get_quest_status(self) -> bool | None:
        """Checks if the user's party is currently on an active quest.

        Returns:
            True if a quest is active, False if not active or no quest,
            None if not in a party or an error occurred.
        """
        try:
            party_data = await self.get_party_data()
            if party_data is None:
                log.info("User not in a party, cannot get quest status.")
                return None  # Not in a party

            quest_info = party_data.get("quest", {})
            # Check if 'active' is true and 'completed' is missing/false
            is_active = isinstance(quest_info, dict) and quest_info.get("active", False) and not quest_info.get("completed")
            log.debug(f"Party quest active status: {is_active}")
            return is_active

        except HabiticaAPIError as e:
            log.error(f"API Error getting quest status: {e}")
            return None
        except Exception as e:
            log.exception(f"Unexpected error getting quest status: {e}")
            return None

    # FUNC: cast_skill
    async def cast_skill(self, spell_id: str, target_id: str | None = None) -> dict[str, Any] | None:
        """Casts a class skill/spell, optionally targeting another user.

        Args:
            spell_id: The key/ID of the skill/spell to cast (e.g., 'smash', 'healAll').
            target_id: Optional UUID of the user to target (for single-target skills).

        Returns:
            A dictionary containing the result of the cast (e.g., updated stats), or None on failure.

        Raises:
            ValueError: If spell_id is empty.
        """
        if not spell_id:
            raise ValueError("spell_id cannot be empty.")

        params = {"targetId": target_id} if target_id else None
        # Endpoint: /user/class/cast/:spellId
        result = await self.post(f"/user/class/cast/{spell_id}", params=params)
        # API returns the updated user data or error
        return cast(dict[str, Any], result) if isinstance(result, dict) else None
-e 
-------- END OF FILE api/mixin/party_mixin.py --------

-------- START OF FILE api/mixin/tag_mixin.py --------
# pixabit/habitica/mixin/tag_mixin.py

# SECTION: MODULE DOCSTRING
"""Mixin class providing Habitica Tag related API methods."""

# SECTION: IMPORTS
from __future__ import annotations

from typing import TYPE_CHECKING, Any, cast

# Use TYPE_CHECKING to avoid circular import issues if API uses models
if TYPE_CHECKING:
    from collections.abc import Callable, Coroutine  # For hinting self methods

    from pixabit.api.habitica_api import HabiticaAPI, HabiticaApiSuccessData

# SECTION: MIXIN CLASS


# KLASS: TagMixin # Renamed from TagsMixin for consistency
class TagMixin:
    """Mixin containing methods for managing Habitica Tags."""

    # Assert self is HabiticaAPI for type hinting internal methods
    if TYPE_CHECKING:
        _request: Callable[..., Coroutine[Any, Any, HabiticaApiSuccessData]]
        get: Callable[..., Coroutine[Any, Any, HabiticaApiSuccessData]]
        post: Callable[..., Coroutine[Any, Any, HabiticaApiSuccessData]]
        put: Callable[..., Coroutine[Any, Any, HabiticaApiSuccessData]]
        delete: Callable[..., Coroutine[Any, Any, HabiticaApiSuccessData]]

    # FUNC: get_tags
    async def get_tags(self) -> list[dict[str, Any]]:
        """Fetches all tags associated with the user.

        Returns:
            A list of tag dictionaries, or an empty list.
        """
        result = await self.get("/tags")
        return cast(list[dict[str, Any]], result) if isinstance(result, list) else []

    # FUNC: create_tag
    async def create_tag(self, name: str) -> dict[str, Any] | None:
        """Creates a new tag.

        Args:
            name: The name for the new tag.

        Returns:
            A dictionary representing the newly created tag, or None on failure.

        Raises:
            ValueError: If the tag name is empty or whitespace.
        """
        name_stripped = name.strip()
        if not name_stripped:
            raise ValueError("Tag name cannot be empty.")
        result = await self.post("/tags", data={"name": name_stripped})
        # API returns the created tag object
        return cast(dict[str, Any], result) if isinstance(result, dict) else None

    # FUNC: update_tag
    async def update_tag(self, tag_id: str, name: str) -> dict[str, Any] | None:
        """Updates the name of an existing tag.

        Args:
            tag_id: The ID of the tag to update.
            name: The new name for the tag.

        Returns:
            A dictionary representing the updated tag, or None on failure.

        Raises:
            ValueError: If tag_id is empty or the new name is empty/whitespace.
        """
        if not tag_id:
            raise ValueError("tag_id cannot be empty.")
        name_stripped = name.strip()
        if not name_stripped:
            raise ValueError("New tag name cannot be empty.")

        result = await self.put(f"/tags/{tag_id}", data={"name": name_stripped})
        # API returns the updated tag object
        return cast(dict[str, Any], result) if isinstance(result, dict) else None

    # FUNC: delete_tag
    async def delete_tag(self, tag_id: str) -> bool:
        """Deletes a specific tag.

        Args:
            tag_id: The ID of the tag to delete.

        Returns:
            True if the operation was successful (API returned no data), False otherwise.

        Raises:
            ValueError: If tag_id is empty.
        """
        if not tag_id:
            raise ValueError("tag_id cannot be empty.")
        result = await self.delete(f"/tags/{tag_id}")
        # Successful DELETE often returns 204 No Content (result is None)
        return result is None

    # FUNC: reorder_tag
    async def reorder_tag(self, tag_id: str, position: int) -> bool:
        """Moves a tag to a specific position in the user's tag list.

        Args:
            tag_id: The ID of the tag to move.
            position: The desired 0-based index for the tag.

        Returns:
            True if the operation was successful (API returned no data), False otherwise.

        Raises:
            ValueError: If tag_id is empty or position is invalid.
        """
        if not tag_id:
            raise ValueError("tag_id cannot be empty.")
        if not isinstance(position, int) or position < 0:
            raise ValueError("Position must be a non-negative integer index.")

        # API endpoint is /reorder-tags, payload requires 'tagId' and 'to'
        payload = {"tagId": tag_id, "to": position}
        result = await self.post("/reorder-tags", data=payload)
        # Successful reorder likely returns 204 No Content (result is None)
        return result is None
-e 
-------- END OF FILE api/mixin/tag_mixin.py --------

-------- START OF FILE api/mixin/task_mixin.py --------
# pixabit/habitica/mixin/task_mixin.py

# SECTION: MODULE DOCSTRING
"""Mixin class providing Habitica Task related API methods."""

# SECTION: IMPORTS
from __future__ import annotations

from enum import Enum
from typing import TYPE_CHECKING, Any, Literal, cast

# Pydantic is used for the TaskData input model
from pydantic import BaseModel

# Use TYPE_CHECKING to avoid circular import issues if API uses models
if TYPE_CHECKING:
    from collections.abc import Callable, Coroutine  # For hinting self methods

    from pixabit.api.habitica_api import HabiticaAPI, HabiticaApiSuccessData

# SECTION: ENUMS


# ENUM: TaskType
class TaskType(str, Enum):
    """Enumeration for Habitica task types."""

    HABIT = "habit"
    DAILY = "daily"
    TODO = "todo"
    REWARD = "reward"


# ENUM: ScoreDirection
class ScoreDirection(str, Enum):
    """Enumeration for task scoring direction."""

    UP = "up"
    DOWN = "down"


# ENUM: Attribute
class Attribute(str, Enum):
    """Enumeration for Habitica character attributes."""

    STRENGTH = "str"
    INTELLIGENCE = "int"
    CONSTITUTION = "con"
    PERCEPTION = "per"


# SECTION: INPUT DATA MODEL


# KLASS: TaskData (Input Model)
class TaskData(BaseModel):
    """Pydantic model representing data for creating a new task."""

    text: str
    type: TaskType
    notes: str | None = None
    priority: float = 1.0  # Default priority
    attribute: Attribute | None = None  # Maps to str, int, con, per
    tags: list[str] | None = None  # List of tag UUIDs

    model_config = {"use_enum_values": True}  # Ensure enum values are used in serialization


# SECTION: MIXIN CLASS


# KLASS: TasksMixin
class TasksMixin:
    """Mixin containing methods for interacting with Habitica Tasks."""

    # Assert self is HabiticaAPI for type hinting internal methods
    if TYPE_CHECKING:
        _request: Callable[..., Coroutine[Any, Any, HabiticaApiSuccessData]]
        get: Callable[..., Coroutine[Any, Any, HabiticaApiSuccessData]]
        post: Callable[..., Coroutine[Any, Any, HabiticaApiSuccessData]]
        put: Callable[..., Coroutine[Any, Any, HabiticaApiSuccessData]]
        delete: Callable[..., Coroutine[Any, Any, HabiticaApiSuccessData]]

    # --- Core Task Methods ---

    # FUNC: get_tasks
    async def get_tasks(
        self,
        task_type: TaskType | Literal["habits", "dailys", "todos", "rewards"] | None = None,
    ) -> list[dict[str, Any]]:
        """Get user tasks, optionally filtered by type.

        Args:
            task_type: Optional task type filter (enum or string). Note Habitica API
                       uses plural for filtering (e.g., 'dailys').

        Returns:
            A list of task dictionaries, or an empty list.
        """
        params: dict[str, Any] | None = None
        if task_type:
            # API uses plural form for type filtering
            type_value = task_type.value if isinstance(task_type, Enum) else task_type
            # Adjust to plural if needed, handle potential inconsistencies
            api_type_filter = type_value + "s" if type_value in ["daily", "todo", "reward"] else type_value
            params = {"type": api_type_filter}

        # Endpoint is /tasks/user
        result = await self.get("tasks/user", params=params)
        return cast(list[dict[str, Any]], result) if isinstance(result, list) else []

    # FUNC: create_task
    async def create_task(self, task: TaskData | dict[str, Any]) -> dict[str, Any] | None:
        """Create a new task.

        Args:
            task: Task data (either as TaskData Pydantic model or dict).

        Returns:
            A dictionary representing the created task, or None on failure.

        Raises:
            ValueError: If required fields ('text', 'type') are missing in the input data.
        """
        # Convert Pydantic model to dict if necessary
        if isinstance(task, TaskData):
            # Use model_dump for Pydantic v2+, ensure enums become values
            data = task.model_dump(mode="json")
        elif isinstance(task, dict):
            data = task
        else:
            raise TypeError("task argument must be a TaskData model or a dictionary.")

        # Validate essential fields before sending
        if not data.get("text") or not data.get("type"):
            raise ValueError("Task data requires 'text' and 'type'.")
        # Ensure type is a valid string if passed as dict
        if data["type"] not in TaskType._value2member_map_:
            raise ValueError(f"Invalid task type: {data['type']}")

        # Endpoint is /tasks/user
        result = await self.post("tasks/user", data=data)
        return cast(dict[str, Any], result) if isinstance(result, dict) else None

    # FUNC: update_task
    async def update_task(self, task_id: str, data: dict[str, Any]) -> dict[str, Any] | None:
        """Update an existing task.

        Args:
            task_id: The ID (_id) of the task to update.
            data: Dictionary containing fields to update.

        Returns:
            A dictionary representing the updated task, or None on failure.

        Raises:
            ValueError: If task_id is empty or data is empty.
        """
        if not task_id:
            raise ValueError("task_id cannot be empty.")
        if not data:
            raise ValueError("Update data cannot be empty.")

        # Endpoint is /tasks/:taskId
        result = await self.put(f"tasks/{task_id}", data=data)
        return cast(dict[str, Any], result) if isinstance(result, dict) else None

    # FUNC: delete_task
    async def delete_task(self, task_id: str) -> bool:
        """Delete a task.

        Args:
            task_id: The ID (_id) of the task to delete.

        Returns:
            True if deletion was successful (API returned no data), False otherwise.

        Raises:
            ValueError: If task_id is empty.
        """
        if not task_id:
            raise ValueError("task_id cannot be empty.")

        # Endpoint is /tasks/:taskId
        result = await self.delete(f"tasks/{task_id}")
        # Successful DELETE often returns 204 No Content (result is None)
        return result is None

    # FUNC: score_task
    async def score_task(
        self,
        task_id: str,
        direction: ScoreDirection | Literal["up", "down"] = ScoreDirection.UP,
    ) -> dict[str, Any] | None:
        """Score (complete or undo/fail) a task.

        Args:
            task_id: The ID (_id) of the task to score.
            direction: "up" to complete/check positive, "down" to undo/fail/check negative (enum or string).

        Returns:
            Score response data including drops, damage, stat changes etc., or None on failure.

        Raises:
            ValueError: If task_id is empty or direction is invalid.
        """
        if not task_id:
            raise ValueError("task_id cannot be empty.")

        # Get the string value from enum or validate the string
        if isinstance(direction, ScoreDirection):
            dir_value = direction.value
        elif isinstance(direction, str) and direction in ["up", "down"]:
            dir_value = direction
        else:
            raise ValueError("Direction must be 'up' or 'down'.")

        # Endpoint is /tasks/:taskId/score/:direction
        result = await self.post(f"tasks/{task_id}/score/{dir_value}")
        return cast(dict[str, Any], result) if isinstance(result, dict) else None

    # --- Helper Task Methods ---

    # FUNC: set_task_attribute (Helper using update_task)
    async def set_task_attribute(
        self,
        task_id: str,
        attribute: Attribute | Literal["str", "int", "con", "per"],
    ) -> dict[str, Any] | None:
        """Set the primary attribute for a task (influences stat gain on completion).

        Args:
            task_id: The ID (_id) of the task.
            attribute: The attribute to set (enum or string 'str', 'int', 'con', 'per').

        Returns:
            The updated task data dictionary, or None on failure.

        Raises:
            ValueError: If attribute is invalid.
        """
        # Get the string value from enum or validate the string
        if isinstance(attribute, Attribute):
            attr_value = attribute.value
        elif isinstance(attribute, str) and attribute in [
            "str",
            "int",
            "con",
            "per",
        ]:
            attr_value = attribute
        else:
            raise ValueError("Invalid attribute. Must be 'str', 'int', 'con', or 'per'.")

        return await self.update_task(task_id, {"attribute": attr_value})

    # FUNC: move_task_to_position
    async def move_task_to_position(self, task_id: str, position: int) -> list[str] | None:
        """Move a task to a specific position within its list (e.g., Todos).

        Args:
            task_id: The ID (_id) of the task to move.
            position: The desired 0-based index.

        Returns:
            A list of task IDs in the new order, or None on failure.

        Raises:
            ValueError: If task_id is empty or position is not an integer.
        """
        if not task_id:
            raise ValueError("task_id cannot be empty.")
        if not isinstance(position, int):
            raise ValueError("Position must be an integer.")

        # Endpoint: /tasks/:taskId/move/to/:position
        result = await self.post(f"tasks/{task_id}/move/to/{position}")
        # API returns the new task order [taskId1, taskId2, ...]
        return cast(list[str], result) if isinstance(result, list) else None

    # FUNC: clear_completed_todos
    async def clear_completed_todos(self) -> bool:
        """Clear (delete) all completed todo tasks for the user.

        Returns:
            True if successful (API returned no data), False otherwise.
        """
        # Endpoint: /tasks/clearCompletedTodos
        result = await self.post("tasks/clearCompletedTodos")
        return result is None

    # --- Tagging Methods ---

    # FUNC: add_tag_to_task
    async def add_tag_to_task(self, task_id: str, tag_id: str) -> dict[str, Any] | None:
        """Adds a tag to a specific task.

        Args:
            task_id: The ID (_id) of the task.
            tag_id: The UUID of the tag to add.

        Returns:
            The updated task data dictionary, or None on failure.

        Raises:
            ValueError: If task_id or tag_id is empty.
        """
        if not task_id or not tag_id:
            raise ValueError("task_id and tag_id cannot be empty.")
        # Endpoint: /tasks/:taskId/tags/:tagId
        result = await self.post(f"/tasks/{task_id}/tags/{tag_id}")
        return cast(dict[str, Any], result) if isinstance(result, dict) else None

    # FUNC: delete_tag_from_task
    async def delete_tag_from_task(self, task_id: str, tag_id: str) -> dict[str, Any] | None:
        """Removes a tag from a specific task.

        Args:
            task_id: The ID (_id) of the task.
            tag_id: The UUID of the tag to remove.

        Returns:
            The updated task data dictionary (likely without the tag), or None on failure.

        Raises:
            ValueError: If task_id or tag_id is empty.
        """
        if not task_id or not tag_id:
            raise ValueError("task_id and tag_id cannot be empty.")
        # Endpoint: /tasks/:taskId/tags/:tagId
        result = await self.delete(f"/tasks/{task_id}/tags/{tag_id}")
        # API returns the updated task after tag removal
        return cast(dict[str, Any], result) if isinstance(result, dict) else None

    # --- Checklist Methods ---

    # FUNC: add_checklist_item
    async def add_checklist_item(self, task_id: str, text: str) -> dict[str, Any] | None:
        """Adds a checklist item to a Daily or Todo task.

        Args:
            task_id: The ID (_id) of the task.
            text: The text content of the checklist item.

        Returns:
            The updated task data dictionary (with the new checklist item), or None on failure.

        Raises:
            ValueError: If task_id or text is empty.
        """
        if not task_id or not text.strip():
            raise ValueError("task_id and text cannot be empty.")
        # Endpoint: /tasks/:taskId/checklist
        result = await self.post(f"/tasks/{task_id}/checklist", data={"text": text.strip()})
        return cast(dict[str, Any], result) if isinstance(result, dict) else None

    # FUNC: update_checklist_item
    async def update_checklist_item(self, task_id: str, item_id: str, text: str) -> dict[str, Any] | None:
        """Updates the text of an existing checklist item.

        Args:
            task_id: The ID (_id) of the parent task.
            item_id: The ID of the checklist item to update.
            text: The new text content for the item.

        Returns:
            The updated task data dictionary, or None on failure.

        Raises:
            ValueError: If task_id, item_id, or text is empty/missing.
        """
        if not task_id or not item_id or not text.strip():  # Check text is not just whitespace
            raise ValueError("task_id, item_id, and text are required.")
        # Endpoint: /tasks/:taskId/checklist/:itemId
        result = await self.put(f"/tasks/{task_id}/checklist/{item_id}", data={"text": text.strip()})
        return cast(dict[str, Any], result) if isinstance(result, dict) else None

    # FUNC: delete_checklist_item
    async def delete_checklist_item(self, task_id: str, item_id: str) -> dict[str, Any] | None:
        """Deletes a checklist item from a task.

        Args:
            task_id: The ID (_id) of the parent task.
            item_id: The ID of the checklist item to delete.

        Returns:
            The updated task data dictionary (without the checklist item), or None on failure.

        Raises:
            ValueError: If task_id or item_id is empty.
        """
        if not task_id or not item_id:
            raise ValueError("task_id and item_id are required.")
        # Endpoint: /tasks/:taskId/checklist/:itemId
        result = await self.delete(f"/tasks/{task_id}/checklist/{item_id}")
        return cast(dict[str, Any], result) if isinstance(result, dict) else None

    # FUNC: score_checklist_item
    async def score_checklist_item(self, task_id: str, item_id: str) -> dict[str, Any] | None:
        """Scores (marks as complete/incomplete) a checklist item.

        Args:
            task_id: The ID (_id) of the parent task.
            item_id: The ID of the checklist item to score.

        Returns:
            The updated task data dictionary (with the checklist item's status changed), or None on failure.

        Raises:
            ValueError: If task_id or item_id is empty.
        """
        if not task_id or not item_id:
            raise ValueError("task_id and item_id are required.")
        # Endpoint: /tasks/:taskId/checklist/:itemId/score
        result = await self.post(f"/tasks/{task_id}/checklist/{item_id}/score")
        return cast(dict[str, Any], result) if isinstance(result, dict) else None
-e 
-------- END OF FILE api/mixin/task_mixin.py --------

-------- START OF FILE api/mixin/user_mixin.py --------
# pixabit/habitica/mixin/user_mixin.py

# SECTION: MODULE DOCSTRING
"""Mixin class providing Habitica User related API methods."""

# SECTION: IMPORTS
from __future__ import annotations

from typing import TYPE_CHECKING, Any, cast

# Use TYPE_CHECKING to avoid circular import issues if API uses models
if TYPE_CHECKING:
    from collections.abc import Callable, Coroutine  # For hinting self methods

    from pixabit.api.habitica_api import HabiticaAPI, HabiticaApiSuccessData

# SECTION: MIXIN CLASS


# KLASS: UserMixin
class UserMixin:
    """Mixin containing methods for interacting with the Habitica User endpoint."""

    # Assert self is HabiticaAPI for type hinting internal methods
    if TYPE_CHECKING:
        _request: Callable[..., Coroutine[Any, Any, HabiticaApiSuccessData]]
        get: Callable[..., Coroutine[Any, Any, HabiticaApiSuccessData]]
        post: Callable[..., Coroutine[Any, Any, HabiticaApiSuccessData]]
        put: Callable[..., Coroutine[Any, Any, HabiticaApiSuccessData]]
        delete: Callable[..., Coroutine[Any, Any, HabiticaApiSuccessData]]

    # FUNC: get_user_data
    async def get_user_data(self) -> dict[str, Any] | None:
        """Fetches the authenticated user's data object.

        Returns:
            A dictionary containing the user's data, or None on failure.
        """
        # Endpoint: /user
        result = await self.get("user")
        # User data is expected to be a dictionary
        return cast(dict[str, Any], result) if isinstance(result, dict) else None

    # FUNC: update_user
    async def update_user(self, update_data: dict[str, Any]) -> dict[str, Any] | None:
        """Updates user preferences or settings.

        Args:
            update_data: A dictionary containing the fields to update (e.g., {"preferences.sleep": True}).
                         Uses dot notation for nested fields.

        Returns:
            A dictionary representing the updated user data, or None on failure.

        Raises:
            ValueError: If update_data is empty.
        """
        if not update_data:
            raise ValueError("update_data cannot be empty.")
        # Endpoint: /user
        result = await self.put("user", data=update_data)
        return cast(dict[str, Any], result) if isinstance(result, dict) else None

    # FUNC: toggle_user_sleep
    async def toggle_user_sleep(self) -> bool | None:
        """Toggles the user's sleep status (resting in the Inn).

        Returns:
            The new sleep status (True if sleeping, False if awake) if successful, None otherwise.
        """
        # Endpoint: /user/sleep
        result = await self.post("user/sleep")
        # The API documentation suggests the 'data' field in the response contains the new sleep status.
        if isinstance(result, bool):
            return result  # API directly returns boolean in 'data'
        elif isinstance(result, dict) and "data" in result and isinstance(result["data"], bool):
            return result["data"]  # Handle if wrapped in standard {success:true, data: bool}
        # Fallback based on observed behavior or if docs are unclear: Check if result is a dict and non-empty?
        # elif isinstance(result, dict):
        #     return result.get('sleep_status_key') # Check if a specific key indicates status
        return None  # Return None if status cannot be determined

    # FUNC: run_cron
    async def run_cron(self) -> dict[str, Any] | None:
        """Manually triggers the user's cron process (resets dailies, etc.).

        Note: Use with caution, intended primarily for development/testing.

        Returns:
            A dictionary confirming the cron run (often empty), or None on failure.
        """
        # Endpoint: /cron
        result = await self.post("cron")
        # Cron response is typically empty or just {success: true, data: {}}
        return cast(dict[str, Any], result) if isinstance(result, dict) else {}  # Return empty dict on success if None

    # FUNC: set_custom_day_start
    async def set_custom_day_start(self, hour: int) -> dict[str, Any] | None:
        """Sets the user's custom day start hour (when dailies reset).

        Args:
            hour: The hour in 24-hour format (0-23) when the day should start.

        Returns:
            The updated user data dictionary, or None on failure.

        Raises:
            ValueError: If the hour is outside the valid range (0-23).
        """
        if not 0 <= hour <= 23:
            raise ValueError("Hour must be between 0 and 23 (inclusive).")

        # Endpoint: /user/custom-day-start
        result = await self.post("user/custom-day-start", data={"dayStart": hour})
        return cast(dict[str, Any], result) if isinstance(result, dict) else None
-e 
-------- END OF FILE api/mixin/user_mixin.py --------

-------- START OF FILE api/mixin/__init__.py --------
# pixabit/habitica/mixin/__init__.py

# This file can remain empty.
# It signifies that the 'mixin' directory is a Python package.
-e 
-------- END OF FILE api/mixin/__init__.py --------

-------- START OF FILE api/__init__.py --------
-e 
-------- END OF FILE api/__init__.py --------

-------- START OF FILE config.py --------
# pixabit/config.py

from pathlib import Path

HABITICA_DATA_PATH: Path = Path("./habitica_cache")
HABITICA_DATA_RAW = "raw_content.json"
HABITICA_DATA_PROCESSED: str = "processed_content.json"
DEFAULT_CACHE_DURATION_DAYS = 7
HABITICA_DATA_PATH.mkdir(parents=True, exist_ok=True)
USER_ID = "50f36c30-60c7-46f7-92d1-be0e7c7259d6"
-e 
-------- END OF FILE config.py --------

-------- START OF FILE helpers/DateTimeHandler.py --------
# pixabit/helpers/DateTimeHandler.py

# ─── Helper ───────────────────────────────────────────────────────────────────
#          DateTime Handling Utility based on Pydantic
# ──────────────────────────────────────────────────────────────────────────────

# SECTION: IMPORTS
from __future__ import annotations

from datetime import datetime, timedelta, timezone
from typing import Any

import dateutil.parser
from dateutil.tz import tzlocal
from pydantic import (
    BaseModel,
    ConfigDict,
    Field,
    model_validator,
)

# Local Imports (assuming logger is available)
try:
    from ._logger import log
except ImportError:
    import logging

    log = logging.getLogger(__name__)
    log.addHandler(logging.NullHandler())

# SECTION: DateTimeHandler MODEL


# KLASS: DateTimeHandler
class DateTimeHandler(BaseModel):
    """Handles date/time operations with consistent timezone handling and formatting.

    Processes various timestamp inputs (ISO string, Unix seconds/ms, datetime)
    into timezone-aware UTC and local datetime objects.

    Attributes:
        timestamp: The original input timestamp (raw).
        utc_datetime: The timestamp converted to a timezone-aware UTC datetime.
        local_datetime: The timestamp converted to the system's local timezone.
        local_timezone: The detected local timezone.
    """

    model_config = ConfigDict(arbitrary_types_allowed=True)

    # Raw input
    timestamp: str | datetime | int | float | None = Field(None, description="Original input timestamp.")

    # Processed outputs
    utc_datetime: datetime | None = Field(None, description="Timestamp converted to aware UTC datetime.")
    local_datetime: datetime | None = Field(None, description="Timestamp converted to local timezone.")

    # Configuration
    local_timezone: Any = Field(default_factory=tzlocal, description="System's local timezone.", exclude=True)  # Exclude from dump

    @model_validator(mode="before")
    @classmethod
    def process_timestamps(cls, values: Any) -> dict[str, Any]:
        """Parses the input timestamp into UTC and then converts to local time.
        Handles initialization logic based on the 'timestamp' input.
        """
        if not isinstance(values, dict):
            # Handle direct instantiation like DateTimeHandler(timestamp=...)
            if "timestamp" in values:
                timestamp_input = values["timestamp"]
            else:
                # Or handle if only the value is passed, assume it's the timestamp
                timestamp_input = values
                values = {"timestamp": timestamp_input}  # Structure it as a dict for processing
        else:
            # Handle dict input from model_validate, etc.
            timestamp_input = values.get("timestamp")

        if timestamp_input is None:
            # Allow initialization without timestamp for using class methods like now()
            return values  # Pass through existing values if any

        # --- Parse to UTC ---
        utc_dt: datetime | None = None
        try:
            if isinstance(timestamp_input, (int, float)):
                # Treat as Unix timestamp (auto-detect ms vs s)
                if abs(timestamp_input) > 2e9:  # Likely milliseconds
                    ts_sec = timestamp_input / 1000.0
                else:
                    ts_sec = float(timestamp_input)
                utc_dt = datetime.fromtimestamp(ts_sec, tz=timezone.utc)

            elif isinstance(timestamp_input, str):
                # Try parsing as ISO 8601 string
                parsed_dt = dateutil.parser.isoparse(timestamp_input)
                # Ensure timezone-aware UTC
                if parsed_dt.tzinfo is None:
                    # Assume UTC if timezone is naive (common practice for ISO strings without tz)
                    utc_dt = parsed_dt.replace(tzinfo=timezone.utc)
                else:
                    # Convert explicitly to UTC
                    utc_dt = parsed_dt.astimezone(timezone.utc)

            elif isinstance(timestamp_input, datetime):
                # If already a datetime, ensure it's UTC
                if timestamp_input.tzinfo is None:
                    # Assume UTC if naive
                    utc_dt = timestamp_input.replace(tzinfo=timezone.utc)
                else:
                    utc_dt = timestamp_input.astimezone(timezone.utc)

            else:
                raise TypeError(f"Unsupported timestamp type: {type(timestamp_input).__name__}")

        except (ValueError, TypeError, OverflowError) as e:
            log.warning(f"Could not parse timestamp '{timestamp_input}': {e}. Setting UTC datetime to None.")
            utc_dt = None
            # Raise validation error? Or just log and set None? For now, None.
            # raise ValueError(f"Invalid timestamp format: {timestamp_input}") from e

        values["utc_datetime"] = utc_dt

        # --- Convert UTC to Local ---
        local_tz = values.get("local_timezone", tzlocal())  # Use existing or default
        if utc_dt:
            try:
                values["local_datetime"] = utc_dt.astimezone(local_tz).replace(microsecond=0)
            except Exception as e:
                log.error(f"Could not convert UTC time {utc_dt} to local time zone {local_tz}: {e}")
                values["local_datetime"] = None  # Set None on conversion failure
        else:
            values["local_datetime"] = None  # If UTC is None, local must be None

        # Ensure local_timezone is set in the output dict if not already present
        values["local_timezone"] = local_tz

        return values

    # --- Class Methods for Instantiation ---
    @classmethod
    def from_iso(cls, iso_timestamp: str) -> DateTimeHandler:
        """Creates DateTimeHandler instance from an ISO 8601 timestamp string."""
        return cls(timestamp=iso_timestamp)

    @classmethod
    def from_unix_ms(cls, unix_ms: int) -> DateTimeHandler:
        """Creates DateTimeHandler instance from a Unix timestamp in milliseconds."""
        return cls(timestamp=unix_ms)

    @classmethod
    def from_unix_seconds(cls, unix_seconds: float | int) -> DateTimeHandler:
        """Creates DateTimeHandler instance from a Unix timestamp in seconds."""
        return cls(timestamp=unix_seconds)

    @classmethod
    def now(cls) -> DateTimeHandler:
        """Creates DateTimeHandler instance with the current time."""
        return cls(timestamp=datetime.now(timezone.utc))

    # --- Formatting and Utility Methods ---
    def is_past(self) -> bool | None:
        """Checks if the UTC datetime is in the past. Returns None if datetime is unknown."""
        if self.utc_datetime is None:
            return None
        return self.utc_datetime < datetime.now(timezone.utc)

    def format_time_difference(self) -> str:
        """Formats the time difference between the local datetime and now.

        Returns:
            A human-readable string like "in 5m", "2h ago", "now", or "N/A".
        """
        if self.local_datetime is None:
            return "N/A"

        now_local = datetime.now(self.local_timezone).replace(microsecond=0)
        delta = self.local_datetime - now_local
        return self._format_timedelta(delta)

    def _format_timedelta(self, delta: timedelta) -> str:
        """Formats a timedelta into a human-readable string."""
        total_seconds_float = delta.total_seconds()

        if abs(total_seconds_float) < 1:
            return "now"

        is_past = total_seconds_float < 0
        abs_delta = abs(delta)
        total_abs_seconds = int(abs_delta.total_seconds())

        days, day_seconds = divmod(total_abs_seconds, 86400)  # 24 * 3600
        hours, hour_seconds = divmod(day_seconds, 3600)
        minutes, seconds = divmod(hour_seconds, 60)

        parts = []
        if days > 1:
            parts.append(f"{days}d")
        elif days == 1:
            # If it's between 1 and 2 days, show hours instead for more precision
            total_hours = hours + 24
            parts.append(f"{total_hours}h")
        elif hours > 0:
            parts.append(f"{hours}h")
        elif minutes > 0:
            parts.append(f"{minutes}m")
        elif seconds > 0:
            # Show seconds only if it's the largest unit
            parts.append(f"{seconds}s")

        time_str = "".join(parts)  # Combine directly without spaces

        if is_past:
            return f"{time_str} ago"
        else:
            return f"in {time_str}"

    def format_local(self, format_str: str = "%Y-%m-%d %H:%M") -> str:
        """Formats the local datetime using a specified format string.

        Args:
            format_str: The strftime format string.

        Returns:
            The formatted local time string, or "N/A".
        """
        if self.local_datetime is None:
            return "N/A"
        return self.local_datetime.strftime(format_str)

    def format_utc(self, format_str: str = "%Y-%m-%d %H:%M UTC") -> str:
        """Formats the UTC datetime using a specified format string.

        Args:
            format_str: The strftime format string.

        Returns:
            The formatted UTC time string, or "N/A".
        """
        if self.utc_datetime is None:
            return "N/A"
        return self.utc_datetime.strftime(format_str)

    def format_with_diff(self, format_str: str = "%Y-%m-%d %H:%M") -> str:
        """Formats the local date/time followed by the relative time difference.

        Args:
            format_str: The strftime format string for the date/time part.

        Returns:
            Combined string like "2023-10-27 15:30 (in 5m)", or "N/A".
        """
        local_fmt = self.format_local(format_str)
        if local_fmt == "N/A":
            return "N/A"
        diff = self.format_time_difference()
        return f"{local_fmt} ({diff})"

    # --- Conversion Methods ---
    def to_iso(self) -> str | None:
        """Converts the UTC datetime back to ISO 8601 format string."""
        if self.utc_datetime is None:
            return None
        # Ensure Z for UTC timezone indication
        return self.utc_datetime.isoformat().replace("+00:00", "Z")

    def to_unix_ms(self) -> int | None:
        """Converts the UTC datetime to a Unix timestamp in milliseconds."""
        if self.utc_datetime is None:
            return None
        return int(self.utc_datetime.timestamp() * 1000)

    def to_unix_seconds(self) -> float | None:
        """Converts the UTC datetime to a Unix timestamp in seconds."""
        if self.utc_datetime is None:
            return None
        return self.utc_datetime.timestamp()


# ──────────────────────────────────────────────────────────────────────────────
# Example Usage (Optional)
if __name__ == "__main__":

    # Example Usage
    print("--- Examples ---")

    # From ISO String
    iso_str = "2023-10-26T10:00:00Z"
    dt_handler_iso = DateTimeHandler.from_iso(iso_str)
    print(f"ISO Input : {iso_str}")
    print(f"UTC       : {dt_handler_iso.format_utc('%Y-%m-%d %H:%M:%S %Z%z')}")
    print(f"Local     : {dt_handler_iso.format_local('%Y-%m-%d %H:%M:%S %Z%z')}")
    print(f"Formatted : {dt_handler_iso.format_with_diff('%b %d, %H:%M')}")
    print(f"Is Past?  : {dt_handler_iso.is_past()}")
    print("-" * 10)

    # From Unix Milliseconds (approx now)
    now_ms = int(datetime.now(timezone.utc).timestamp() * 1000)
    dt_handler_ms = DateTimeHandler.from_unix_ms(now_ms)
    print(f"Unix MS   : {now_ms}")
    print(f"Formatted : {dt_handler_ms.format_with_diff()}")
    print("-" * 10)

    # Future Date (Unix Seconds)
    future_sec = datetime.now(timezone.utc).timestamp() + 3600 * 3  # 3 hours from now
    dt_handler_future = DateTimeHandler.from_unix_seconds(future_sec)
    print(f"Unix Secs : {future_sec:.0f} (future)")
    print(f"Formatted : {dt_handler_future.format_with_diff()}")
    print("-" * 10)

    # From existing datetime object (naive, assumed UTC)
    naive_dt = datetime(2023, 1, 1, 12, 0, 0)
    dt_handler_naive = DateTimeHandler(timestamp=naive_dt)
    print(f"Naive DT  : {naive_dt}")
    print(f"UTC       : {dt_handler_naive.format_utc('%Y-%m-%d %H:%M:%S %Z')}")
    print(f"Local     : {dt_handler_naive.format_local('%Y-%m-%d %H:%M:%S %Z')}")
    print("-" * 10)

    # Current time
    dt_handler_now = DateTimeHandler.now()
    print("Now       :")
    print(f"Formatted : {dt_handler_now.format_with_diff()}")
    print(f"ISO       : {dt_handler_now.to_iso()}")
    print(f"Unix MS   : {dt_handler_now.to_unix_ms()}")


# ──────────────────────────────────────────────────────────────────────────────
-e 
-------- END OF FILE helpers/DateTimeHandler.py --------

-------- START OF FILE helpers/get_json_schema.py --------
# pixabit/helpers/_get_json_schema.py

# SECTION: MODULE DOCSTRING
"""Utility Script for Generating a Schema from Habitica Content Data.

NOTE: This script is likely intended for one-off schema generation or analysis
      based on an existing JSON file (e.g., 'content_cache_extracted.json').
      It's probably NOT part of the main runtime application logic.

Analyzes the structure of a nested dictionary (expected to be Habitica's
`gear_flat` data) to infer field types, required status, and generate a
basic JSON schema representation.
"""

# SECTION: IMPORTS
import json
from pprint import pprint
from typing import (  # Keep Dict/Set for internal hints if preferred
    Any,
    Dict,
    Set,
    cast,
)

# SECTION: FUNCTIONS


# FUNC: analyze_field_types
def analyze_field_types(data_dict: dict[str, Any]) -> dict[str, Any]:
    """Recursively analyzes a dictionary to build a schema with type information.

    Handles nested objects and arrays of objects or simple types.

    Args:
        data_dict: The dictionary item to analyze.

    Returns:
        A dictionary representing the inferred schema structure and types.
    """
    schema: dict[str, Any] = {}

    for key, value in data_dict.items():
        value_type = type(value).__name__

        if isinstance(value, dict):
            # Recursively analyze nested objects
            schema[key] = {
                "type": "object",
                "properties": analyze_field_types(value),
            }
        elif isinstance(value, list):
            # Handle arrays
            if value:
                # Get type of first item for non-empty arrays
                first_item_type = type(value[0]).__name__
                if first_item_type == "dict":
                    # If array of objects, analyze the structure of the *first* object
                    # Assumes homogeneous array structure for simplicity
                    item_schema = analyze_field_types(value[0])
                    schema[key] = {
                        "type": "array",
                        "items": {"type": "object", "properties": item_schema},
                    }
                else:
                    # Array of simple types
                    schema[key] = {
                        "type": "array",
                        "items": {"type": first_item_type},
                    }
            else:
                # Empty array - type unknown
                schema[key] = {"type": "array", "items": {"type": "unknown"}}
        else:
            # Simple scalar values
            schema[key] = {"type": value_type}

    return schema


# FUNC: determine_required_fields
def determine_required_fields(
    items: dict[str, dict[str, Any]],
) -> tuple[dict[str, dict[str, Any]], int]:
    """Determines which top-level fields are required by analyzing multiple dictionary items.

    A field is considered required if it exists in every item provided.

    Args:
        items: A dictionary where keys are item IDs and values are the item data dictionaries.

    Returns:
        A tuple containing:
            - A dictionary mapping field names to requirement info ({'required': bool, 'count': int}).
            - The total number of items analyzed.
    """
    all_fields: set[str] = set()
    field_counts: dict[str, int] = {}
    total_items = len(items)

    if total_items == 0:
        return {}, 0

    # First pass: collect all possible top-level fields and count occurrences
    for item_data in items.values():
        if not isinstance(item_data, dict):
            continue  # Skip invalid items
        for field in item_data.keys():
            all_fields.add(field)
            field_counts[field] = field_counts.get(field, 0) + 1

    # Build schema part with required information
    required_info: dict[str, dict[str, Any]] = {}
    for field in all_fields:
        count = field_counts.get(field, 0)
        is_required = count == total_items
        required_info[field] = {"required": is_required, "count": count}

    return required_info, total_items


# FUNC: merge_type_and_required_info
def merge_type_and_required_info(
    type_schema: dict[str, Any],
    required_info: dict[str, dict[str, Any]],
    total_items: int,
) -> dict[str, Any]:
    """Merges the inferred type schema with the required field information.

    Args:
        type_schema: The schema dictionary containing type information (from analyze_field_types).
        required_info: The dictionary containing required status and counts for fields.
        total_items: The total number of items analyzed.

    Returns:
        A merged schema dictionary containing type, requirement status, and coverage percentage.
    """
    merged_schema: dict[str, Any] = {}

    for field, type_info in type_schema.items():
        req_data = required_info.get(field, {"required": False, "count": 0})
        coverage_str = (
            f"{req_data['count']}/{total_items} ({req_data['count']/total_items*100:.1f}%)"
            if total_items > 0
            else "N/A"
        )

        # Combine the type info with required status and coverage
        merged_schema[field] = {
            **type_info,  # includes 'type' and potentially 'properties' or 'items'
            "required": req_data["required"],
            "coverage": coverage_str,
        }

    return merged_schema


# FUNC: get_complete_schema
def get_complete_schema(data: dict[str, Any]) -> tuple[dict[str, Any], int]:
    """Generates a comprehensive schema by analyzing item structure and requirement frequency.

    Args:
        data: The full loaded JSON data containing the target dictionary (e.g., 'gear_flat').

    Returns:
        A tuple containing:
            - The generated schema dictionary.
            - The total number of items analyzed.

    Raises:
        KeyError: If the expected data key (e.g., 'gear_flat') is not found.
        TypeError: If the data under the key is not a dictionary.
    """
    # --- Target the specific dictionary to analyze ---
    # Modify this key if analyzing a different part of the JSON
    target_key = "gear_flat"
    items_dict = data.get(target_key)

    if items_dict is None:
        raise KeyError(f"Key '{target_key}' not found in the input data.")
    if not isinstance(items_dict, dict):
        raise TypeError(f"Data under key '{target_key}' must be a dictionary.")
    # --- End Target ---

    # Analyze field requirements across all items
    required_info, total_items = determine_required_fields(items_dict)

    # Analyze the structure (types, nesting) based on the *first* item
    # Assumes structure is relatively consistent across items
    if not items_dict:
        return {}, 0  # Handle empty input dict

    first_item_key = next(iter(items_dict))
    first_item = items_dict[first_item_key]
    type_schema = analyze_field_types(first_item)

    # Merge the type information with requirement info
    complete_schema = merge_type_and_required_info(
        type_schema, required_info, total_items
    )

    return complete_schema, total_items


# SECTION: MAIN EXECUTION (EXAMPLE)


# FUNC: main_schema_generation
def main_schema_generation(
    input_json_path: str = "content_cache_extracted.json",
    output_schema_path: str | None = "generated_schema.json",
):
    """Loads data, generates schema, prints, and optionally saves the schema."""
    print("--- Schema Generation Utility ---")
    print(f"Input JSON: {input_json_path}")

    try:
        # Load the source JSON data
        with open(input_json_path, encoding="utf-8") as f:
            data = json.load(f)
        print("Input JSON loaded successfully.")

        # Generate the schema
        schema, total_items = get_complete_schema(data)
        print(f"\nSchema analysis complete. Analyzed {total_items} items.")

        # Display the schema using pprint
        print("\n--- Generated Schema ---")
        pprint(
            schema, sort_dicts=False
        )  # sort_dicts=False preserves order more often

        # Optionally save the schema to a file
        if output_schema_path:
            try:
                with open(output_schema_path, "w", encoding="utf-8") as f_out:
                    json.dump(schema, f_out, indent=2, ensure_ascii=False)
                print(f"\nSchema saved to: {output_schema_path}")
            except Exception as e:
                print(f"\nError saving schema to {output_schema_path}: {e}")

    except FileNotFoundError:
        print(f"\nError: Input JSON file not found at '{input_json_path}'")
    except (KeyError, TypeError) as e:
        print(f"\nError processing input data: {e}")
    except json.JSONDecodeError:
        print(f"\nError: Invalid JSON format in '{input_json_path}'")
    except Exception as e:
        print(f"\nAn unexpected error occurred: {e}")


if __name__ == "__main__":
    # --- Configuration ---
    INPUT_JSON_FILE = "content_cache_extracted.json"  # Source data file name
    OUTPUT_SCHEMA_FILE = (
        "generated_schema.json"  # Output file name (or None to just print)
    )
    # --- End Configuration ---

    main_schema_generation(
        input_json_path=INPUT_JSON_FILE, output_schema_path=OUTPUT_SCHEMA_FILE
    )
-e 
-------- END OF FILE helpers/get_json_schema.py --------

-------- START OF FILE helpers/_clean_name.py --------
# pixabit/helpers/_clean_name.py

# SECTION: MODULE DOCSTRING
"""Provides utility functions for cleaning filenames.

Replaces characters that are illegal in many file systems, typically using
full-width Unicode variants or underscores, and handles leading whitespace.
"""

# SECTION: IMPORTS
import re
from re import Pattern  # Import Pattern for compiled regex type hint

# SECTION: CONSTANTS
# Translation table for illegal characters -> full-width variants/space
# Characters: " \ * / : < > ? \ | TAB NL VT FF CR
CHARACTER_TRANSLATION_TABLE = str.maketrans(
    '"\\*/:<>?\\|\t\n\v\f\r',  # Illegal characters + whitespace variants
    "＂＼＊／：＜＞？＼￨     ",  # Replacements (full-width + spaces)
)
# Compiled regex pattern to find one or more leading whitespace characters
LEADING_SPACE_PATTERN: Pattern[str] = re.compile(r"^\s+")


# SECTION: FUNCTIONS


# FUNC: replace_illegal_filename_characters
def replace_illegal_filename_characters(input_filename: str) -> str:
    """Replaces illegal filename characters with full-width Unicode variants or spaces.

    Also strips leading/trailing whitespace from the result.

    Args:
        input_filename: The original filename string.

    Returns:
        The cleaned filename string.
    """
    if not isinstance(input_filename, str):
        return ""  # Handle non-string input gracefully
    return input_filename.translate(CHARACTER_TRANSLATION_TABLE).strip()


# FUNC: replace_illegal_filename_characters_leading_underscores
def replace_illegal_filename_characters_leading_underscores(
    input_filename: str,
) -> str:
    """Replaces illegal characters and converts leading whitespace to underscores.

    Illegal characters are replaced using `CHARACTER_TRANSLATION_TABLE`. Any
    sequence of leading whitespace characters in the *original* string is
    replaced by the same number of underscore characters. Strips trailing
    whitespace from the final result.

    Args:
        input_filename: The original filename string.

    Returns:
        The cleaned filename string with leading spaces converted to underscores.
    """
    if not isinstance(input_filename, str):
        return ""  # Handle non-string input gracefully

    # First, replace illegal characters globally
    output_filename = input_filename.translate(CHARACTER_TRANSLATION_TABLE)

    # Then, specifically replace any sequence of leading spaces with underscores
    # Using lambda ensures the correct number of underscores match the space length
    output_filename = LEADING_SPACE_PATTERN.sub(
        lambda match: "_" * len(match.group(0)),
        output_filename,
    )

    # Finally, strip any remaining trailing whitespace (leading are now underscores)
    return output_filename.rstrip()


# FUNC: replace_illegal_filename_characters_prefix_underscore
def replace_illegal_filename_characters_prefix_underscore(
    input_filename: str,
) -> str:
    """Replaces illegal chars and adds '_' prefix if the original started with space.

    Illegal characters are replaced using `CHARACTER_TRANSLATION_TABLE`. If the
    *original* filename (before replacements) started with any whitespace, an
    underscore `_` is prepended to the result. Strips leading/trailing
    whitespace from the final result (after potential prefixing).

    Args:
        input_filename: The original filename string.

    Returns:
        The cleaned filename string, potentially prefixed with an underscore.
    """
    if not isinstance(input_filename, str):
        return ""  # Handle non-string input gracefully

    # Perform standard illegal character replacement first
    output_filename = input_filename.translate(CHARACTER_TRANSLATION_TABLE)

    # Check the *original* string for leading space before deciding to prefix
    # Use `isspace()` check on the first character for clarity
    if input_filename and input_filename[0].isspace():
        # Prepend underscore and remove leading space(s) from the *translated* string
        result = "_" + output_filename.lstrip()
    else:
        result = output_filename

    # Strip any remaining whitespace (leading or trailing) from the final result
    return result.strip()
-e 
-------- END OF FILE helpers/_clean_name.py --------

-------- START OF FILE helpers/_date.py --------
# pixabit/helpers/_date.py

# SECTION: MODULE DOCSTRING
"""Provides utility functions for date and time manipulation, especially for Habitica timestamps.

Includes functions for handling ISO 8601 timestamps, converting between UTC
and local time, checking if dates are past, and formatting timedeltas.
Requires `python-dateutil` and `tzlocal`.
"""

# SECTION: IMPORTS
from datetime import datetime, timedelta, timezone, tzinfo
from typing import Optional

import dateutil.parser
from dateutil.tz import tzlocal  # Preferred import for local timezone object

# Use themed console/print if available from ._rich
try:
    from ._rich import console

    # No need to redefine print if console.print is used directly
except ImportError:
    import logging  # Use standard logging if rich isn't available

    # Define a basic console object for logging if rich fails
    class FallbackConsole:
        def print(self, *args, style: str = "", **kwargs):
            level = logging.WARNING if style == "warning" else logging.INFO
            logging.log(level, " ".join(map(str, args)))

        def log(self, *args, style: str = "", **kwargs):
            level = logging.WARNING if style == "warning" else logging.INFO
            logging.log(level, " ".join(map(str, args)))

    console = FallbackConsole()
    logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")


# SECTION: FUNCTIONS


def convert_unix_to_utc(timestamp_ms):
    timestamp_s = timestamp_ms / 1000
    dt_utc = datetime.datetime.fromtimestamp(timestamp_s, datetime.timezone.utc)
    return dt_utc


# FUNC: convert_timestamp_to_utc
def convert_timestamp_to_utc(timestamp: str | None) -> datetime | None:
    """Converts an ISO 8601 timestamp string to a timezone-aware datetime object in UTC.

    Handles timestamps with or without timezone information (assumes UTC if naive).
    Also handles potential integer/float timestamps (assumed seconds).

    Args:
        timestamp: The timestamp string (e.g., "2023-10-27T10:00:00.000Z"),
                   a number (seconds since epoch), or None.

    Returns:
        A timezone-aware datetime object in UTC, or None if parsing fails or input is None.
    """
    if timestamp is None:
        return None

    dt_object: datetime | None = None
    try:
        if isinstance(timestamp, (int, float)):
            # Assume seconds since epoch if it's a number
            dt_object = datetime.fromtimestamp(timestamp, tz=timezone.utc)
        elif isinstance(timestamp, str):
            # Use dateutil.parser which handles various ISO 8601 formats including 'Z'
            dt_object = dateutil.parser.isoparse(timestamp)
            # If isoparse results in a naive datetime, assume it was UTC
            if (
                dt_object.tzinfo is None
                or dt_object.tzinfo.utcoffset(dt_object) is None
            ):
                dt_object = dt_object.replace(tzinfo=timezone.utc)
            # Ensure the final object is in UTC timezone
            dt_object = dt_object.astimezone(timezone.utc)
        elif isinstance(timestamp, datetime):
            # If already a datetime object, ensure it's UTC
            dt_object = timestamp
            if dt_object.tzinfo is None:
                dt_object = dt_object.replace(tzinfo=timezone.utc)
            else:
                dt_object = dt_object.astimezone(timezone.utc)
        else:
            # Handle unexpected types
            raise TypeError(
                f"Unsupported timestamp type: {type(timestamp).__name__}"
            )

        return dt_object

    except (ValueError, TypeError, OverflowError) as e:
        console.print(
            f"Error parsing timestamp '{timestamp}' to UTC: {e}",
            style="warning",
        )
        return None


# FUNC: is_date_passed
def is_date_passed(
    timestamp_input: str | datetime | int | float | None,
) -> bool | None:
    """Checks if the date/time represented by the timestamp is in the past.

    Compares the timestamp (converted to UTC) against the current UTC time.

    Args:
        timestamp_input: The timestamp string, datetime object, epoch seconds, or None.

    Returns:
        True if the timestamp is strictly in the past, False if it's now or in the future.
        Returns None if the timestamp is invalid or None.
    """
    utc_time = convert_timestamp_to_utc(timestamp_input)  # type: ignore[arg-type] # Handled inside
    if utc_time is None:
        return None  # Invalid or None timestamp input

    # Get the current time in UTC
    now_utc = datetime.now(timezone.utc)

    # Perform the comparison
    return utc_time < now_utc


# FUNC: get_local_timezone
def get_local_timezone() -> tzinfo:
    """Safely gets the local timezone object using tzlocal."""
    try:
        # tzlocal() is the function to get the tzinfo object
        local_tz = tzlocal()
        if local_tz is None:  # tzlocal *might* return None in rare cases
            raise ValueError("tzlocal returned None")
        return local_tz
    except Exception as e:
        console.print(
            f"Error getting local timezone: {e}. Falling back to UTC.",
            style="warning",
        )
        return timezone.utc  # Fallback safely to UTC


# FUNC: convert_to_local_time
def convert_to_local_time(
    timestamp_input: str | datetime | int | float | None,
) -> datetime | None:
    """Converts a timestamp (UTC or with offset) to a timezone-aware datetime in the local system timezone.

    Args:
        timestamp_input: The timestamp string (ISO 8601), datetime object, epoch seconds, or None.

    Returns:
        A timezone-aware datetime object in the local timezone, with microseconds
        removed for cleaner display, or None on error or None input.
    """
    utc_time = convert_timestamp_to_utc(timestamp_input)  # type: ignore[arg-type] # Handled inside
    if utc_time is None:
        return None  # Handle invalid input from the start

    try:
        local_timezone = get_local_timezone()
        # Convert the UTC datetime object to the local timezone
        local_time = utc_time.astimezone(local_timezone)
        # Remove microseconds for cleaner display
        return local_time.replace(microsecond=0)
    except Exception as e:  # Catch potential errors during timezone conversion
        local_tz_name = getattr(
            local_timezone, "zone", "Unknown"
        )  # Try to get zone name
        console.print(
            f"Error converting UTC time '{utc_time}' to local zone '{local_tz_name}': {e}",
            style="warning",
        )
        return None


# FUNC: format_timedelta
def format_timedelta(delta: timedelta) -> str:
    """Formats a timedelta into a human-readable string like "in 2d 03:15:30" or "1d 10:05:00 ago".

    Args:
        delta: The timedelta object to format.

    Returns:
        A human-readable string representation of the timedelta.
    """
    total_seconds_float = delta.total_seconds()

    if abs(total_seconds_float) < 1:  # Handle very small durations near zero
        return "now"

    is_past = total_seconds_float < 0
    abs_delta = abs(delta)
    total_abs_seconds = int(abs_delta.total_seconds())

    days = total_abs_seconds // (24 * 3600)
    seconds_within_day = total_abs_seconds % (24 * 3600)

    hours, remainder = divmod(seconds_within_day, 3600)
    minutes, seconds = divmod(remainder, 60)

    parts: list[str] = []
    if days > 0:
        parts.append(f"{days}d")

    # Always show HH:MM:SS for the time part, padding with zeros
    parts.append(f"{hours:02}:{minutes:02}:{seconds:02}")

    time_str = " ".join(parts)

    if is_past:
        return f"{time_str} ago"
    else:
        return f"in {time_str}"


# FUNC: format_datetime_with_diff
def format_datetime_with_diff(
    timestamp_input: str | datetime | int | float | None,
) -> str:
    """Converts a timestamp to local time and returns a formatted string indicating time difference.

    Args:
        timestamp_input: The timestamp string (ISO 8601), datetime object, epoch seconds, or None.

    Returns:
        A string like "YYYY-MM-DD HH:MM:SS (in Xd HH:MM:SS)" or "... ago".
        Returns "Invalid Timestamp" or similar on parsing error.
        Returns "N/A" if input is None.
    """
    if timestamp_input is None:
        return "N/A"

    local_time = convert_to_local_time(timestamp_input)
    if local_time is None:
        # Use repr for better debugging of the original input
        return f"Invalid Timestamp ({repr(timestamp_input)})"

    try:
        # Get current time in the *same* local timezone for accurate comparison
        local_timezone = get_local_timezone()  # Get local tz again
        now_local = datetime.now(local_timezone).replace(microsecond=0)

        # Calculate the difference
        time_difference = local_time - now_local

        # Format the difference and the local time
        formatted_diff = format_timedelta(time_difference)
        # Format local time without microseconds
        local_time_str = local_time.strftime("%Y-%m-%d %H:%M:%S")

        return f"{local_time_str} ({formatted_diff})"
    except Exception as e:
        console.print(
            f"Error formatting local time difference for '{timestamp_input}': {e}",
            style="warning",
        )
        # Fallback if formatting fails but conversion worked
        try:
            return (
                local_time.strftime("%Y-%m-%d %H:%M:%S")
                + " (Error formatting diff)"
            )
        except Exception:
            return f"Invalid Timestamp ({repr(timestamp_input)})"  # Fallback if strftime fails too
-e 
-------- END OF FILE helpers/_date.py --------

-------- START OF FILE helpers/_get_json_schema.py --------
import json
from pprint import pprint

# Load your JSON data
with open("content_cache_extracted.json", encoding="utf-8") as f:
    data = json.load(f)

# Access the gear_flat dictionary
gear_flat = data["gear_flat"]

# Initialize empty schema to build up
all_fields = set()
field_counts = {}

# First pass: collect all possible fields and count occurrences
for item_key, item in gear_flat.items():
    for field in item.keys():
        all_fields.add(field)
        field_counts[field] = field_counts.get(field, 0) + 1

# Create schema with type information and required status
schema = {}
total_items = len(gear_flat)

for item_key, item in gear_flat.items():
    for field, value in item.items():
        if field not in schema:
            schema[field] = {"type": type(value).__name__, "required": False}
        elif type(value).__name__ != schema[field]["type"]:
            schema[field][
                "type"
            ] = f"mixed ({schema[field]['type']}, {type(value).__name__})"

# Mark fields as required or optional
for field in all_fields:
    # If field appears in all items, mark as required
    schema[field]["required"] = field_counts.get(field, 0) == total_items

# Sort fields by required status (required first, then optional)
sorted_schema = {
    k: v
    for k, v in sorted(
        schema.items(), key=lambda x: (not x[1]["required"], x[0])
    )
}

# Output the schema with required information
print(f"Schema for equipment items (total items: {total_items}):")
print(
    "Required fields are present in all items, optional fields may be missing in some items."
)
print()

# Display fields grouped
print("REQUIRED FIELDS:")
for field, info in sorted_schema.items():
    if info["required"]:
        print(f"  {field}: {info['type']}")

print("\nOPTIONAL FIELDS:")
for field, info in sorted_schema.items():
    if not info["required"]:
        coverage = field_counts.get(field, 0) / total_items * 100
        print(
            f"  {field}: {info['type']} (present in {field_counts.get(field, 0)}/{total_items} items, {coverage:.1f}%)"
        )

# Generate formal JSON Schema
json_schema = {
    "type": "object",
    "required": [field for field, info in schema.items() if info["required"]],
    "properties": {
        key: {
            "type": (
                "string"
                if info["type"] == "str"
                else (
                    "integer"
                    if info["type"] == "int"
                    else (
                        "number"
                        if info["type"] in ["float", "int"]
                        else "boolean" if info["type"] == "bool" else "object"
                    )
                )
            )
        }
        for key, info in schema.items()
    },
}

print("\nJSON Schema format:")
pprint(json_schema)
import json
from pprint import pprint
from typing import Any, Dict, Set


def analyze_field_types(
    data_dict: dict[str, Any], field_counts: dict[str, int] = None
) -> dict[str, Any]:
    """Recursively analyze a dictionary to build a schema.
    Returns a dictionary with schema information including nested objects.
    """
    schema = {}

    for key, value in data_dict.items():
        if isinstance(value, dict):
            # Recursively analyze nested objects
            schema[key] = {
                "type": "object",
                "properties": analyze_field_types(value),
            }
        elif isinstance(value, list):
            # Handle arrays
            if value and all(isinstance(item, dict) for item in value):
                # If array of objects, analyze the structure of objects
                array_schema = {}
                for item in value:
                    item_schema = analyze_field_types(item)
                    # Merge schemas from different array items
                    for field, field_info in item_schema.items():
                        if field not in array_schema:
                            array_schema[field] = field_info
                schema[key] = {
                    "type": "array",
                    "items": {"type": "object", "properties": array_schema},
                }
            elif value:
                # Determine type of first item for non-empty arrays
                item_type = type(value[0]).__name__
                schema[key] = {"type": "array", "items": {"type": item_type}}
            else:
                # Empty array
                schema[key] = {"type": "array", "items": {"type": "unknown"}}
        else:
            # Simple values
            schema[key] = {"type": type(value).__name__}

    return schema


def determine_required_fields(items: dict[str, Dict]) -> dict[str, Dict]:
    """Determine which fields are required by analyzing multiple dictionary items."""
    all_fields = set()
    field_counts = {}
    total_items = len(items)

    # First collect all possible fields
    for item_key, item in items.items():
        for field in item.keys():
            all_fields.add(field)
            field_counts[field] = field_counts.get(field, 0) + 1

    # Build schema with required information
    schema = {}
    for field in all_fields:
        is_required = field_counts.get(field, 0) == total_items
        schema[field] = {
            "required": is_required,
            "count": field_counts.get(field, 0),
        }

    return schema, total_items


def merge_type_and_required_info(
    type_schema: Dict, required_info: Dict, total: int
) -> Dict:
    """Merge type information with required field information."""
    merged_schema = {}

    for field, type_info in type_schema.items():
        required_data = required_info.get(
            field, {"required": False, "count": 0}
        )

        if isinstance(type_info, dict) and "type" in type_info:
            # Simple field
            merged_schema[field] = {
                **type_info,
                "required": required_data["required"],
                "coverage": f"{required_data['count']}/{total} ({required_data['count']/total*100:.1f}%)",
            }
        else:
            # This field's value is already a complex structure (object/array)
            merged_schema[field] = {
                **type_info,
                "required": required_data["required"],
                "coverage": f"{required_data['count']}/{total} ({required_data['count']/total*100:.1f}%)",
            }

    return merged_schema


# Main process
def get_complete_schema(data):
    # Get the gear items from the structure
    gear_flat = data["gear_flat"]

    # First analyze the field requirements
    required_info, total_items = determine_required_fields(gear_flat)

    # Then analyze first item for type info (including nested objects)
    first_item_key = next(iter(gear_flat))
    first_item = gear_flat[first_item_key]
    type_schema = analyze_field_types(first_item)

    # For each field that is an object, check across other items as well
    for item_key, item in gear_flat.items():
        if item_key == first_item_key:
            continue
        for field, value in item.items():
            if field in type_schema and isinstance(value, dict):
                # If it's not already represented as an object, update it
                if (
                    isinstance(type_schema[field], dict)
                    and type_schema[field].get("type") != "object"
                ):
                    nested_schema = analyze_field_types({field: value})
                    type_schema[field] = nested_schema[field]

    # Merge type and required info
    complete_schema = merge_type_and_required_info(
        type_schema, required_info, total_items
    )

    return complete_schema, total_items


# Load data and get schema
with open("content_Cache_Extracted.json", encoding="utf-8") as f:
    data = json.load(f)

schema, total_items = get_complete_schema(data)

# Format and display results
print(f"Complete schema analysis (total items: {total_items}):")
pprint(schema)
-e 
-------- END OF FILE helpers/_get_json_schema.py --------

-------- START OF FILE helpers/_json.py --------
# pixabit/helpers/_json.py
# ─── Helper ───────────────────────────────────────────────────────────────────
#                JSON Save/Load Utilities
# ──────────────────────────────────────────────────────────────────────────────

# SECTION: MODULE DOCSTRING
"""Provides utility functions for saving and loading Python data to/from JSON files.

Includes pretty printing, UTF-8 encoding, directory creation, and error handling.
Uses the application's configured logger. Supports saving/loading Pydantic models.
"""

# SECTION: IMPORTS
import json
from pathlib import Path
from typing import Any, Type, TypeVar, cast  # Use Type for model classes

# Use | for Union is implied by Python 3.10+ target
from pydantic import BaseModel, ValidationError

# Assume logger is available one level up
try:
    from ._logger import log
except ImportError:
    import logging

    log = logging.getLogger(__name__)
    log.addHandler(logging.NullHandler())

# SECTION: TYPE VARIABLES
T_BaseModel = TypeVar("T_BaseModel", bound=BaseModel)
JSONSerializable = dict[str, Any] | list[Any]  # Consistent typing
LoadResult = JSONSerializable | None

# SECTION: HELPER FUNCTIONS


# FUNC: _resolve_path
def _resolve_path(filepath: str | Path, folder: str | Path | None = None) -> Path:
    """Helper function to resolve the final file path."""
    if folder is not None:
        folder_path = Path(folder).resolve()
        # Extract just the filename from filepath if folder is given
        filename = Path(filepath).name
        return folder_path / filename
    else:
        return Path(filepath).resolve()


# SECTION: CORE FUNCTIONS


# FUNC: save_json
def save_json(
    data: JSONSerializable,
    filepath: str | Path,
    folder: str | Path | None = None,
    indent: int = 4,
    ensure_ascii: bool = False,
) -> bool:
    """Saves Python data (dict or list) to a JSON file with pretty printing.

    Ensures the output directory exists. Handles potential JSON serialization
    errors and file I/O errors, logging messages.

    Args:
        data: The Python dictionary or list to save.
        filepath: The full path (including filename and .json extension) for
                  the output file, or just the filename if folder is specified.
        folder: Optional folder path where the file should be saved.
                If provided, filepath will be treated as just the filename.
        indent: JSON indentation level.
        ensure_ascii: If True, escape non-ASCII characters.

    Returns:
        True if saving was successful, False otherwise.
    """
    output_path = _resolve_path(filepath, folder)
    log.debug(f"Attempting to save JSON data to: '{output_path}'")

    try:
        # Create parent directory(ies) if they don't exist
        output_path.parent.mkdir(parents=True, exist_ok=True)

        # Write the file with UTF-8 encoding and specified indentation
        with output_path.open("w", encoding="utf-8") as f:
            json.dump(
                data, f, indent=indent, ensure_ascii=ensure_ascii, default=str
            )  # Added default=str for non-serializable types like datetime if not handled by caller

        log.info(f"Successfully saved JSON data to: '{output_path}'")
        return True

    except TypeError as e:
        log.error(f"Data structure not JSON serializable for '{output_path}'. Check for non-standard types (like datetime). Error: {e}")
        return False
    except OSError as e:
        log.error(f"Could not write file '{output_path}'. Error: {e}")
        return False
    except Exception as e:
        log.exception(f"An unexpected error occurred saving to '{output_path}': {e}")  # Log full traceback for unexpected errors
        return False


# FUNC: load_json
def load_json(
    filepath: str | Path,
    folder: str | Path | None = None,
) -> LoadResult:
    """Loads data from a JSON file.

    Args:
        filepath: The path to the JSON file, or just the filename if folder is specified.
        folder: Optional folder path where the file is located.
                If provided, filepath will be treated as just the filename.

    Returns:
        The loaded Python dictionary or list, or None if the file doesn't exist,
        cannot be read, or contains invalid JSON.
    """
    input_path = _resolve_path(filepath, folder)

    if not input_path.is_file():
        log.warning(f"JSON file not found at '{input_path}'")
        return None

    log.debug(f"Attempting to load JSON from: '{input_path}'")
    try:
        with input_path.open("r", encoding="utf-8") as f:
            data = json.load(f)

        # Basic validation of loaded data type
        if isinstance(data, (dict, list)):
            log.debug(f"Successfully loaded JSON data from: '{input_path}'")
            return data
        else:
            log.warning(f"Invalid data type ({type(data).__name__}) in JSON file: '{input_path}'. Expected dict or list.")
            return None

    except (OSError, json.JSONDecodeError) as e:
        log.error(f"Failed to load or parse JSON file '{input_path}'. Error: {e}")
        return None
    except Exception as e:
        log.exception(f"An unexpected error occurred loading '{input_path}': {e}")  # Log full traceback for unexpected errors
        return None


# SECTION: PYDANTIC INTEGRATION


# FUNC: save_pydantic_model
def save_pydantic_model(
    model: BaseModel,
    filepath: str | Path,
    folder: str | Path | None = None,
    exclude_none: bool = True,
    indent: int = 4,
) -> bool:
    """Saves a Pydantic V2 model to a JSON file using model_dump.

    Args:
        model: The Pydantic model instance to save.
        filepath: The path where the JSON file will be saved, or just the filename
                  if folder is specified.
        folder: Optional folder path where the file should be saved.
        exclude_none: Whether to exclude fields with None values from the output.
        indent: JSON indentation level.

    Returns:
        True if saving was successful, False otherwise.
    """
    if not isinstance(model, BaseModel):
        log.error("Invalid input: 'model' must be a Pydantic BaseModel instance.")
        return False

    output_path = _resolve_path(filepath, folder)
    log.debug(f"Attempting to save Pydantic model {type(model).__name__} to JSON: '{output_path}'")

    try:
        # Use model_dump_json for direct JSON string output
        json_str = model.model_dump_json(exclude_none=exclude_none, indent=indent)

        # Create parent directory(ies) if they don't exist
        output_path.parent.mkdir(parents=True, exist_ok=True)

        # Write the JSON string to the file
        with output_path.open("w", encoding="utf-8") as f:
            f.write(json_str)

        log.info(f"Successfully saved Pydantic model to: '{output_path}'")
        return True

    except ValidationError as e:  # Should not happen during dump, but belt-and-suspenders
        log.error(f"Pydantic validation error during model dump for '{output_path}'. Error: {e}")
        return False
    except TypeError as e:
        log.error(f"Data structure from model dump not JSON serializable for '{output_path}'. Error: {e}")
        return False
    except OSError as e:
        log.error(f"Could not write file '{output_path}'. Error: {e}")
        return False
    except Exception as e:
        log.exception(f"An unexpected error occurred saving Pydantic model to '{output_path}': {e}")  # Log full traceback for unexpected errors
        return False


# FUNC: load_pydantic_model
def load_pydantic_model(
    model_class: Type[T_BaseModel],
    filepath: str | Path,
    folder: str | Path | None = None,
    context: dict[str, Any] | None = None,  # Added context parameter
) -> T_BaseModel | None:
    """Loads a JSON file into a Pydantic V2 model instance using model_validate.

    Args:
        model_class: The Pydantic model class (e.g., User, Task).
        filepath: The path to the JSON file, or just the filename if folder is specified.
        folder: Optional folder path where the file is located.
        context: Optional dictionary context to pass to `model_validate`.

    Returns:
        An instance of the model_class populated with data if successful, None otherwise.
    """
    data = load_json(filepath, folder=folder)
    if data is None:
        # load_json already logged the reason (not found or parse error)
        return None

    input_path = _resolve_path(filepath, folder)
    if not isinstance(data, dict):
        log.warning(f"JSON data loaded from '{input_path}' is not a dictionary, cannot parse into {model_class.__name__}.")
        return None

    log.debug(f"Attempting to validate JSON into Pydantic model {model_class.__name__} from '{input_path}'")
    try:
        # Use model_validate for Pydantic V2
        instance = model_class.model_validate(data, context=context)  # Pass context here

        log.debug(f"Successfully validated JSON into Pydantic model {model_class.__name__}")
        return instance

    except ValidationError as e:
        log.error(f"Pydantic validation failed for {model_class.__name__} from '{input_path}':\n{e}")
        return None
    except Exception as e:
        log.exception(f"An unexpected error occurred parsing JSON into Pydantic model {model_class.__name__} from '{input_path}': {e}")
        return None


# ──────────────────────────────────────────────────────────────────────────────
-e 
-------- END OF FILE helpers/_json.py --------

-------- START OF FILE helpers/_logger.py --------
import logging
import sys
from logging.handlers import RotatingFileHandler
from pathlib import Path

from textual.logging import TextualHandler

# --- Constants ---
LOG_FILENAME = "app.log"
LOG_FORMAT_FILE = "%(asctime)s - %(filename)s:%(lineno)d - %(levelname)s - %(message)s"
SUCCESS_LEVEL_NUM = 25

# --- Custom Success Level ---
logging.addLevelName(SUCCESS_LEVEL_NUM, "SUCCESS")


def success(self, message, *args, **kws):
    if self.isEnabledFor(SUCCESS_LEVEL_NUM):
        self._log(SUCCESS_LEVEL_NUM, message, args, **kws)


logging.Logger.success = success


def setup_logging(
    log_level: int = logging.DEBUG,
    logger_name: str = "Pixabit",
    log_dir: Path = None,
) -> logging.Logger:
    """Configura logging con TextualHandler para la consola de Textual"""
    # Crear directorio de logs si es necesario
    if log_dir:
        log_dir.mkdir(parents=True, exist_ok=True)
        log_path = log_dir / LOG_FILENAME
    else:
        log_path = LOG_FILENAME

    # Get the logger
    log = logging.getLogger(logger_name)
    log.setLevel(log_level)

    # Clear existing handlers to prevent duplicates
    if log.hasHandlers():
        log.handlers.clear()

    # Usar TextualHandler para la consola de Textual
    textual_handler = TextualHandler()
    textual_handler.setLevel(logging.WARNING)
    log.addHandler(textual_handler)

    # File handler para logs en archivo
    file_handler = RotatingFileHandler(log_path, maxBytes=10 * 1024 * 1024, backupCount=5, encoding="utf-8")
    file_handler.setLevel(logging.DEBUG)
    file_formatter = logging.Formatter(LOG_FORMAT_FILE)
    file_handler.setFormatter(file_formatter)
    log.addHandler(file_handler)

    # Configure root logger to filter messages from other modules
    root_logger = logging.getLogger()
    if not root_logger.handlers:
        # Add a null handler to avoid "No handler found" warnings
        root_logger.addHandler(logging.NullHandler())

    # Disable propagation to root logger
    log.propagate = False

    return log


# Instancia singleton del logger (usar esta en toda la aplicación)
_log_instance = None


def get_logger():
    global _log_instance
    if _log_instance is None:
        _log_instance = setup_logging(log_level=logging.DEBUG, logger_name="Pixabit")
    return _log_instance


# Obtener la instancia del logger
log = get_logger()


# Configurar loggers de otras bibliotecas para prevenir mensajes no deseados
def configure_third_party_loggers():
    """Configura loggers de terceros para controlar su nivel de detalle"""
    # Lista de loggers de terceros que quieres silenciar o configurar
    third_party_loggers = [
        "urllib3",
        "requests",
        # Añade aquí otros módulos que generen logs no deseados
    ]

    for logger_name in third_party_loggers:
        logger = logging.getLogger(logger_name)
        logger.setLevel(logging.WARNING)  # Solo muestra warnings y errores
        logger.propagate = False  # No propaga a loggers padres


# Ejecutar configuración de loggers de terceros
configure_third_party_loggers()
-e 
-------- END OF FILE helpers/_logger.py --------

-------- START OF FILE helpers/_md_to_rich.py --------
# pixabit/helpers/_md_to_rich.py

# SECTION: MODULE DOCSTRING
"""Provides utilities for converting Markdown to Rich Text objects.

Uses markdown-it-py for parsing and Rich for constructing styled text.
Includes default styling and allows for custom style overrides.
Optionally integrates with Textual via a MarkdownStatic widget.

NOTE: This renderer provides fine-grained control but might be complex.
      Consider using Rich's built-in `Markdown` class if simpler rendering suffices.
"""

# SECTION: IMPORTS
import re
from typing import Any, Dict, List, Optional, Sequence, TypeVar, Union, cast

# Markdown parsing
try:
    import markdown_it
    from markdown_it.token import Token

    MARKDOWN_IT_AVAILABLE = True
except ImportError:
    MARKDOWN_IT_AVAILABLE = False
    Token = TypeVar("Token")  # Dummy type if not available

# Rich rendering
from rich.console import Console, ConsoleOptions, RenderResult
from rich.panel import Panel
from rich.style import Style, StyleType
from rich.text import Text, TextType

# Textual integration (optional import)
try:
    from textual.reactive import reactive  # Import reactive specifically
    from textual.widget import Widget
    from textual.widgets import Static

    TEXTUAL_AVAILABLE = True
except ImportError:
    TEXTUAL_AVAILABLE = False
    Widget = object  # Dummy base class
    Static = object  # Dummy base class

# Use themed console if available from ._rich
try:
    from ._rich import console
except ImportError:
    # Basic console fallback
    class PrintConsole:
        def print(self, *args, **kwargs):
            print(*args)

    console = PrintConsole()  # type: ignore


# SECTION: UTILITY FUNCTIONS


# FUNC: escape_rich
def escape_rich(text: str) -> str:
    """Escape Rich markup control characters (square brackets) in text.

    Args:
        text: The text to escape.

    Returns:
        String with Rich control characters escaped. Returns empty string if input is None or empty.
    """
    if not text:
        return ""
    # Escape square brackets which Rich uses for markup
    return text.replace("[", r"\[").replace("]", r"\]")


# SECTION: MARKDOWN RENDERER CLASS


# KLASS: MarkdownRenderer
class MarkdownRenderer:
    """Converts Markdown to Rich Text with custom styling using markdown-it-py.

    Attributes:
        DEFAULT_STYLES: Class variable holding default styles for elements.
        md_parser: The markdown-it parser instance.
        styles: The effective styles (defaults merged with custom).
    """

    # Default styles for different Markdown elements
    DEFAULT_STYLES: dict[str, Style] = {
        "h1": Style(bold=True, color="cyan", underline=True),
        "h2": Style(bold=True, color="bright_cyan"),
        "h3": Style(bold=True, color="blue"),
        "h4": Style(underline=True, color="bright_blue"),
        "h5": Style(italic=True, color="blue"),
        "h6": Style(italic=True, dim=True),
        "strong": Style(bold=True),
        "em": Style(italic=True),
        "code_inline": Style(
            bgcolor="#303030", color="#00ff00"
        ),  # Dark bg, green text
        "code_block": Style(dim=True),  # Dim background for blocks
        "strike": Style(strike=True),
        "link": Style(underline=True, color="bright_blue"),
        "list_item": Style(),  # Base style for list items (prefix added separately)
        "blockquote": Style(italic=True, color="green"),
        "hr": Style(color="bright_black", dim=True),
        "table": Style(),  # Placeholder for potential table styling
        "checkbox_unchecked": Style(),
        "checkbox_checked": Style(dim=True),  # Dim completed checklist items
    }

    # FUNC: __init__
    def __init__(self, custom_styles: dict[str, Style] | None = None):
        """Initialize the Markdown renderer.

        Args:
            custom_styles: Dictionary mapping element names (e.g., 'h1', 'link')
                           to Rich Style objects. These override defaults.
        """
        if not MARKDOWN_IT_AVAILABLE:
            raise ImportError(
                "MarkdownRenderer requires 'markdown-it-py' to be installed."
            )

        # Create markdown-it parser with extensions
        # Enable strikethrough ('strikethrough') and tables ('table')
        self.md_parser = (
            markdown_it.MarkdownIt(
                "commonmark", {"breaks": True, "html": False}
            ).enable(  # Disable raw HTML
                "strikethrough"
            )
            # .enable("table") # Uncomment if table rendering is needed
        )

        # Merge default styles with any custom styles
        self.styles = self.DEFAULT_STYLES.copy()
        if custom_styles:
            self.styles.update(custom_styles)

    # --- Style Application Helpers ---

    # FUNC: _apply_style
    def _apply_style(self, current_style: Style, style_key: str) -> Style:
        """Applies a named style on top of the current style."""
        style_to_add = self.styles.get(style_key)
        if style_to_add:
            # Combine styles: attributes from style_to_add override current_style
            return current_style + style_to_add
        return current_style

    # FUNC: _remove_style
    def _remove_style(self, current_style: Style, style_key: str) -> Style:
        """Removes attributes defined in a named style from the current style.

        This is tricky. We approximate by creating a style with only the
        attributes from the target style set to None or default.
        A more robust way might involve tracking active style keys.
        For now, we'll just reset to parent style conceptually.
        """
        # This simple approach doesn't truly "remove" overlapping styles.
        # It relies on the parent context having the correct base style.
        # For common cases like bold/italic, it works if nesting is handled.
        return current_style  # Placeholder - relies on context stack in _process_tokens

    # --- Core Conversion Method ---

    # FUNC: markdown_to_rich_text
    def markdown_to_rich_text(self, markdown_str: str) -> Text:
        """Convert Markdown string to a Rich Text object using markdown-it tokens.

        Args:
            markdown_str: Markdown-formatted string.

        Returns:
            Rich Text object with appropriate styling.
        """
        if not markdown_str:
            return Text()

        # Parse the markdown
        tokens = self.md_parser.parse(markdown_str)

        # Create a Text object for the result
        result = Text()
        # Process the tokens recursively, starting with an empty style stack
        self._process_tokens(tokens, result, style_stack=[Style()])

        return result

    # --- Token Processing Logic ---

    # FUNC: _process_tokens
    def _process_tokens(
        self,
        tokens: Sequence[Token],
        text_obj: Text,
        style_stack: list[Style],  # Stack to manage nested styles
    ) -> None:
        """Process markdown-it tokens recursively and apply styles.

        Args:
            tokens: List of markdown-it tokens to process.
            text_obj: Rich Text object to append styled content to.
            style_stack: A list representing the stack of active styles.
                         The effective style is the top of the stack.
        """
        i = 0
        while i < len(tokens):
            token = tokens[i]
            current_style = style_stack[-1]  # Get the currently active style

            # Handle inline tokens with children recursively
            if token.type == "inline" and token.children:
                self._process_tokens(token.children, text_obj, style_stack)
                i += 1
                continue

            # --- Block Element Open/Close ---
            elif token.type.endswith("_open"):
                new_style = current_style  # Default to inheriting style
                style_key = ""
                prefix = ""
                needs_newline = False

                if token.type == "heading_open":
                    level = int(token.tag[1])
                    style_key = f"h{level}"
                    needs_newline = True  # Add newline before heading starts
                elif token.type == "strong_open":
                    style_key = "strong"
                elif token.type == "em_open":
                    style_key = "em"
                elif token.type == "s_open":
                    style_key = "strike"  # Strikethrough
                elif token.type == "link_open":
                    style_key = "link"
                    href = token.attrs.get("href", "") if token.attrs else ""
                    # Apply link attribute directly to the style
                    new_style = self._apply_style(
                        current_style, style_key
                    ).update_link(href or None)
                    style_stack.append(
                        new_style
                    )  # Push specialized style with link
                    i += 1
                    continue  # Skip default style push
                elif token.type == "blockquote_open":
                    style_key = "blockquote"
                    prefix = "> "
                    needs_newline = True
                elif token.type == "bullet_list_open":
                    style_key = "list_item"  # Apply base list style
                elif token.type == "list_item_open":
                    # Handle prefix and checkbox within list_item processing
                    style_key = "list_item"  # Inherit list item style
                elif (
                    token.type == "code_block" or token.type == "fence"
                ):  # Treat as single token
                    style_key = "code_block"
                    # Append code directly, don't push style for children
                    if text_obj and not text_obj.plain.endswith("\n"):
                        text_obj.append("\n")
                    text_obj.append(
                        token.content.rstrip(),
                        self.styles.get(style_key, Style()),
                    )
                    text_obj.append("\n")
                    i += 1
                    continue

                # Apply the style for the opening tag
                if style_key:
                    new_style = self._apply_style(current_style, style_key)

                if (
                    needs_newline
                    and text_obj
                    and not text_obj.plain.endswith("\n")
                ):
                    text_obj.append("\n")
                if prefix:
                    text_obj.append(
                        prefix, new_style
                    )  # Apply style to prefix too

                style_stack.append(new_style)  # Push the new style context

            # --- Block Element Close ---
            elif token.type.endswith("_close"):
                if len(style_stack) > 1:  # Don't pop the base style
                    style_stack.pop()

                # Add spacing after certain block elements
                if token.type in (
                    "paragraph_close",
                    "blockquote_close",
                    "heading_close",
                    "list_item_close",
                ):
                    # Avoid double newlines if already present
                    if text_obj and not text_obj.plain.endswith("\n\n"):
                        if text_obj.plain.endswith("\n"):
                            text_obj.append("\n")
                        else:
                            text_obj.append("\n\n")

            # --- Specific Inline Elements ---
            elif token.type == "text":
                # Handle checkboxes within list items here
                parent_token_type = tokens[i - 1].type if i > 0 else ""
                content = token.content
                item_prefix = ""
                text_style = current_style

                if parent_token_type == "list_item_open":
                    # Default bullet
                    item_prefix = "• "
                    stripped_content = content.lstrip()
                    # Check for GFM checkboxes [ ] or [x]
                    if stripped_content.startswith("[ ] "):
                        item_prefix = "☐ "
                        content = content.lstrip()[3:]  # Remove checkbox markup
                        text_style = self._apply_style(
                            current_style, "checkbox_unchecked"
                        )
                    elif stripped_content.lower().startswith("[x] "):
                        item_prefix = "☑ "
                        content = content.lstrip()[3:]  # Remove checkbox markup
                        text_style = self._apply_style(
                            current_style, "checkbox_checked"
                        )

                    # Add prefix with list item style, then text with potentially updated style
                    text_obj.append(
                        item_prefix, self.styles.get("list_item", Style())
                    )
                    text_obj.append(content, text_style)

                else:
                    # Regular text, apply current style from stack top
                    text_obj.append(content, current_style)

            elif token.type == "code_inline":
                text_obj.append(
                    token.content, self.styles.get("code_inline", Style())
                )

            elif token.type == "softbreak":
                # CommonMark soft breaks are rendered as spaces or newlines depending on context
                # For simple Rich text, often a space is sufficient unless 'breaks' option forces newline
                text_obj.append(
                    " "
                )  # Or "\n" if md_parser.options['breaks'] is True
            elif token.type == "hardbreak":
                text_obj.append("\n")

            elif token.type == "hr":
                # Add newline if needed before hr
                if text_obj and not text_obj.plain.endswith("\n"):
                    text_obj.append("\n")
                # Use Theme style or default
                text_obj.append(
                    "─" * console.width, self.styles.get("hr", Style())
                )
                text_obj.append("\n\n")

            # --- Tables (Basic - requires 'table' extension enabled) ---
            # elif token.type == "table_open": ... handle table start ...
            # elif token.type == "thead_open": ... handle header start ...
            # elif token.type == "tr_open": ... handle row start ...
            # elif token.type == "th_open": ... handle header cell start ...
            # elif token.type == "td_open": ... handle data cell start ...
            # Table rendering to Rich Text is complex, often better handled by Rich's Table object.

            # Increment token index
            i += 1

    # --- Convenience Rendering Methods ---

    # FUNC: render_to_console
    def render_to_console(
        self, markdown_str: str, target_console: Console | None = None
    ) -> None:
        """Render markdown directly to a Rich console.

        Args:
            markdown_str: Markdown-formatted string.
            target_console: Optional console instance (uses imported `console` if None).
        """
        con = target_console or console
        rich_text = self.markdown_to_rich_text(markdown_str)
        con.print(rich_text)

    # FUNC: render_to_panel
    def render_to_panel(
        self, markdown_str: str, title: str | None = None, **panel_kwargs: Any
    ) -> Panel:
        """Render markdown inside a Rich panel.

        Args:
            markdown_str: Markdown-formatted string.
            title: Optional title for the panel.
            **panel_kwargs: Additional keyword arguments for rich.panel.Panel.

        Returns:
            Rich Panel containing the rendered markdown.
        """
        rich_text = self.markdown_to_rich_text(markdown_str)
        return Panel(rich_text, title=title, **panel_kwargs)


# SECTION: TEXTUAL INTEGRATION (Optional)

if TEXTUAL_AVAILABLE and MARKDOWN_IT_AVAILABLE:

    # KLASS: MarkdownStatic
    class MarkdownStatic(Static):
        """A Textual Static widget that renders Markdown content using MarkdownRenderer."""

        # Define a reactive property for markdown content
        markdown = reactive("", layout=True)

        # FUNC: __init__
        def __init__(
            self,
            markdown: str = "",
            renderer: MarkdownRenderer | None = None,
            *args: Any,
            **kwargs: Any,
        ):
            """Initialize the markdown static widget.

            Args:
                markdown: Initial Markdown content to render.
                renderer: Optional custom MarkdownRenderer instance. Defaults to a new instance.
                *args, **kwargs: Additional arguments for the Static widget.
            """
            super().__init__(
                "", *args, **kwargs
            )  # Initialize Static with empty string
            self._renderer = renderer or MarkdownRenderer()
            # Set reactive property AFTER super init, which triggers the render
            self.markdown = markdown

        # FUNC: render (Override Static's render or watch the reactive property)
        # Watching the reactive property is generally preferred in Textual >0.10
        def watch_markdown(self, new_markdown: str) -> None:
            """Called when the 'markdown' reactive property changes."""
            rich_text = self._renderer.markdown_to_rich_text(new_markdown)
            self.update(rich_text)

        # Optional: Method to update content programmatically
        # FUNC: set_markdown (Alternative way to update)
        def set_markdown(self, markdown: str) -> None:
            """Update the widget with new markdown content."""
            self.markdown = markdown  # Setting the reactive property triggers watch_markdown


# SECTION: EXPORTS
__all__ = [
    "MarkdownRenderer",
    "escape_rich",
]

# Add Textual integration to exports if available
if TEXTUAL_AVAILABLE and MARKDOWN_IT_AVAILABLE:
    __all__.append("MarkdownStatic")
-e 
-------- END OF FILE helpers/_md_to_rich.py --------

-------- START OF FILE helpers/_pydantic.py --------
# pixabit/helpers/_pydantic.py

# SECTION: MODULE DOCSTRING
"""Helper utilities for working consistently with Pydantic models.

Provides a configured base model, common validators (if any added later),
enums, version detection, and utility functions compatible with Pydantic V1 & V2.
"""

# SECTION: IMPORTS
from datetime import datetime  # Keep if used by models or future validators
from enum import Enum
from pathlib import Path
from typing import (  # Use | in Python 3.10+
    Any,
    Generic,
    Type,
    TypeVar,
    Union,
    cast,
    get_type_hints,  # Keep if introspection needed later
)

# Import appropriate Pydantic version and handle potential ImportError
# SECTION: PYDANTIC VERSION HANDLING
try:
    # Pydantic V2+
    from pydantic import (
        BaseModel as PydanticBaseModel,  # Alias to avoid potential conflicts
    )
    from pydantic import (
        ConfigDict,
        Field,
        ValidationError,
        create_model,
        field_validator,
        model_validator,
        v1,  # Import v1 compatibility layer if needed elsewhere
    )
    from pydantic.json import pydantic_encoder  # V2 compatible

    PYDANTIC_V2 = True
    BaseModel = PydanticBaseModel  # Use the alias

except ImportError:
    # Pydantic V1 fallback
    from pydantic import (
        BaseConfig,  # V1 config class
        Field,
        ValidationError,
        create_model,
        validator,  # V1 validator decorator
    )
    from pydantic import BaseModel as PydanticBaseModel  # Alias
    from pydantic.json import pydantic_encoder  # V1 compatible

    PYDANTIC_V2 = False
    BaseModel = PydanticBaseModel  # Use the alias
    # Define ConfigDict for V1 compatibility if needed by type hints elsewhere
    ConfigDict = dict[str, Any]  # Simple dict type for V1 compatibility


# SECTION: BASE MODEL CONFIGURATION


# KLASS: PixabitBaseModel
class PixabitBaseModel(BaseModel):
    """Base model with consistent configuration for all Pixabit models."""

    if PYDANTIC_V2:
        model_config = ConfigDict(
            arbitrary_types_allowed=True,  # Allow complex types if needed
            extra="ignore",  # Ignore extra fields during parsing
            validate_assignment=True,  # Validate fields when assigned after init
            populate_by_name=True,  # Allow population by field name OR alias
            use_enum_values=True,  # Use enum values by default in serialization
        )
    else:  # Pydantic V1 Config

        class Config(BaseConfig):
            arbitrary_types_allowed = True
            extra = "ignore"  # V1 equivalent: Extra.ignore
            validate_assignment = True
            allow_population_by_field_name = (
                True  # V1 equivalent of populate_by_name
            )
            use_enum_values = True

    # Removed model_dump_safe as standard model_dump/dict() covers exclude


# SECTION: TYPE VARIABLES
T = TypeVar("T", bound=BaseModel)  # Generic type for model operations


# SECTION: COMMON VALIDATORS / UTILITIES


# FUNC: ensure_path (Example validator - keep or remove if unused)
def ensure_path(v: str | Path | Any) -> Path:
    """Converts a string to a Path object if necessary."""
    if isinstance(v, Path):
        return v
    if isinstance(v, str):
        return Path(v)
    # Raise error or return a default if type is wrong?
    raise TypeError("Value must be a string or Path object to ensure Path.")


# Example usage in a model:
# class MyModel(PixabitBaseModel):
#     file_path: Path
#     _validate_path = field_validator("file_path", mode="before")(ensure_path) if PYDANTIC_V2 else validator("file_path", pre=True, allow_reuse=True)(ensure_path)


# SECTION: COMMON ENUMS


# ENUM: StatusEnum
class StatusEnum(str, Enum):
    """Common status values used across models."""

    PENDING = "pending"
    ACTIVE = "active"
    COMPLETED = "completed"
    ERROR = "error"
    UNKNOWN = "unknown"  # Added unknown state


# SECTION: HELPER FUNCTIONS


# FUNC: create_from_dict
def create_from_dict(model_class: Type[T], data: dict[str, Any]) -> T:
    """Creates a Pydantic model instance from a dictionary with validation.

    Handles version differences between Pydantic V1 (parse_obj) and V2+ (model_validate).

    Args:
        model_class: The Pydantic model class (e.g., User, Task).
        data: The dictionary containing data to populate the model.

    Returns:
        An instance of model_class populated with data.

    Raises:
        ValidationError: If the data fails validation against the model.
        TypeError: If 'data' is not a dictionary.
    """
    if not isinstance(data, dict):
        raise TypeError(
            f"Input data must be a dictionary to create {model_class.__name__}"
        )
    try:
        if PYDANTIC_V2:
            # Pydantic V2+ uses model_validate
            return model_class.model_validate(data)
        else:
            # Pydantic V1 uses parse_obj
            return model_class.parse_obj(data)  # type: ignore[attr-defined]
    except ValidationError as e:
        # Customize the error message slightly if desired
        # print(f"Validation Error for {model_class.__name__}: {e}") # Debug print
        raise e  # Re-raise the original validation error
    except Exception as e:
        # Catch other potential errors during instantiation
        raise TypeError(f"Error creating {model_class.__name__}: {e}") from e


# FUNC: model_to_dict
def model_to_dict(
    model: BaseModel,
    exclude_none: bool = True,
    by_alias: bool = False,
    **kwargs: Any,
) -> dict[str, Any]:
    """Converts a Pydantic model instance to a dictionary.

    Handles version differences between Pydantic V1 (dict) and V2+ (model_dump).

    Args:
        model: The Pydantic model instance.
        exclude_none: Whether to exclude fields with None values.
        by_alias: Whether to use field aliases in the output dictionary keys.
        **kwargs: Additional arguments passed to dict() or model_dump().

    Returns:
        A dictionary representation of the model.

    Raises:
        TypeError: If 'model' is not a Pydantic BaseModel instance.
    """
    if not isinstance(model, BaseModel):
        raise TypeError("Input must be a Pydantic BaseModel instance.")
    try:
        if PYDANTIC_V2:
            # Pydantic V2+ uses model_dump
            return model.model_dump(
                exclude_none=exclude_none, by_alias=by_alias, **kwargs
            )
        else:
            # Pydantic V1 uses dict
            # Note: V1's dict() doesn't have all the same kwargs as V2's model_dump
            return model.dict(  # type: ignore[attr-defined]
                exclude_none=exclude_none, by_alias=by_alias, **kwargs
            )
    except Exception as e:
        raise TypeError(
            f"Error converting model {type(model).__name__} to dict: {e}"
        ) from e


# SECTION: EXPORTS
__all__ = [
    # Pydantic Core Re-exports (for convenience)
    "BaseModel",
    "Field",
    "ValidationError",
    "create_model",
    # Custom Base Model
    "PixabitBaseModel",
    # Version detection and specific decorators/types
    "PYDANTIC_V2",
    "ConfigDict",  # Available for both V1 (as dict) and V2
    *(["model_validator", "field_validator"] if PYDANTIC_V2 else ["validator"]),
    # Common Utilities / Enums
    "ensure_path",  # Example validator
    "StatusEnum",
    "create_from_dict",
    "model_to_dict",
    "pydantic_encoder",  # JSON encoder helper
]
-e 
-------- END OF FILE helpers/_pydantic.py --------

-------- START OF FILE helpers/_repr.py --------
# pixabit/helpers/_repr.py

# SECTION: MODULE DOCSTRING
"""Provides a function to generate a generic, readable string representation for objects.

Includes the class name and non-private attributes with their values,
useful for debugging and simple object inspection.
"""

# SECTION: IMPORTS
import inspect
from typing import (  # Keep get_type_hints if potentially useful
    Any,
    get_type_hints,
)

# SECTION: FUNCTION


# FUNC: generic_repr
def generic_repr(obj: Any) -> str:
    """Generates a string representation of an object for debugging.

    Includes the class name and non-private/non-callable attributes.
    Limits the representation length of values for readability.

    Args:
        obj: The object to represent.

    Returns:
        A string in the format "ClassName(attr1=value1, attr2='value2', ...)"
        or "None" if the object is None.
    """
    if obj is None:
        return "None"

    class_name = obj.__class__.__name__
    attributes: list[str] = []
    max_repr_len = 50  # Max length for value representation

    # Inspect members of the object
    for name, value in inspect.getmembers(obj):
        # Skip private attributes (starting with '_')
        # Skip methods and functions
        if name.startswith("_") or inspect.isroutine(value):
            # isroutine covers methods, functions, built-in functions, etc.
            continue

        # Represent the value, handling potential errors and limiting length
        try:
            if isinstance(value, (list, tuple, dict, set)) and len(value) > 5:
                # Show length for long collections instead of full repr
                value_repr = f"{type(value).__name__}[len={len(value)}]"
            else:
                value_repr = repr(value)

            # Truncate long representations
            if len(value_repr) > max_repr_len:
                value_repr = value_repr[:max_repr_len] + "..."

            attributes.append(f"{name}={value_repr}")

        except Exception:
            # Fallback if getting repr fails
            attributes.append(f"{name}=<Error>")

    # Assemble the final string
    attributes_str = ", ".join(attributes)
    return f"{class_name}({attributes_str})"


# Example Usage (can be removed or kept for testing)
if __name__ == "__main__":

    class Sample:
        def __init__(self):
            self.public_attr = 123
            self.another = "hello world this is a long string"
            self._private_attr = 456
            self.list_attr = [1, 2, 3, 4, 5, 6, 7]
            self.dict_attr = {"a": 1, "b": 2}

        def method(self):
            pass

    s = Sample()
    print(generic_repr(s))
    print(generic_repr(None))
    print(generic_repr([1, 2, 3]))
-e 
-------- END OF FILE helpers/_repr.py --------

-------- START OF FILE helpers/_rich.py --------
# pixabit/helpers/_rich.py

# SECTION: MODULE DOCSTRING
"""Initializes and configures the Rich Console instance with a custom theme.

Loads theme definitions from a specified file ('styles' by default), handles
potential loading errors by falling back to a default theme. Exports the
configured 'console' instance and commonly used Rich components for consistent
UI elements across the application. Also installs Rich tracebacks.
"""

# SECTION: IMPORTS
import builtins
import logging  # Use standard logging for internal messages here
import sys
from pathlib import Path
from typing import Any, Optional, TextIO  # Added TextIO

# Rich Imports
from rich import box
from rich.columns import Columns
from rich.console import Console, ConsoleRenderable
from rich.highlighter import ReprHighlighter  # Useful for debugging
from rich.layout import Layout
from rich.live import Live
from rich.logging import RichHandler
from rich.markdown import Markdown
from rich.panel import Panel
from rich.progress import (
    BarColumn,
    Progress,
    SpinnerColumn,
    TaskProgressColumn,
    TextColumn,
    TimeElapsedColumn,
    TimeRemainingColumn,
    track,
)
from rich.prompt import Confirm, IntPrompt, Prompt
from rich.rule import Rule
from rich.syntax import Syntax
from rich.table import Table
from rich.text import Text
from rich.theme import Theme, ThemeStack  # Import ThemeStack for fallback
from rich.traceback import install as install_traceback

# Optional art import
try:
    from art import text2art

    ART_AVAILABLE = True
except ImportError:
    ART_AVAILABLE = False

    # Define dummy function if art not installed
    def text2art(*args: Any, **kwargs: Any) -> str:
        """Dummy text2art function when 'art' library is not available."""
        logging.warning("'art' library not found, ASCII art disabled.")
        # Return the first arg if it's the text, otherwise a default
        return str(args[0]) if args else "Pixabit"


# SECTION: PATHS AND DEFAULTS

# Define theme filename
THEME_FILENAME = "styles.theme"  # Use a descriptive extension

# Determine base directory more robustly
# Assumes this file is in pixabit/helpers/
try:
    _current_file_path = Path(__file__).resolve()
    # Go up two levels from pixabit/helpers/_rich.py to project root
    _project_root = _current_file_path.parent.parent.parent
    # Look for theme file in project_root/themes/ or project_root/
    _theme_search_paths = [
        _project_root / "themes" / THEME_FILENAME,
        _project_root / THEME_FILENAME,
        _current_file_path.parent / THEME_FILENAME,  # Fallback next to this file
    ]
except NameError:  # Fallback if __file__ is not defined (e.g., interactive)
    _project_root = Path.cwd()
    _theme_search_paths = [
        Path("themes") / THEME_FILENAME,
        Path(THEME_FILENAME),
    ]

# Find the first existing theme file
theme_file_path: Path | None = next((p for p in _theme_search_paths if p.is_file()), None)


# Default theme dictionary (simplified fallback based on names in styles file)
DEFAULT_THEME_DICT = {
    "bar.complete": "#a6e3a1",
    "bar.finished": "#74c7ec",
    "bar.pulse": "#f5c2e7",
    "danger": "bold #eb6f92 on #e0def4",
    "debug": "dim #908caa",
    "dim": "dim",
    "error": "bold #f38ba8",
    "field": "dim",
    "file": "underline #b4bdf8",
    "highlight": "bold #eb6f92",
    "info": "bold #cba6f7",
    "keyword": "bold #c4a7e7",
    "link_style": "underline #89b4fa",
    "log.level.debug": "dim #908caa",
    "log.level.error": "bold #f38ba8",
    "log.level.info": "#cba6f7",
    "log.level.warning": "bold #f6c177",
    "progress.description": "#e0def4",
    "progress.percentage": "#9ccfd8",
    "progress.remaining": "#f6c177",
    "prompt.choices": "#94e2d5",
    "prompt.default": "#7f849c",
    "prompt.invalid": "#f38ba8",
    "regular": "default",
    "repr.bool_false": "bold #f38ba8",
    "repr.bool_true": "bold #a6e3a1",
    "repr.none": "dim #908caa",
    "repr.number": "#fab387",
    "repr.str": "#a6e3a1",
    "repr.url": "underline #89b4fa",
    "rp_foam": "#9ccfd8",
    "rp_gold": "#f6c177",
    "rp_iris": "#c4a7e7",
    "rp_love": "#eb6f92",
    "rp_muted": "#6e6a86",
    "rp_overlay": "#26233a",
    "rp_pine": "#31748f",
    "rp_rose": "#ebbcba",
    "rp_subtle_color": "#908caa",
    "rp_surface": "#1f1d2e",
    "rp_text": "#e0def4",
    "rule.line": "#45475A",
    "subtle": "dim #908caa",
    "success": "bold #a6e3a1",
    "table.header": "bold #cba6f7",
    "warning": "bold #f6c177",
}

# SECTION: THEME LOADING AND CONSOLE INITIALIZATION

# Initialize console with fallback theme stack first
_fallback_theme = Theme(DEFAULT_THEME_DICT)
console: Console = Console(
    theme=_fallback_theme,
    color_system="auto",  # Prueba con "standard", "256" o "truecolor" si "auto" no funciona
    highlight=False,
    width=None,  # Ajusta al ancho de la terminal
    emoji=False,
)
_custom_theme_loaded = False

if theme_file_path:
    try:
        # Attempt to load theme from file
        custom_theme = Theme.read(str(theme_file_path))  # Use Theme.read for direct loading
        _fallback_theme.push(custom_theme)  # Push loaded theme on top
        _custom_theme_loaded = True
        # Use standard logging here as the logger helper might depend on this console
        logging.info(f"Successfully loaded theme from '{theme_file_path}'")
    except Exception as e:
        # Use builtins.print for critical fallback messages before console is fully trusted
        builtins.print(
            f"WARNING: Error loading theme from '{theme_file_path}': {e}",
            file=sys.stderr,
        )
        builtins.print("WARNING: Falling back to default theme.", file=sys.stderr)
        # Fallback theme is already in the stack
else:
    logging.warning(f"Theme file '{THEME_FILENAME}' not found in search paths. Using default theme.")

# Use the configured console for subsequent operations
_themed_print = console.print  # Capture the themed print method

# SECTION: PRETTY TRACEBACKS
# Installs a handler to format tracebacks using the configured Rich console
try:
    install_traceback(
        console=console,
        show_locals=False,  # Set True for detailed debugging if needed
        word_wrap=True,  # Enable word wrap for tracebacks
        width=None,  # Let Rich determine width based on console
        suppress=[],  # Add libraries to suppress frames from (e.g., ['click'])
    )
    logging.debug("Rich traceback handler installed.")
except Exception as e:
    logging.error(f"Failed to install Rich tracebacks: {e}")

# SECTION: CONVENIENCE PRINT FUNCTION


# Define a wrapper function 'print' that uses the themed console's print method.
# This allows other modules to just use `from pixabit.helpers._rich import print`.
# Use different name internally to avoid recursion errors.
def print(*args: Any, **kwargs: Any) -> None:
    """Prints to the configured Rich console using the loaded theme.

    Provides a convenient way to access `console.print` throughout the application.

    Args:
        *args: Positional arguments passed to `rich.console.Console.print`.
        **kwargs: Keyword arguments passed to `rich.console.Console.print`.
    """
    # Call the internal themed print function captured earlier
    _themed_print(*args, **kwargs)


# SECTION: EXPORTS
# Export the configured console instance and commonly used Rich components
# Other modules can import these directly from pixabit.helpers._rich
__all__ = [
    "console",  # The configured Console instance
    "print",  # The themed print function wrapper
    # Rich UI Components
    "Confirm",
    "IntPrompt",
    "Prompt",
    "Table",
    "Panel",
    "Columns",
    "Layout",
    "Rule",
    "box",
    "Progress",
    "track",
    "Live",
    "Markdown",
    "Text",
    "Syntax",
    "ReprHighlighter",
    # Progress Bar Columns
    "SpinnerColumn",
    "BarColumn",
    "TextColumn",
    "TimeElapsedColumn",
    "TimeRemainingColumn",
    "TaskProgressColumn",
    # Base Types / Theming
    "Theme",
    "ConsoleRenderable",
    "RichHandler",  # Export RichHandler type for use in logger setup
    # Optional ASCII Art
    "ART_AVAILABLE",
    "text2art",
]
-e 
-------- END OF FILE helpers/_rich.py --------

-------- START OF FILE helpers/_textual.py --------
# pixabit/helpers/_textual.py

# SECTION: MODULE DOCSTRING
"""Provides convenient access to core Textual components and potentially enhanced base widgets.

Acts as a central point for importing commonly used Textual classes, reducing
redundant imports in other TUI modules. Includes examples of potentially
enhanced or styled base widgets.
"""

# SECTION: IMPORTS
from typing import (  # Keep TypeVar/cast for EnhancedContainer
    Any,
    Optional,
    Type,
    TypeVar,
    cast,
)

# --- Textual Core ---
from textual import events, log, on, work  # Import work decorator
from textual.app import App, ComposeResult
from textual.binding import Binding
from textual.containers import (
    Container,
    Grid,
    Horizontal,
    HorizontalScroll,  # Added
    ScrollableContainer,
    Vertical,
    VerticalScroll,  # Added
)
from textual.css.query import NoMatches, QueryError  # Added QueryError
from textual.dom import DOMNode
from textual.geometry import (  # Added Region, Spacing
    Offset,
    Region,
    Size,
    Spacing,
)
from textual.message import Message
from textual.reactive import reactive
from textual.screen import ModalScreen, Screen  # Added ModalScreen
from textual.widget import Widget
from textual.widgets import (
    Button,
    Checkbox,
    ContentSwitcher,
    DataTable,
    DirectoryTree,  # Added
    Footer,
    Header,
    Input,
    Label,
    ListItem,
    ListView,
    LoadingIndicator,
    Markdown,  # Added Markdown base widget
    MarkdownViewer,
    OptionList,
    Placeholder,  # Added Placeholder
    Pretty,  # Added Pretty
    RadioButton,
    RadioSet,
    RichLog,  # Added RichLog
    Rule,  # Added Rule
    Select,
    Sparkline,  # Added Sparkline
    Static,
    Switch,
    TabbedContent,
    TabPane,
    Tabs,  # Added Tabs
    TextArea,
    # Added TextLog
    Tree,
)
from textual.worker import Worker, get_current_worker  # Added Worker management

# SECTION: TYPE VARIABLES
T = TypeVar("T", bound=Widget)  # Generic type for widget querying


# SECTION: ENHANCED/CUSTOM WIDGETS (Examples)


# KLASS: EnhancedContainer
class EnhancedContainer(Container):
    """Example Container with additional convenience methods."""

    # FUNC: get_widget_by_id
    def get_widget_by_id(self, widget_id: str, *, expected_type: Type[T] = Widget) -> T:  # type: ignore
        """Safely gets a widget by ID, raising a specific error if not found or type mismatch.

        Args:
            widget_id: The ID of the widget to find (without the # prefix).
            expected_type: The expected type of the widget (e.g., Button, Input).

        Returns:
            The found widget instance, cast to the expected type.

        Raises:
            ValueError: If no widget with the ID is found.
            TypeError: If the found widget is not of the expected type.
        """
        try:
            widget = self.query_one(f"#{widget_id}", expected_type)
            return widget  # Already checked type with query_one
        except NoMatches:
            # Error if widget not found
            log.error(f"Widget with ID '#{widget_id}' not found within {self}.")
            raise ValueError(f"No widget with ID '{widget_id}' found.")
        except QueryError as e:
            # Error if widget found but type doesn't match expected_type
            log.error(f"Widget query error for '#{widget_id}': {e}")
            # Extract actual type if possible from error or query again without type check
            try:
                actual_widget = self.query_one(f"#{widget_id}")
                actual_type = type(actual_widget).__name__
            except Exception:
                actual_type = "unknown"
            raise TypeError(f"Widget '#{widget_id}' found, but its type ('{actual_type}') does not match expected type ('{expected_type.__name__}').") from e


# KLASS: PixabitButton (Example styled button)
class PixabitButton(Button):
    """Example Button using CSS variables defined in the application's theme."""

    DEFAULT_CSS = """
    PixabitButton {
        /* Assuming these CSS variables are defined in pixabit.tcss */
        background: $accent; /* Use accent color for background */
        color: $text;
        border: tall $accent-darken-2; /* Slightly darker border */
        min-width: 8; /* Ensure minimum width */
        padding: 0 2; /* Horizontal padding */
        height: 3; /* Explicit height */
        content-align: center middle; /* Center text */
    }

    PixabitButton:hover {
        background: $accent-lighten-1; /* Lighter on hover */
        border-color: $accent;
    }

    /* Focus state might need specific styling if default isn't sufficient */
    PixabitButton.-focused {
         border: thick $accent-lighten-2;
         outline: none; /* May need outline depending on terminal */
    }

    PixabitButton.-active {
         background: $accent-darken-2; /* Darker when pressed */
    }
    """
    # You can add custom methods or properties here if needed
    pass


# SECTION: EXPORTS
# Export common Textual components for use throughout the application
__all__ = [
    # --- Textual Core ---
    "App",
    "ComposeResult",
    "Binding",
    "Widget",
    "Screen",
    "ModalScreen",
    "reactive",
    "events",
    "on",
    "work",  # Export work decorator
    "log",
    "Message",
    "DOMNode",
    # Geometry
    "Size",
    "Offset",
    "Region",
    "Spacing",
    # Selectors / Querying
    "NoMatches",
    "QueryError",
    # Workers
    "Worker",
    "get_current_worker",
    # --- Containers ---
    "Container",
    "Grid",
    "Horizontal",
    "Vertical",
    "ScrollableContainer",
    "HorizontalScroll",
    "VerticalScroll",
    # --- Basic Widgets ---
    "Button",
    "Checkbox",
    "Input",
    "Label",
    "ListItem",
    "ListView",
    "OptionList",
    "RadioButton",
    "RadioSet",
    "Select",
    "Static",
    "Switch",
    # --- More Advanced Widgets ---
    "ContentSwitcher",
    "DataTable",
    "DirectoryTree",
    "Footer",
    "Header",
    "LoadingIndicator",
    "Markdown",
    "MarkdownViewer",
    "Placeholder",
    "Pretty",
    "RichLog",
    "Rule",
    "Sparkline",
    "TabbedContent",
    "TabPane",
    "Tabs",
    "TextArea",
    "Tree",
    # --- Custom Base Widgets ---
    "EnhancedContainer",  # Example enhanced container
    "PixabitButton",  # Example styled button
]
-e 
-------- END OF FILE helpers/_textual.py --------

-------- START OF FILE helpers/__init__.py --------
-e 
-------- END OF FILE helpers/__init__.py --------

-------- START OF FILE models/challenge.py --------
# pixabit/models/challenge.py


# SECTION: MODULE DOCSTRING
"""─── Challenge Model ────────────────────────────────────────────────────.

Defines  Pydantic  models  for  representing  Habitica  Challenges,   their
associated   metadata   (leader,   group),   and   provides   a   container
(`ChallengeList`) for managing collections of challenges and  linking  them
to tasks. Includes support for group privacy and legacy status calculation.
───────────────────────────────────────────────────────────────────────────
"""

# SECTION: IMPORTS
from __future__ import annotations

import asyncio
import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Any, Iterator, Literal

# External Libs
import emoji_data_python
from pydantic import (
    BaseModel,
    ConfigDict,
    Field,
    ValidationError,
    ValidationInfo,
    computed_field,
    field_validator,
    model_validator,
)

from pixabit.config import USER_ID

# Local Imports (Ensure these resolve correctly)
try:
    from pixabit.api.client import HabiticaClient
    from pixabit.config import HABITICA_DATA_PATH
    from pixabit.helpers._json import save_json, save_pydantic_model
    from pixabit.helpers._logger import log
    from pixabit.helpers.DateTimeHandler import DateTimeHandler

    from .task import AnyTask, Task, TaskList
    from .user import User
except ImportError:
    # --- Fallbacks ---
    log = logging.getLogger(__name__)
    log.addHandler(logging.NullHandler())
    log.warning("Using placeholder dependencies.")

    class Task(BaseModel):
        id: str | None = None
        challenge: dict | None = None
        type: str = "unknown"
        text: str = ""

    AnyTask = Task

    class TaskList:
        def __init__(self, t=None):
            self.t = t or []

        def __iter__(self):
            return iter(self.t)

        def __len__(self):
            return len(self.t)

        @classmethod
        def from_raw_api_list(cls, li):
            return cls(li)

    class DateTimeHandler:
        __init__ = lambda s, t: None
        utc_datetime = None

    class HabiticaClient:
        async def get_challenges(self):
            return []

        async def get_tasks(self):
            return []

        async def get_user_data(self):
            return {"_id": "mock_user", "profile": {"challenges": []}}

    HABITICA_DATA_PATH = Path("./pixabit_cache")

    class User(BaseModel):
        id: str = "mock_user"
        profile: dict = {"challenges": []}
        auth: dict = {}

        @classmethod
        def model_validate(cls, data):
            return cls(**data)

    def save_json(d, f, **k):
        pass

    # --- End Fallbacks ---

# SECTION: PYDANTIC SUB-MODELS


# KLASS: ChallengeLeader
class ChallengeLeader(BaseModel):
    """Represents the leader info potentially nested within challenge data."""

    model_config = ConfigDict(extra="ignore", populate_by_name=True)
    id: str = Field(..., alias="_id", description="Leader's user ID.")
    name: str | None = Field(None, description="Leader's display name (parsed).")

    @model_validator(mode="before")
    @classmethod
    def extract_profile_name(cls, data: Any) -> dict[str, Any]:
        if isinstance(data, dict):
            if "name" not in data and "profile" in data and isinstance(data["profile"], dict):
                data["name"] = data["profile"].get("name")
            if "_id" in data and "id" not in data:
                data["id"] = data["_id"]
        return data if isinstance(data, dict) else {}

    @field_validator("name", mode="before")
    @classmethod
    def parse_name_emoji(cls, value: Any) -> str | None:
        if isinstance(value, str):
            return emoji_data_python.replace_colons(value).strip()
        return None


# KLASS: ChallengeGroup
class ChallengeGroup(BaseModel):
    """Represents the group info nested within challenge data."""

    model_config = ConfigDict(extra="ignore", populate_by_name=True)
    id: str = Field(..., alias="_id", description="Group ID.")
    name: str = Field("Unnamed Group", description="Group name (parsed).")
    type: Literal["party", "guild", "tavern"] | str | None = Field(None, description="Group type.")
    privacy: Literal["private", "public"] | str = Field("public", description="Group privacy setting.")  # Default to public

    @field_validator("name", mode="before")
    @classmethod
    def parse_name_emoji(cls, value: Any) -> str:
        default = "Unnamed Group"
        if isinstance(value, str):
            parsed = emoji_data_python.replace_colons(value).strip()
            return parsed if parsed else default
        return default

    @model_validator(mode="before")
    @classmethod
    def map_id(cls, data: Any) -> dict[str, Any]:
        if isinstance(data, dict):
            if "_id" in data and "id" not in data:
                data["id"] = data["_id"]
        return data if isinstance(data, dict) else {}

    @field_validator("privacy", mode="before")
    @classmethod
    def normalize_privacy(cls, value: Any) -> str:
        if isinstance(value, str):
            lower_val = value.lower()
            if lower_val in ["private", "public"]:
                return lower_val
            log.warning(f"Unknown privacy value '{value}'.")
            return value
        return "public"


# ──────────────────────────────────────────────────────────────────────────────

# SECTION: MAIN CHALLENGE MODEL


# KLASS: Challenge
class Challenge(BaseModel):
    """Represents a single Habitica Challenge entity."""

    model_config = ConfigDict(extra="ignore", populate_by_name=True, validate_assignment=False)

    # --- Core Identification & Text ---
    id: str = Field(..., description="Unique challenge ID (mapped from _id if necessary).")
    name: str = Field("Unnamed Challenge", description="Challenge name (parsed emoji).")
    short_name: str | None = Field(None, alias="shortName", description="Short name/slug (parsed emoji).")
    summary: str = Field("", description="Summary text (parsed emoji).")  # Default to empty string
    description: str = Field("", description="Full description (parsed emoji).")  # Default to empty string

    # --- Relationships ---
    # Nested models, validated internally
    leader: ChallengeLeader | None = Field(None, description="Challenge leader details.")
    group: ChallengeGroup | None = Field(None, description="Associated group details (party/guild).")

    # --- Metadata & Status ---
    prize: int = Field(0, description="Gem prize for the winner (if any).")
    member_count: int = Field(0, alias="memberCount", description="Number of participants.")
    official: bool = Field(False, description="Is this an official Habitica challenge?")
    created_at: datetime | None = Field(None, alias="createdAt", description="Timestamp created (UTC).")
    updated_at: datetime | None = Field(None, alias="updatedAt", description="Timestamp updated (UTC).")
    # 'broken' indicates a problem (e.g., 'CHALLENGE_DELETED')
    broken: str | None = Field(None, description="Status if broken, e.g., 'CHALLENGE_DELETED'.")
    # Status flag derived from 'broken' field
    is_broken: bool = Field(False, description="True if the 'broken' field has a value.")

    # --- TUI Context Specific ---
    # Populated externally based on context (e.g., user's participation)
    owned: bool | None = Field(None, description="Is challenge owned by the fetching user? (Set externally)", exclude=False)
    joined: bool | None = Field(None, description="Has the fetching user joined this challenge? (Set externally)", exclude=False)

    # --- Linked Data ---
    # Populated externally by ChallengeList.link_tasks()
    tasks: list[Task] = Field(default_factory=list, description="Tasks belonging to this challenge.", exclude=False)

    @computed_field(description="True if not the Tavern challenge.")
    @property
    def is_legacy(self) -> bool:
        if self.group and isinstance(self.group.name, str) and self.group.name == "Tavern":
            return False
        return True

    @model_validator(mode="before")
    @classmethod
    def check_and_assign_id(cls, data: Any) -> dict[str, Any]:
        if not isinstance(data, dict):
            return {}
        values = data.copy()
        values["is_broken"] = bool(values.get("broken"))
        if "_id" in values and "id" not in values:
            values["id"] = values["_id"]
        return values

    @model_validator(mode="after")
    def check_ownership(self, info: ValidationInfo) -> Challenge:
        """Sets the 'owned' flag based on comparing the leader ID to the
        current user ID provided in the validation context.
        Runs after initial fields and nested models are validated.
        """
        current_user_id: str | None = None
        # Get context if available
        if info.context and isinstance(info.context, dict):
            current_user_id = info.context.get("current_user_id")

        # Fallback to global config if context missing
        if current_user_id is None:
            current_user_id = USER_ID  # Assumes USER_ID is imported from config
            if not current_user_id or current_user_id == "fallback_user_id_from_config":  # Check usability
                current_user_id = None

        # Determine ownership
        if current_user_id and self.leader and self.leader.id == current_user_id:
            self.owned = True
            # log.debug(f"Challenge {self.id} marked as owned by leader match.")
        else:
            # Set explicitly to False if context was available but didn't match,
            # otherwise keep None if context was missing.
            self.owned = False if current_user_id else None
            if not self.leader:
                log.debug(f"Challenge {self.id} has no leader, cannot determine ownership by leader.")
            elif not current_user_id:
                log.debug(f"Cannot determine ownership for Challenge {self.id}, missing user context.")

        # Need to return self for 'after' validator
        return self

    @field_validator("id", mode="after")
    @classmethod
    def check_id(cls, v: str) -> str:
        if not v or not isinstance(v, str):
            raise ValueError("Challenge ID is required.")
        return v

    @field_validator("name", "short_name", "summary", "description", mode="before")
    @classmethod
    def parse_text_fields(cls, value: Any, info: ValidationInfo) -> str | None:
        # (Implementation remains the same - simplified for brevity here)
        default = "Unnamed Challenge" if info.field_name == "name" else ("" if info.field_name in ["summary", "description"] else None)
        if isinstance(value, str):
            p = emoji_data_python.replace_colons(value).strip()
            return p if (p or info.field_name != "name") else default
        return default

    @field_validator("created_at", "updated_at", mode="before")
    @classmethod
    def parse_datetimes_utc(cls, value: Any) -> datetime | None:
        if value is None:
            return None
        h = DateTimeHandler(timestamp=value)
        return h.utc_datetime

    @field_validator("prize", "member_count", mode="before")
    @classmethod
    def ensure_int(cls, value: Any) -> int:
        if value is None:
            return 0
        try:
            return int(float(value))
        except:
            return 0

    def add_task(self, task: AnyTask) -> None:
        if isinstance(task, Task) and task not in self.tasks:
            self.tasks.append(task)

    def add_tasks(self, tasks_to_add: list[AnyTask]) -> None:
        if isinstance(tasks_to_add, list):
            [self.add_task(task) for task in tasks_to_add]

    # --- Corrected Literal string types ---
    def get_tasks_by_type(self, task_type: Literal["habit", "daily", "todo", "reward"]) -> list[AnyTask]:
        return [t for t in self.tasks if hasattr(t, "task_type") and t.task_type == task_type]

    def __repr__(self) -> str:
        # Simplified repr construction
        parts = []
        if self.is_broken:
            parts.append(f"BROKEN({self.broken or ''})")
        if not self.is_legacy:
            parts.append("TAVERN")
        if self.joined:
            parts.append("Joined")
        if self.official:
            parts.append("Official")
        flags_str = f" ({', '.join(parts)})" if parts else ""
        task_count = len(self.tasks)
        name_str = self.name or "Unnamed"
        name_preview = name_str[:25].replace("\n", " ") + ("..." if len(name_str) > 25 else "")
        id_str = self.id[:8] if self.id else "NoID"
        return f"Challenge(id='{id_str}', name='{name_preview}', tasks={task_count}{flags_str})"

    def __str__(self) -> str:
        return self.name or "Unnamed Challenge"


# SECTION: CHALLENGE LIST CONTAINER


# KLASS: ChallengeList
class ChallengeList(BaseModel):
    """ChallengeList."""

    model_config = ConfigDict(extra="forbid", frozen=False, arbitrary_types_allowed=False)
    challenges: list[Challenge] = Field(default_factory=list)

    @classmethod
    def from_raw_data(
        cls,
        raw_challenge_list: list[dict[str, Any]],
        # --- Add Context Parameter ---
        context: dict[str, Any] | None = None,
        # --- End Add ---
    ) -> ChallengeList:
        """Processes raw API data, validating into Challenge models, passing context."""
        if not isinstance(raw_challenge_list, list):
            return cls(challenges=[])
        validated_challenges: list[Challenge] = []
        count = 0
        for i, raw in enumerate(raw_challenge_list):
            if not isinstance(raw, dict):
                continue
            try:
                challenge_instance = Challenge.model_validate(raw, context=context)

                validated_challenges.append(challenge_instance)
                count += 1
            except Exception as e:
                log.error(f"Chal[{i}] validation fail ID:{raw.get('_id','N/A')} Name:{raw.get('name','N/A')}: {e}")
        log.debug(f"ChallengeList.from_raw_data validated {count} challenges.")
        return cls(challenges=validated_challenges)

    def link_tasks(self, task_list_obj: TaskList) -> int:
        if not isinstance(task_list_obj, TaskList) or not self.challenges:
            return 0
        log.info(f"Linking tasks (count={len(task_list_obj)}) to {len(self.challenges)} challenges...")
        lookup = {c.id: c for c in self.challenges}
        for challenge in self.challenges:
            challenge.tasks.clear()  # Clear first
        linked, skip_link, skip_found = 0, 0, 0
        for task in task_list_obj:  # Iterate through the tasks from TaskList
            if not isinstance(task, Task) or not task.challenge:
                skip_link += 1
                continue
            challenge_id_from_task = task.challenge.challenge_id  # Correct variable name
            if not challenge_id_from_task:
                skip_link += 1
                continue
            target_challenge = lookup.get(challenge_id_from_task)  # Correct variable name
            if target_challenge:
                target_challenge.add_task(task)
                linked += 1
            else:
                skip_found += 1
                # log.debug(f"Task {getattr(task,'id','N/A')} links to missing chal {challenge_id_from_task}")
        log.info(f"Linking done. Linked:{linked}, NoLinkInfo:{skip_link}, NotFound:{skip_found}.")
        return linked

    def __len__(self) -> int:
        return len(self.challenges)

    def __iter__(self) -> Iterator[Challenge]:
        return iter(self.challenges)

    def __getitem__(self, index: int | slice) -> Challenge | list[Challenge]:
        if isinstance(index, int):
            if not 0 <= index < len(self.challenges):
                raise IndexError("Challenge index out of range")
        # Slicing works inherently on the list
        return self.challenges[index]

    def get_by_id(self, challenge_id: str) -> Challenge | None:
        """Finds a challenge by its ID."""
        # Can optimize with the lookup dict if created/stored persistently, but linear scan is fine too
        return next((c for c in self.challenges if c.id == challenge_id), None)

    # --- Filter methods - Placeholder implementations returning empty lists ---
    def _filter(self, criteria: callable[[Challenge], bool]) -> ChallengeList:
        # Basic filter mechanism used by others
        return ChallengeList(challenges=[c for c in self.challenges if criteria(c)])  # Corrected this

    def filter_by_name(self, name_part: str, case_sensitive=False) -> ChallengeList:
        name_match = name_part if case_sensitive else name_part.lower()
        return self._filter(lambda c: name_match in (c.name if case_sensitive else c.name.lower()))

    def filter_by_leader(self, leader_id: str) -> ChallengeList:
        return self._filter(lambda c: c.leader and c.leader.id == leader_id)

    def filter_by_group(self, group_id: str | None = None, group_type: str | None = None) -> ChallengeList:
        def criteria(c: Challenge) -> bool:
            group = c.group
            if not group:
                return False  # Challenge must have group for filtering
            if group_id and group.id != group_id:
                return False
            if group_type and group.type != group_type:
                return False
            return True

        return self._filter(criteria)

    def filter_official(self, official: bool = True) -> ChallengeList:
        return self._filter(lambda c: c.official == official)

    def filter_broken(self, is_broken: bool = True) -> ChallengeList:
        return self._filter(lambda c: c.is_broken == is_broken)

    def filter_joined(self, joined: bool = True) -> ChallengeList:
        return self._filter(lambda c: c.joined == joined)

    # --- End Filters ---

    def __repr__(self) -> str:
        return f"ChallengeList(count={len(self.challenges)})"


# ──────────────────────────────────────────────────────────────────────────────
# ──────────────────────────────────────────────────────────────────────────────

# SECTION: MAIN EXECUTION (Example/Test)


async def main():
    """Demo function to retrieve, process, and display challenges."""
    log.info("--- Challenge Models Demo ---")
    challenge_list_instance: ChallengeList | None = None
    user_instance: User | None = None
    task_list_instance: TaskList | None = None
    try:
        # --- >>> Import Helpers used ONLY in main HERE <<< ---
        try:
            from pixabit.api.client import HabiticaClient  # Ensure client is imported if not global
            from pixabit.helpers._json import save_json

            # Import models again locally IF the global ones might be placeholders
            # from .user import User
            # from .task import TaskList
        except ImportError as main_import_err:
            log.critical(f"Main function failed to import essential helpers/client: {main_import_err}")
            return  # Cannot proceed without save_json/client
        # --- >>> End Main-Specific Imports <<< ---

        # --- Setup ---
        cache_dir = HABITICA_DATA_PATH / "challenges_demo"
        cache_dir.mkdir(parents=True, exist_ok=True)
        # Use consistent path objects
        raw_challenges_path = cache_dir / "challenges_raw.json"
        raw_tasks_path = cache_dir / "tasks_for_challenges_raw.json"
        raw_user_path = cache_dir / "user_for_challenges_raw.json"
        # Define processed save path
        processed_challenges_path = cache_dir / "processed_challenges.json"  # More descriptive name

        api = HabiticaClient()  # Assumes configured

        log.info("Fetching data...")
        results = await asyncio.gather(
            api.get_all_challenges_paginated(), api.get_tasks(), api.get_user_data(), return_exceptions=True  # Assuming this fetches all pages
        )
        raw_challenges = results[0] if not isinstance(results[0], Exception) else None
        raw_tasks = results[1] if not isinstance(results[1], Exception) else None
        raw_user = results[2] if not isinstance(results[2], Exception) else None

        # --- Validate Fetched Data ---
        if not isinstance(raw_challenges, list):
            log.error("Failed to fetch challenges.")
            return
        # Allow tasks/user fetch to potentially fail but continue
        if not isinstance(raw_tasks, list):
            log.warning("Failed to fetch tasks.")
        if not isinstance(raw_user, dict):
            log.error("Failed to fetch user data.")

        log.success(f"Fetched {len(raw_challenges)} Chals, {len(raw_tasks or [])} Tasks, User: {'OK' if raw_user else 'Failed'}")
        # Save raw data
        save_json(raw_challenges, raw_challenges_path.name, folder=raw_challenges_path.parent)
        if raw_tasks:
            save_json(raw_tasks, raw_tasks_path.name, folder=raw_tasks_path.parent)
        if raw_user:
            save_json(raw_user, raw_user_path.name, folder=raw_user_path.parent)

        # --- Validate Models ---
        log.info("Validating models...")
        # Define context (even if ownership validation moved, pass for potential future use)
        user_id_context = getattr(User.model_validate(raw_user), "id", None) if raw_user and User else None  # Safely get user ID for context
        validation_context = {"current_user_id": user_id_context}

        challenge_list_instance = ChallengeList.from_raw_data(raw_challenges, context=validation_context)  # Pass context
        if raw_tasks:
            task_list_instance = TaskList.from_raw_api_list(raw_tasks)
        if raw_user and User:
            try:
                user_instance = User.model_validate(raw_user)
            except Exception as e:
                log.error(f"User validation failed: {e}")  # Keep user_instance=None on fail

        log.success(f"Validated: {len(challenge_list_instance)} Chals, {len(task_list_instance or [])} Tasks, User: {user_instance is not None}")

        # --- Process: Set Status & Link ---
        # Set Joined Status (using validated user_instance)
        if user_instance and challenge_list_instance:
            log.info("Setting joined status...")
            # ... (logic for getting joined_ids from user_instance.profile.challenges) ...
            joined_ids = set(getattr(getattr(user_instance, "profile", None), "challenges", []))
            # owned_ids = ... # Placeholder for owned IDs
            for chal in challenge_list_instance.challenges:
                chal.joined = chal.id in joined_ids
                # chal.owned = chal.id in owned_ids
            log.info("Joined status updated.")

        # Link Tasks
        if challenge_list_instance and task_list_instance:
            log.info("Linking tasks...")
            linked_count = challenge_list_instance.link_tasks(task_list_instance)  # Get count if needed for logging
            log.info(f"Linking complete ({linked_count} tasks linked).")

        # --- Save Processed Challenge List (After Linking) ---
        if challenge_list_instance:
            log.info(f"Saving processed challenge list to {processed_challenges_path}...")
            try:
                # 1. Dump the ChallengeList to a dict, carefully excluding nested fields
                #    We need to specify the path to exclude within the structure.
                #    Structure: ChallengeList -> challenges (list) -> [index] -> tasks (list) -> [index] -> styled_text/notes
                #    The exclude syntax for nested lists isn't straightforward for model_dump.
                #    Easier: Dump challenges individually excluding bad fields, then reconstruct.

                processed_challenges_list = []
                for challenge in challenge_list_instance.challenges:
                    # Dump each challenge, excluding the Task fields causing issues
                    # The 'tasks' list itself is included, but styled_text/notes WITHIN those tasks are not.
                    # NOTE: Pydantic's nested exclude doesn't easily target computed fields
                    #       within nested list models directly. We may need to dump tasks manually.

                    challenge_dict = challenge.model_dump(
                        mode="json",
                        exclude={  # Exclude from the Challenge level
                            "tasks": {"__all__": {"styled_text", "styled_notes"}}  # For items in the 'tasks' list...  # Exclude these fields for all tasks
                        },
                    )
                    processed_challenges_list.append(challenge_dict)

                # Re-wrap the list of challenge dicts into the structure expected by ChallengeList save
                # Our ChallengeList BaseModel *only* has the 'challenges' field
                data_to_save = {"challenges": processed_challenges_list}

                # 2. Save the resulting dictionary using the basic save_json helper
                from pixabit.helpers._json import save_json

                save_successful = save_json(data_to_save, processed_challenges_path.name, folder=processed_challenges_path.parent)

                if save_successful:
                    log.success("Processed challenges saved.")
                else:
                    log.error("Failed to save processed challenges (save_json returned False).")

            except ImportError:
                log.error("Cannot save processed challenges: save_json helper missing.")
            except ValidationError as dump_err:
                log.error(f"Pydantic error during challenge list dump: {dump_err}")
            except Exception as save_err:
                log.exception(f"Error saving processed challenges: {save_err}")

        # --- Display ---
        if challenge_list_instance:
            print("\n--- Loaded Challenges (Sample Display) ---")
            print(f"Total: {len(challenge_list_instance)}")
            # ... (Display logic as before, e.g., show first 5) ...
            for i, chal in enumerate(challenge_list_instance.challenges[:5]):
                print(f"\n[{i+1}] {repr(chal)}")
                if chal.group:
                    print(f"    Group: '{chal.group.name}' (Privacy: {chal.group.privacy})")
                if chal.leader:
                    print(f"    Leader: {chal.leader.name}")
                # Optionally show linked tasks preview
                if chal.tasks:
                    print(f"    Tasks Sample: [{chal.tasks[0].text[:20] if chal.tasks else ''}...]")

    except Exception as e:
        log.exception(f"Main execution error: {e}")

    log.info("--- Challenge Models Demo Finished ---")


if __name__ == "__main__":
    logging.basicConfig(level=logging.DEBUG, format="%(asctime)s [%(levelname)-8s] %(name)s - %(message)s", datefmt="%H:%M:%S")
    logging.getLogger("Pixabit").setLevel(logging.DEBUG)
    asyncio.run(main())

# ──────────────────────────────────────────────────────────────────────────────
-e 
-------- END OF FILE models/challenge.py --------

-------- START OF FILE models/game_content.py --------
# pixabit/models/game_content.py

# ─── Model ────────────────────────────────────────────────────────────────────
#          Habitica Static Game Content Models & Manager
# ──────────────────────────────────────────────────────────────────────────────

# SECTION: MODULE DOCSTRING
"""Manages Habitica's static game content (spells, gear, quests, etc.).
Provides Pydantic models for content items and a manager class (`StaticContentManager`)
for fetching, caching (raw and processed), and accessing this data efficiently.
"""

# SECTION: IMPORTS
from __future__ import annotations

import json
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Any, Literal, Type  # Use standard lowercase etc.

import emoji_data_python

# External Libs
from pydantic import (
    BaseModel,
    ConfigDict,
    Field,
    ValidationError,
    computed_field,  # <<<---- Import computed_field
    field_validator,
    model_validator,
)

# Local Imports (assuming helpers and api are accessible)
try:
    from pixabit.api.client import HabiticaClient  # Needed to fetch content
    from pixabit.config import (
        DEFAULT_CACHE_DURATION_DAYS,  # Default expiry
        HABITICA_DATA_PATH,  # Main cache dir
    )
    from pixabit.helpers._json import load_json, load_pydantic_model, save_json, save_pydantic_model
    from pixabit.helpers._logger import log
    from pixabit.helpers.DateTimeHandler import DateTimeHandler
except ImportError:
    import logging

    log = logging.getLogger(__name__)
    log.addHandler(logging.NullHandler())
    # Fallbacks for isolated testing
    HABITICA_DATA_PATH = Path("./pixabit_cache")
    DEFAULT_CACHE_DURATION_DAYS = 7

    def load_json(p, **k):
        return None

    def save_json(d, p, **k):
        log.warning("Save skipped, helper missing.")

    def load_pydantic_model(m, p, **k):
        return None

    def save_pydantic_model(m, p, **k):
        log.warning("Save skipped, helper missing.")

    log.warning("game_content.py: Could not import helpers/api/config. Using fallbacks.")


# SECTION: CONSTANTS & CONFIG
CACHE_SUBDIR_STATIC = "static_content"
RAW_CONTENT_FILENAME = "habitica_content_raw.json"
PROCESSED_CONTENT_FILENAME = "habitica_content_processed.json"

# Ensure base path exists
HABITICA_DATA_PATH.mkdir(parents=True, exist_ok=True)
STATIC_CACHE_DIR = HABITICA_DATA_PATH / CACHE_SUBDIR_STATIC
STATIC_CACHE_DIR.mkdir(parents=True, exist_ok=True)


# SECTION: PYDANTIC MODELS FOR CONTENT ITEMS


# --- Gear Models ---


class GearEvent(BaseModel):
    """Nested event info within gear items."""

    model_config = ConfigDict(extra="ignore")
    start_date: datetime | None = Field(None, alias="startDate")
    end_date: datetime | None = Field(None, alias="endDate")
    # season: str | None = None # Season can also be top-level

    @field_validator("start_date", "end_date", mode="before")
    @classmethod
    def parse_datetime_utc(cls, v: Any) -> datetime | None:
        handler = DateTimeHandler(timestamp=v)
        return handler.utc_datetime


# KLASS: Gear
class Gear(BaseModel):
    """Habitica gear model (parsed from content.gear.flat)."""

    model_config = ConfigDict(
        extra="ignore",  # Ignore unused fields like 'purchase', 'canDrop', etc.
        populate_by_name=True,  # Use aliases for API fields
        frozen=True,  # Gear definitions are static
    )

    key: str = Field(..., description="Unique identifier key (e.g., 'weapon_warrior_1').")
    text: str = Field(..., description="Display name of the gear.")
    notes: str = Field("", description="Description or flavor text.")
    value: float = Field(0, description="Purchase price in Gold (sometimes Gems for special).")  # Often int, use float for safety

    # Direct stat attributes (no nesting)
    strength: float = Field(0.0, alias="str")
    intelligence: float = Field(0.0, alias="int")
    constitution: float = Field(0.0, alias="con")
    perception: float = Field(0.0, alias="per")

    # Other attributes
    type: Literal["weapon", "armor", "head", "shield", "back", "body", "eyewear", "headAccessory"] | None = Field(None, description="Slot type.")
    special_class: str | None = Field(None, alias="specialClass", description="Class required to get bonus ('warrior', 'rogue', etc.), or 'special'.")

    two_handed: bool = Field(False, alias="twoHanded", description="Whether the weapon is two-handed.")
    gear_set: str | None = Field(
        None, alias="set", description="Gear set key ('base', 'golden', 'seasonal', etc.)."
    )  # Changed from sett -> set to avoid python keyword conflict

    # Event info (if applicable)
    event: GearEvent | None = None
    # Season directly on item (sometimes overrides event.season?)
    season: str | None = None

    # Index/Order within set? Seems less useful.
    # index: str | None = None

    @field_validator("text", "notes", mode="before")
    @classmethod
    def parse_text_emoji(cls, v: Any) -> str:
        if isinstance(v, str):
            return emoji_data_python.replace_colons(v).strip()
        return ""

    @field_validator("value", mode="before")
    @classmethod
    def parse_value(cls, v: Any) -> float:
        # Sometimes value is gold, sometimes gems - often represented * 4 for gold value
        # Treat as float for flexibility. Raw might be integer gold amount.
        try:
            return float(v)
        except (ValueError, TypeError):
            return 0.0


# --- Spell Models ---
# KLASS: Spell
class Spell(BaseModel):
    """Habitica spell/skill model."""

    model_config = ConfigDict(extra="ignore", populate_by_name=True, frozen=True)

    key: str = Field(..., description="Unique skill key (e.g., 'fireball').")
    text: str = Field(..., description="Display name of the skill.")
    notes: str = Field("", description="In-game description/notes.")
    mana: float = Field(0.0, description="Mana cost.")
    target: Literal["self", "user", "party", "task", "certainUsers"] | str | None = Field(None, description="Target type (e.g., 'self', 'user', 'party', 'task').")
    klass: Literal["wizard", "healer", "warrior", "rogue", "special"] | None = Field(None, description="Associated class or 'special'.")
    lvl: int = Field(1, description="Level required to learn/use.")

    # Optional API fields
    immediate_use: bool = Field(False, alias="immediateUse")
    purchase_type: str | None = Field(None, alias="purchaseType")  # Typically null for class skills
    value: int | None = Field(0)  # Gold value if purchasable (usually 0 for skills)

    @field_validator("text", "notes", mode="before")
    @classmethod
    def parse_text_emoji(cls, v: Any) -> str:
        if isinstance(v, str):
            return emoji_data_python.replace_colons(v).strip()
        return ""

    @field_validator("mana", mode="before")
    @classmethod
    def ensure_float(cls, v: Any) -> float:
        try:
            return float(v)
        except (ValueError, TypeError):
            return 0.0

    @field_validator("lvl", "value", mode="before")
    @classmethod
    def ensure_int(cls, v: Any) -> int:
        if v is None:
            return 0
        try:
            return int(v)
        except (ValueError, TypeError):
            return 0


# --- Quest Models ---
class QuestBoss(BaseModel):
    """Model for boss properties within a quest."""

    model_config = ConfigDict(extra="ignore", populate_by_name=True, frozen=True)
    name: str = Field(..., description="Boss display name.")
    hp: float = Field(0.0, description="Initial HP of the boss.")
    strength: float = Field(0.0, alias="str", description="Boss strength (influences damage to party).")
    defense: float = Field(0.0, alias="def", description="Boss defense (influences damage dealt).")
    # rage: float? # Maybe add if needed

    @field_validator("hp", "strength", "defense", mode="before")
    @classmethod
    def ensure_float(cls, v: Any) -> float:
        try:
            return float(v)
        except (ValueError, TypeError):
            return 0.0


class QuestDropItem(BaseModel):
    """Individual item details within quest drops."""

    model_config = ConfigDict(extra="ignore", frozen=True)
    type: str = Field(...)  # e.g., "Food", "Eggs", "HatchingPotions"
    key: str = Field(...)  # Item key


class QuestDrop(BaseModel):
    """Model for drop properties within a quest."""

    model_config = ConfigDict(extra="ignore", frozen=True)
    exp: int = Field(0, description="Experience points awarded.")
    gp: float = Field(0.0, description="Gold awarded.")  # Can be float? Let's assume so.
    # Can be a list of Item dicts or a dict mapping Item Type -> List of Item Keys
    # We'll simplify to a list of specific item drops for now
    items: list[QuestDropItem] = Field(default_factory=list)

    @field_validator("exp", mode="before")
    @classmethod
    def ensure_int(cls, v: Any) -> int:
        try:
            return int(v)
        except (ValueError, TypeError):
            return 0

    @field_validator("gp", mode="before")
    @classmethod
    def ensure_float(cls, v: Any) -> float:
        try:
            return float(v)
        except (ValueError, TypeError):
            return 0.0

    @model_validator(mode="before")
    @classmethod
    def structure_drop_items(cls, data: Any) -> dict[str, Any]:
        """Standardizes the 'items' field into a list of QuestDropItem."""
        if not isinstance(data, dict):
            return data if isinstance(data, dict) else {}

        items_data = data.get("items", {})  # API response might have items directly
        if isinstance(items_data, dict):
            standardized_items = []
            for item_type, keys in items_data.items():
                if isinstance(keys, list):
                    for item_key in keys:
                        if isinstance(item_key, str):
                            standardized_items.append(QuestDropItem(type=item_type, key=item_key))
            data["items"] = standardized_items
        elif isinstance(items_data, list):
            # Assume list is already in {type:..., key:...} format? Adapt if needed.
            # For now, let validation handle it, or parse explicitly here if structure is known.
            pass  # Let Pydantic try parsing list of dicts into list[QuestDropItem]
        else:
            data["items"] = []  # Ensure items is a list

        return data


class QuestCollect(BaseModel):
    """Model for item collection goals within a quest."""

    model_config = ConfigDict(extra="allow", frozen=True)  # Allow any item keys
    # Stores {item_key: count_needed}
    # Pydantic will populate this dict directly from the API response.
    # Example: {"petal": 10, "shiny_seed": 5}


class QuestUnlockCondition(BaseModel):
    """Model for quest unlock condition."""

    model_config = ConfigDict(extra="ignore", frozen=True)
    condition: str | None = None  # Textual description or key? API seems inconsistent
    text: str | None = None  # Unlock message


# KLASS: Quest
class Quest(BaseModel):
    """Habitica quest model (parsed from content.quests)."""

    model_config = ConfigDict(
        extra="ignore",  # Ignore other fields like wiki link, previous quest etc.
        populate_by_name=True,
        frozen=True,  # Quest definitions are static
    )

    key: str = Field(..., description="Unique quest key (e.g., 'atom1').")
    text: str = Field(..., description="Quest title/name.")
    notes: str = Field("", description="Quest description.")
    completion_msg: str = Field("", alias="completion", description="Message shown on completion.")
    category: str | None = Field(None, description="Quest category (e.g., 'boss', 'collect', 'pet').")

    # Nested models
    boss: QuestBoss | None = None
    collect: QuestCollect | None = None  # Holds item keys and counts needed
    drop: QuestDrop = Field(default_factory=QuestDrop)

    # Gold cost to buy quest scroll
    value: int = Field(0, description="Scroll cost in Gold (Gems*4?)")  # Seems to be Gold
    # Level required to start? Seems absent in static content, maybe implied by category/key?
    # lvl: int | None = Field(None, description="Minimum level required?")

    # Group quest can be started in
    # group: dict? # Seems complex, ignoring for now

    # Unlock conditions / Prerequisites (simplified)
    unlock_condition: str | None = Field(None, alias="unlockCondition", description="Text explaining how to unlock.")
    # prereqQuests: list[str] = [] # List of previous quest keys needed

    # --- Validators ---
    @field_validator("text", "notes", "completion_msg", mode="before")
    @classmethod
    def parse_text_emoji(cls, v: Any) -> str:
        if isinstance(v, str):
            return emoji_data_python.replace_colons(v).strip()
        return ""

    @field_validator("value", mode="before")
    @classmethod
    def ensure_int(cls, v: Any) -> int:
        try:
            return int(v)
        except (ValueError, TypeError):
            return 0

    @field_validator("unlock_condition", mode="before")
    @classmethod
    def parse_unlock(cls, v: Any) -> str | None:
        """Handles unlockCondition being object or string."""
        if isinstance(v, dict):
            return v.get("text")  # Prefer text if object
        elif isinstance(v, str):
            return v
        return None

    # --- ADD computed_field properties ---
    @computed_field(description="Derived: True if quest has boss data.")
    @property
    def is_boss_quest(self) -> bool:
        """Calculates if this is a boss quest based on loaded boss data."""
        # Check if boss exists and has relevant stats > 0
        return self.boss is not None and (self.boss.hp > 0 or self.boss.strength > 0)

    @computed_field(description="Derived: True if quest has collection data.")
    @property
    def is_collect_quest(self) -> bool:
        """Calculates if this is a collection quest based on loaded collect data."""
        # Check if collect dict exists and is non-empty
        # Use model_dump to check content after potential validation/parsing
        return self.collect is not None and bool(self.collect.model_dump())

    # --- END computed_field properties ---


# ──────────────────────────────────────────────────────────────────────────────

# SECTION: MAIN CONTENT CONTAINER MODEL


# KLASS: GameContent
class GameContent(BaseModel):
    """Container for processed static game content."""

    model_config = ConfigDict(frozen=True)  # Content is static once loaded

    gear: dict[str, Gear] = Field(default_factory=dict)
    quests: dict[str, Quest] = Field(default_factory=dict)
    spells: dict[str, Spell] = Field(default_factory=dict)  # Store all spells flat for easy lookup
    # Add other categories if needed (e.g., pets, mounts, backgrounds)
    # pets: dict[str, PetInfo] = Field(default_factory=dict)
    # mounts: dict[str, MountInfo] = Field(default_factory=dict)

    last_fetched_at: datetime | None = Field(None, description="Timestamp when the raw content was last fetched.")
    processed_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), description="Timestamp when this processed model was created.")

    @classmethod
    def from_raw_content(cls, raw_content: dict[str, Any], fetched_at: datetime) -> GameContent:
        """Parses the raw '/content' API response into structured models."""
        processed_gear: dict[str, Gear] = {}
        processed_quests: dict[str, Quest] = {}
        processed_spells: dict[str, Spell] = {}

        # --- Process Gear (from gear.flat) ---
        raw_gear_flat = raw_content.get("gear", {}).get("flat", {})
        if isinstance(raw_gear_flat, dict):
            for key, data in raw_gear_flat.items():
                if not isinstance(data, dict):
                    continue
                try:
                    # Inject key into data dict for validation
                    data["key"] = key
                    processed_gear[key] = Gear.model_validate(data)
                except ValidationError as e:
                    log.warning(f"Validation failed for gear '{key}': {e}")
                except Exception as e:
                    log.exception(f"Unexpected error processing gear '{key}': {e}")

        # --- Process Quests ---
        raw_quests = raw_content.get("quests", {})
        if isinstance(raw_quests, dict):
            for key, data in raw_quests.items():
                if not isinstance(data, dict):
                    continue
                try:
                    data["key"] = key  # Ensure key is present
                    processed_quests[key] = Quest.model_validate(data)
                except ValidationError as e:
                    log.warning(f"Validation failed for quest '{key}': {e}")
                except Exception as e:
                    log.exception(f"Unexpected error processing quest '{key}': {e}")

        # --- Process Spells (flatten all classes into one dict) ---
        raw_spells = raw_content.get("spells", {})
        if isinstance(raw_spells, dict):
            for spell_class, spells_in_class in raw_spells.items():
                if isinstance(spells_in_class, dict):
                    for key, data in spells_in_class.items():
                        if not isinstance(data, dict):
                            continue
                        try:
                            data["key"] = key  # Ensure key is present
                            data["klass"] = spell_class  # Inject class
                            processed_spells[key] = Spell.model_validate(data)
                        except ValidationError as e:
                            log.warning(f"Validation failed for spell '{key}' (class: {spell_class}): {e}")
                        except Exception as e:
                            log.exception(f"Unexpected error processing spell '{key}': {e}")

        log.info(f"Processed static content: {len(processed_gear)} gear items, {len(processed_quests)} quests, {len(processed_spells)} spells.")

        return cls(gear=processed_gear, quests=processed_quests, spells=processed_spells, last_fetched_at=fetched_at)


# ──────────────────────────────────────────────────────────────────────────────


# SECTION: STATIC CONTENT MANAGER


# KLASS: StaticContentManager
class StaticContentManager:
    """Manages fetching, caching, and access to Habitica's static game content."""

    def __init__(
        self,
        cache_dir: Path = STATIC_CACHE_DIR,
        raw_filename: str = RAW_CONTENT_FILENAME,
        processed_filename: str = PROCESSED_CONTENT_FILENAME,
        cache_duration_days: int = DEFAULT_CACHE_DURATION_DAYS,
        api_client: HabiticaClient | None = None,  # Optional: provide existing client
    ):
        """Initialize the content manager.

        Args:
            cache_dir: Directory for storing cached files.
            raw_filename: Filename for the raw API response cache.
            processed_filename: Filename for the processed GameContent model cache.
            cache_duration_days: How long processed cache is considered fresh.
            api_client: Optional HabiticaClient instance to use for fetching.
                        If None, a new instance will be created internally.
        """
        self.cache_dir = cache_dir
        self.raw_cache_path = cache_dir / raw_filename
        self.processed_cache_path = cache_dir / processed_filename
        self.cache_duration = timedelta(days=cache_duration_days)
        self.api_client = api_client or HabiticaClient()  # Create client if not provided

        self._content: GameContent | None = None  # In-memory cache
        self._lock = asyncio.Lock()  # Prevent race conditions during load

        # Ensure directories exist
        self.cache_dir.mkdir(parents=True, exist_ok=True)

    def _is_cache_fresh(self, cache_model: GameContent) -> bool:
        """Check if the processed cache file is still fresh."""
        if not cache_model or not cache_model.processed_at:
            return False
        # Ensure processed_at is timezone-aware for comparison
        processed_at_aware = cache_model.processed_at
        if processed_at_aware.tzinfo is None:
            processed_at_aware = processed_at_aware.replace(tzinfo=timezone.utc)

        return (datetime.now(timezone.utc) - processed_at_aware) < self.cache_duration

    async def load_content(self, force_refresh: bool = False) -> GameContent | None:
        """Loads game content, using cache or fetching from API as needed.

        Handles locking to prevent simultaneous loads.

        Args:
            force_refresh: If True, bypass all caches and fetch directly from API.

        Returns:
            The processed GameContent model, or None if loading fails.
        """
        async with self._lock:  # Acquire lock before proceeding
            # 1. Check in-memory cache first (unless forcing refresh)
            if self._content and not force_refresh:
                # Optional: Re-check freshness even for in-memory? Only needed if long-running process
                # if self._is_cache_fresh(self._content): # Can add this check if desired
                log.debug("Using in-memory static content cache.")
                return self._content

            # 2. Try loading from processed Pydantic model cache (if not forcing refresh)
            if not force_refresh and self.processed_cache_path.exists():
                log.debug(f"Attempting to load processed content from: {self.processed_cache_path}")
                cached_model = load_pydantic_model(GameContent, self.processed_cache_path)
                if cached_model and self._is_cache_fresh(cached_model):
                    log.info("Using fresh processed static content cache.")
                    self._content = cached_model
                    return self._content
                elif cached_model:
                    log.info("Processed static content cache is stale.")
                else:
                    log.warning("Failed to load processed static content cache.")

            # 3. Try loading from raw JSON cache (parse if available)
            raw_content_data = None
            raw_fetch_time = None
            if self.raw_cache_path.exists():
                log.debug(f"Attempting to load raw content from: {self.raw_cache_path}")
                raw_content_data = load_json(self.raw_cache_path)
                if raw_content_data:
                    # Try to get modification time as fallback fetch time
                    try:
                        mtime = self.raw_cache_path.stat().st_mtime
                        raw_fetch_time = datetime.fromtimestamp(mtime, timezone.utc)
                        log.info(f"Using raw static content cache (fetched around {raw_fetch_time}). Processing...")
                    except Exception:
                        raw_fetch_time = datetime.now(timezone.utc)  # Fallback
                        log.info("Using raw static content cache (fetch time unknown). Processing...")

                    # Process the raw data loaded from cache
                    try:
                        self._content = GameContent.from_raw_content(raw_content_data, raw_fetch_time)
                        # Save the newly processed data back to processed cache
                        self.save_processed_content()
                        return self._content
                    except Exception as e:
                        log.exception(f"Error processing raw content from cache: {e}")
                        self._content = None  # Ensure content is cleared on error
                else:
                    log.warning("Failed to load raw static content cache file.")

            # 4. Fetch from API as last resort (or if force_refresh is True)
            log.info(f"{'Forcing refresh' if force_refresh else 'Fetching new'} static content from Habitica API...")
            try:
                current_time = datetime.now(timezone.utc)
                fetched_data = await self.api_client.get_content()
                log.success("Successfully fetched raw content from API.")

                # Save the newly fetched raw data
                save_json(fetched_data, self.raw_cache_path)

                # Process the fetched data
                self._content = GameContent.from_raw_content(fetched_data, current_time)

                # Save the newly processed data
                self.save_processed_content()
                return self._content

            except Exception as e:
                log.exception(f"Failed to fetch or process static content from API: {e}")
                # If fetch fails, try to return potentially stale in-memory cache if it exists
                if self._content:
                    log.warning("API fetch failed. Returning potentially stale in-memory content.")
                    return self._content
                else:
                    # If absolutely no content could be loaded/fetched
                    log.error("Could not load static content from any source.")
                    return None  # Indicate failure

    def save_processed_content(self) -> None:
        """Saves the current in-memory _content model to the processed cache file."""
        if not self._content:
            log.warning("No processed content available in memory to save.")
            return

        if save_pydantic_model(self._content, self.processed_cache_path):
            log.info(f"Saved processed static content to {self.processed_cache_path}")
        else:
            log.error(f"Failed to save processed static content to {self.processed_cache_path}")

    async def refresh_from_api(self) -> GameContent | None:
        """Convenience method to force a refresh from the API."""
        return await self.load_content(force_refresh=True)

    # --- Accessor Methods ---
    # These methods ensure content is loaded before returning data
    # They return the specific dictionaries directly.

    async def get_gear(self) -> dict[str, Gear]:
        """Returns the dictionary of all processed gear items."""
        content = await self.load_content()
        return content.gear if content else {}

    async def get_gear_item(self, key: str) -> Gear | None:
        """Gets a specific gear item by key."""
        gear_dict = await self.get_gear()
        return gear_dict.get(key)

    async def get_quests(self) -> dict[str, Quest]:
        """Returns the dictionary of all processed quest items."""
        content = await self.load_content()
        return content.quests if content else {}

    async def get_quest(self, key: str) -> Quest | None:
        """Gets a specific quest by key."""
        quest_dict = await self.get_quests()
        return quest_dict.get(key)

    async def get_spells(self) -> dict[str, Spell]:
        """Returns the dictionary of all processed spell items."""
        content = await self.load_content()
        return content.spells if content else {}

    async def get_spell(self, key: str) -> Spell | None:
        """Gets a specific spell by key."""
        spell_dict = await self.get_spells()
        return spell_dict.get(key)


# ──────────────────────────────────────────────────────────────────────────────

# SECTION: MAIN EXECUTION (Example/Test)
import asyncio  # Needed for running async main


async def main():
    """Demo function to initialize and use the StaticContentManager."""
    log.info("--- Static Content Manager Demo ---")

    # Initialize manager (will create client internally)
    content_manager = StaticContentManager(cache_dir=STATIC_CACHE_DIR)

    try:
        # --- Load Content (uses cache or fetches) ---
        log.info("Loading content (initial load)...")
        content_loaded = await content_manager.load_content()

        if not content_loaded:
            log.error("Failed to load initial content. Exiting demo.")
            return

        log.success("Initial content loaded.")
        print(f"  Gear items: {len(content_loaded.gear)}")
        print(f"  Quests: {len(content_loaded.quests)}")
        print(f"  Spells: {len(content_loaded.spells)}")

        # --- Access specific data types ---
        log.info("Accessing specific data...")
        all_gear = await content_manager.get_gear()
        all_quests = await content_manager.get_quests()
        # print(f"  Fetched all gear again: {len(all_gear)} items")

        # Example: Get a specific item
        test_gear_key = "weapon_warrior_1"  # Change if needed
        gear_item = await content_manager.get_gear_item(test_gear_key)
        if gear_item:
            print(f"  Found Gear '{test_gear_key}': {gear_item.text} (STR: {gear_item.stats.strength})")
        else:
            print(f"  Gear '{test_gear_key}' not found.")

        test_quest_key = "atom1"  # Change if needed
        quest_item = await content_manager.get_quest(test_quest_key)
        if quest_item:
            print(f"  Found Quest '{test_quest_key}': {quest_item.text} (Category: {quest_item.category})")
            if quest_item.is_boss_quest:
                print(f"    Boss Quest: Name={quest_item.boss.name}, HP={quest_item.boss.hp}")
        else:
            print(f"  Quest '{test_quest_key}' not found.")

        # --- Force Refresh ---
        # log.info("Forcing content refresh from API...")
        # await content_manager.refresh_from_api()
        # log.success("Content refreshed.")

    except Exception as e:
        log.exception(f"An error occurred during the content manager demo: {e}")


if __name__ == "__main__":
    # Basic logging config if running standalone
    logging.basicConfig(level=logging.DEBUG, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")
    asyncio.run(main())

# ──────────────────────────────────────────────────────────────────────────────
-e 
-------- END OF FILE models/game_content.py --------

-------- START OF FILE models/message.py --------
# pixabit/models/message.py

# ─── Model ────────────────────────────────────────────────────────────────────
#            Habitica Message Models (Inbox & Group Chat)
# ──────────────────────────────────────────────────────────────────────────────

# SECTION: MODULE DOCSTRING
"""Defines Pydantic models for Habitica messages (Inbox/PM and Group Chat).

Includes:
- `MessageSenderStyles`: Represents nested sender style information.
- `Message`: Represents an individual message entity with improved parsing.
- `MessageList`: A Pydantic `BaseModel` container class to manage a collection
  of Message objects, providing context-aware processing (like conversation IDs),
  sorting, and filtering.
"""

# SECTION: IMPORTS
from __future__ import annotations

import logging
from collections import defaultdict
from datetime import datetime, timezone
from typing import Any, Iterator  # Use standard lowercase etc.

# External Libs
import emoji_data_python
from pydantic import (
    BaseModel,
    ConfigDict,
    Field,
    FieldValidationInfo,
    ValidationError,
    ValidationInfo,  # For context access
    field_validator,
    model_validator,
)

# Local Imports
try:
    from pixabit.config import USER_ID  # Import the actual user ID from config
    from pixabit.helpers._logger import log
    from pixabit.helpers.DateTimeHandler import DateTimeHandler
except ImportError:
    log = logging.getLogger(__name__)
    log.addHandler(logging.NullHandler())
    USER_ID = "fallback_user_id_from_config"  # Fallback if config not found

    class DateTimeHandler:
        def __init__(self, timestamp: Any):
            self._ts = timestamp

        @property
        def utc_datetime(self) -> datetime | None:
            try:
                return datetime.fromisoformat(str(self._ts).replace("Z", "+00:00"))
            except:
                return None

    log.warning("message.py: Could not import config/helpers. Using fallbacks.")


# SECTION: HELPER FUNCTIONS


# FUNC: determine_conversation_id
def determine_conversation_id(message: Message, current_user_id: str | None) -> str | None:
    """Calculates conversation ID based on processed Message object and current user context.

    Args:
        message: The validated Message object.
        current_user_id: The ID of the user viewing the messages.

    Returns:
        A string identifying the conversation (group ID, other user's ID, 'system',
        or a special ID for messages to self/unknown), or None if indeterminate.
    """
    # 1. Group Chats are simplest
    if message.group_id:
        return message.group_id

    # 2. System messages
    if message.sender_id == "system":
        return "system"  # Dedicated ID for system messages

    # 3. Private Messages - requires current_user_id context
    if not current_user_id:
        # Cannot determine PM partner without knowing 'me'
        log.debug(f"Cannot determine PM conversation ID for msg {message.id}: current_user_id missing.")
        # Return None or a placeholder? None seems cleaner for grouping logic.
        return None  # Or perhaps f"unknown_context_{message.id[:6]}"

    sender = message.sender_id
    recipient = message.recipient_id  # User ID of the inbox owner (present for inbox msgs)

    # Logic refinement based on message origin (inbox vs. sent)
    if message.sent_by_me:
        # Message was sent BY current_user
        # Check who it was sent TO. Often recipient_id is *not* populated for sent PMs in raw data.
        # The 'ownerId' (recipient_id) usually points to the *inbox owner*.
        # We need to rely on OTHER information if available (e.g., endpoint context or `userV` field)
        # For now, assume recipient_id IS the *other* person if populated and not 'me'.
        if recipient and recipient != current_user_id:
            return recipient  # Conversation with the recipient
        else:
            # Recipient is missing or is myself on a sent message. Cannot determine partner.
            # log.debug(f"Cannot determine conversation partner for SENT message {message.id}. Sender: {sender}, Recipient: {recipient}")
            # Need a way to distinguish 'message sent to self' from 'cannot determine recipient'.
            # If recipient == current_user_id -> it's a message to self.
            if recipient == current_user_id:
                return f"self:{current_user_id}"  # Special ID for messages to self
            else:
                return f"unknown_recipient:{message.id[:8]}"  # Placeholder for unknown

    else:
        # Message was received BY current_user (sent_by_me is False)
        # The conversation partner is the sender (unless it's system).
        if sender and sender != current_user_id:
            return sender  # Conversation is with the sender
        elif sender == current_user_id:
            # This means sender=me, but sent_by_me=False? Inconsistent data?
            # Could happen if viewing own messages in someone else's shared party/guild view?
            # Or if `sent_by_me` logic failed. Assume it's a message *I* sent.
            log.warning(
                f"Inconsistent state for message {message.id}: Sender is current user, but sent_by_me is False. Treating as 'sent to self' for conversation ID."
            )
            return f"self:{current_user_id}"
        else:
            # Sender is missing or invalid, and not sent by me.
            log.debug(f"Could not determine conversation partner for RECEIVED message {message.id}. Sender: {sender}, Recipient: {recipient}")
            return f"unknown_sender:{message.id[:8]}"  # Placeholder for unknown


# SECTION: PYDANTIC SUB-MODELS


# KLASS: MessageSenderStyles
class MessageSenderStyles(BaseModel):
    """Represents nested user style information potentially in message data."""

    # Allow other fields related to styles but ignore them for now
    model_config = ConfigDict(extra="ignore", populate_by_name=True)

    # Explicitly model fields we might care about (like class)
    klass: str | None = Field(None, alias="class", description="Sender's class ('rogue', 'wizard', etc.)")

    @model_validator(mode="before")
    @classmethod
    def extract_nested_class(cls, data: Any) -> dict[str, Any]:
        """Extracts 'class' from a nested 'stats' dictionary if present."""
        if not isinstance(data, dict):
            return data if isinstance(data, dict) else {}
        values = data.copy()
        # If 'class' is already top-level, use it
        if "class" not in values:
            stats_data = data.get("stats")
            if isinstance(stats_data, dict):
                # Map stats.class -> class (for alias 'klass')
                values["class"] = stats_data.get("class")
        return values


# SECTION: MAIN MESSAGE MODEL


# KLASS: Message
class Message(BaseModel):
    """Represents an individual message in Habitica (Inbox or Group Chat)."""

    # Allow fields like userV, _v which we ignore
    model_config = ConfigDict(extra="allow", populate_by_name=True, validate_assignment=True)

    # --- Core IDs & Context ---
    id: str = Field(..., description="Unique message document ID (mapped from _id).")
    # Sender ID ('uuid'): 'system' for system messages, user UUID otherwise
    sender_id: str | None = Field(None, alias="uuid", description="UUID of the sender ('system' for system messages).")
    # Group ID ('groupId'): 'party', guild ID, 'tavern', etc. Null for PMs.
    group_id: str | None = Field(None, alias="groupId", description="ID of the group chat context, if any.")
    # Recipient ID ('ownerId'): Primarily for inbox messages, user ID of the inbox owner.
    # Crucial for determining conversation partner in PMs.
    recipient_id: str | None = Field(None, alias="ownerId", description="User ID of the inbox owner (recipient context).")

    # --- Sender Info (Partially from direct fields, partially nested) ---
    # 'user' field often holds display name in chat messages
    sender_display_name: str | None = Field(None, alias="user", description="Sender's display name (parsed).")
    # 'username' field often holds login name
    sender_username: str | None = Field(None, alias="username", description="Sender's login name.")
    # Nested style information (optional)
    sender_styles: MessageSenderStyles | None = Field(None, alias="userStyles", description="Sender's style info.")

    # --- Content & Timestamp ---
    text: str = Field("", description="Formatted message text (parsed).")
    # Raw markdown source (less common in API now, but can be useful if present)
    unformatted_text: str | None = Field(None, alias="unformattedText", description="Raw markdown source text (parsed).")
    timestamp: datetime | None = Field(None, description="Timestamp message sent/received (UTC).")

    # --- Engagement & Flags ---
    # Store likes/flags as dict {user_id: bool/timestamp} - Use bool for simplicity
    # Default factory ensures these are initialized as empty dicts
    likes: dict[str, bool] = Field(default_factory=dict, description="Dictionary of user IDs who liked the message.")
    flags: dict[str, bool] = Field(default_factory=dict, description="Dictionary of user IDs who flagged the message.")
    flag_count: int = Field(0, alias="flagCount", description="Reported count of flags.")

    # --- System Message Info ---
    # 'info' field contains structured data for system messages (spell casts, quest progress, etc.)
    info: dict[str, Any] | None = Field(None, description="Structured data for system messages.")

    # --- Fields Calculated during MessageList Processing ---
    # These depend on context (current_user_id)
    sent_by_me: bool | None = Field(
        None,
        exclude=True,  # Exclude from model dump as it's context-dependent runtime info
        description="Derived: True if message was sent by the current user context.",
    )
    conversation_id: str | None = Field(
        None,
        exclude=True,  # Exclude from dump
        description="Derived: Grouping ID (group_id or other user's ID in PMs, 'system', etc.).",
    )
    is_pm: bool | None = Field(
        None,
        exclude=True,  # Exclude from dump
        description="Derived: True if message is likely a Private Message.",
    )

    # --- Validators ---

    @model_validator(mode="before")
    @classmethod
    def prepare_data(cls, data: Any) -> dict[str, Any]:
        """Map '_id' to 'id' before other validation."""
        if not isinstance(data, dict):
            # Let Pydantic raise type error
            return data

        values = data.copy()
        # Map _id if needed
        if "_id" in values and "id" not in values:
            values["id"] = values["_id"]

        # Default sender display name from username if needed?
        # This is tricky as 'user' field often contains the display name.
        # if not values.get('user') and values.get('username'):
        #      values['user'] = values['username'] # Risky, 'user' is primary

        return values

    @field_validator("id", mode="after")
    @classmethod
    def check_id(cls, v: str) -> str:
        """Ensure ID is a non-empty string after potential mapping."""
        if not v or not isinstance(v, str):
            raise ValueError("Message ID (_id) is required and must be a string.")
        return v

    # Consolidate text parsing for all relevant fields
    @field_validator("text", "unformatted_text", "sender_display_name", "sender_username", mode="before")
    @classmethod
    def parse_text_fields(cls, value: Any, info: FieldValidationInfo) -> str | None:
        """Parses text fields: replaces emoji, strips whitespace. Handles None for optional."""
        if isinstance(value, str):
            parsed = emoji_data_python.replace_colons(value).strip()
            # Defaulting behavior handled by Field(default=...)
            return parsed if parsed else None  # Return None if strip results in empty for optional fields
        # Allow None for optional fields like unformatted_text, sender_username, sender_display_name
        return None

    @field_validator("timestamp", mode="before")
    @classmethod
    def parse_timestamp_utc(cls, value: Any) -> datetime | None:
        """Parses timestamp using DateTimeHandler."""
        handler = DateTimeHandler(timestamp=value)
        if value is not None and handler.utc_datetime is None:
            log.warning(f"Could not parse timestamp for message field: {value!r}")
        return handler.utc_datetime

    @field_validator("flag_count", mode="before")
    @classmethod
    def ensure_int(cls, value: Any) -> int:
        """Ensures flag_count is an integer, defaulting to 0."""
        if value is None:
            return 0
        try:
            return int(float(value))  # Handle potential float input
        except (ValueError, TypeError):
            log.debug(f"Could not parse message flag_count: {value!r}. Using 0.")
            return 0

    # --- Computed Properties / Methods ---

    @property
    def is_system_message(self) -> bool:
        """Checks if this is likely a system message."""
        # Check sender_id explicitly or presence of 'info' field
        return self.sender_id == "system" or bool(self.info)

    @property
    def sender_class(self) -> str | None:
        """Extracts sender's class from sender_styles, if available."""
        return self.sender_styles.klass if self.sender_styles else None

    def __repr__(self) -> str:
        """Provides a developer-friendly string representation."""
        sender = "System" if self.is_system_message else (self.sender_username or self.sender_id or "Unknown")
        ts = self.timestamp.strftime("%Y-%m-%d %H:%M") if self.timestamp else "NoTime"
        # Show derived conversation_id if available
        conv = f" (ConvID: {self.conversation_id})" if self.conversation_id else ""
        pm_flag = " (PM)" if self.is_pm else ""
        sent_flag = " (Sent)" if self.sent_by_me else (" (Rcvd)" if self.sent_by_me is False else "")
        text_preview = self.text[:30].replace("\n", " ") + ("..." if len(self.text) > 30 else "")
        return f"Message(id='{self.id}', from='{sender}', time='{ts}{sent_flag}{pm_flag}{conv}', text='{text_preview}')"


# ──────────────────────────────────────────────────────────────────────────────


# SECTION: MESSAGE LIST CONTAINER


# KLASS: MessageList
class MessageList(BaseModel):
    """BaseModel container for managing Message objects.

    Handles validation, context-dependent field calculation (sent_by_me,
    conversation_id, is_pm), sorting, and provides filtering methods.
    """

    model_config = ConfigDict(
        extra="forbid",  # No unexpected fields
        arbitrary_types_allowed=False,
    )

    # The main data field: list of validated Message objects
    messages: list[Message] = Field(default_factory=list, description="Validated list of Message objects.")

    # Add context field directly? No, use ValidationInfo during validation.

    @model_validator(mode="before")
    @classmethod
    def process_raw_messages(cls, data: Any, info: ValidationInfo) -> dict[str, Any]:
        """Validates raw message data, calculates derived fields using context, sorts, and returns the structured data for the MessageList model.

        Expects 'current_user_id' in validation context (`info.context`).
        Expects input `data` to be the raw list of message dicts.
        """
        # --- Get Context ---
        # Fetch current_user_id from the validation context passed during instantiation.
        # Use the globally imported USER_ID as a fallback if context is missing (e.g., during tests).
        current_user_id: str | None = None
        if info.context and isinstance(info.context, dict):
            current_user_id = info.context.get("current_user_id")

        # If still None, maybe use the global USER_ID (consider implications)
        if current_user_id is None:
            # Use the one imported from config as a last resort, might not be right context always
            current_user_id = USER_ID  # Requires USER_ID to be imported from pixabit.config
            if not current_user_id or current_user_id == "fallback_user_id_from_config":  # Check if it's a real ID
                log.warning("'current_user_id' not found in context or config. Derived message fields (sent_by_me, conversation_id) may be inaccurate.")
                current_user_id = None  # Explicitly set to None if unusable

        # --- Process Input ---
        # This validator receives the *entire* input intended for the model.
        # If called like `MessageList.model_validate(raw_list, context=...)`, data is raw_list.
        # If called like `MessageList.model_validate({"messages": raw_list}, context=...)`, data is the dict.
        raw_message_list: list[Any] | None = None
        if isinstance(data, list):
            raw_message_list = data
        elif isinstance(data, dict):
            # Assume the list is under the 'messages' key if input is dict
            raw_message_list = data.get("messages")
            if not isinstance(raw_message_list, list):
                raise ValidationError.from_exception_data(
                    title=cls.__name__,
                    line_errors=[{"loc": ("messages",), "input": data.get("messages"), "type": "list_expected"}],
                )
        else:
            raise ValidationError.from_exception_data(
                title=cls.__name__,
                line_errors=[{"loc": (), "input": data, "type": "list_or_dict_expected"}],
            )

        # --- Validate, Enrich, and Collect Messages ---
        processed_messages: list[Message] = []
        validation_errors = []

        for index, item in enumerate(raw_message_list):
            if not isinstance(item, dict):
                log.warning(f"Skipping non-dict item at index {index} in message list.")
                validation_errors.append(f"Item at index {index} is not a dictionary.")
                continue

            try:
                # 1. Validate raw dict into Message model
                msg = Message.model_validate(item)

                # 2. Calculate context-dependent fields if user_id is known
                if current_user_id:
                    # Determine if sent by current user
                    # Explicit check against sender ID is primary.
                    # The old 'sent' flag from raw data is less reliable.
                    msg.sent_by_me = msg.sender_id == current_user_id
                    # Check if recipient is me when sent by someone else
                    # sent_to_me = not msg.sent_by_me and msg.recipient_id == current_user_id

                    # Determine conversation ID using the helper function
                    msg.conversation_id = determine_conversation_id(msg, current_user_id)

                    # Determine if PM (no group_id and not system message)
                    msg.is_pm = not msg.group_id and not msg.is_system_message
                else:
                    # Cannot reliably determine these without user context
                    msg.sent_by_me = None
                    msg.conversation_id = msg.group_id or ("system" if msg.is_system_message else None)  # Fallback
                    msg.is_pm = not msg.group_id and not msg.is_system_message  # Can still guess PM structure

                processed_messages.append(msg)

            except ValidationError as e:
                item_id = item.get("id", item.get("_id", f"index_{index}"))
                log.error(f"Validation failed for message ID '{item_id}': {e}")
                # Collect detailed errors if needed
                validation_errors.extend(e.errors(include_input=False))  # Pydantic v2 way
            except Exception as e:
                item_id = item.get("id", item.get("_id", f"index_{index}"))
                log.exception(f"Unexpected error processing message ID '{item_id}': {e}")
                validation_errors.append(f"Unexpected error processing message {item_id}")

        if validation_errors:
            # Decide how to handle errors: log, raise summary error, or continue
            # For robustness, log and continue is often preferred for lists.
            log.warning(f"Encountered {len(validation_errors)} errors during message list processing.")
            # Example: raise ValidationError.from_exception_data(...) if strictness needed

        # 3. Sort messages by timestamp (most recent last)
        # Use a safe default time for messages lacking a timestamp
        default_time = datetime.min.replace(tzinfo=timezone.utc)
        processed_messages.sort(key=lambda m: m.timestamp or default_time)
        log.debug(f"Processed and sorted {len(processed_messages)} messages.")

        # --- Return Structured Data for Model ---
        # Pydantic expects the validator to return a dictionary matching the model fields
        return {"messages": processed_messages}

    # --- Access and Filtering Methods ---
    # Operate on the validated `self.messages` list

    def __len__(self) -> int:
        return len(self.messages)

    def __iter__(self) -> Iterator[Message]:
        return iter(self.messages)

    def __getitem__(self, index: int | slice) -> Message | list[Message]:
        if isinstance(index, int):
            if not 0 <= index < len(self.messages):
                raise IndexError("Message index out of range")
        # Slicing works inherently
        return self.messages[index]

    def get_by_id(self, message_id: str) -> Message | None:
        """Finds a message by its unique ID."""
        return next((m for m in self.messages if m.id == message_id), None)

    def filter_by_sender(self, sender_id_or_name: str, case_sensitive: bool = False) -> MessageList:
        """Returns messages sent by a specific user ID or username. Returns new MessageList."""
        if not case_sensitive:
            sender_id_or_name_lower = sender_id_or_name.lower()
            filtered = [
                m
                for m in self.messages
                if (m.sender_id and m.sender_id.lower() == sender_id_or_name_lower) or (m.sender_username and m.sender_username.lower() == sender_id_or_name_lower)
            ]
        else:
            filtered = [m for m in self.messages if m.sender_id == sender_id_or_name or m.sender_username == sender_id_or_name]
        return MessageList(messages=filtered)  # Return new instance

    def filter_by_conversation(self, conversation_id: str) -> MessageList:
        """Returns messages belonging to a specific conversation ID (group or PM partner). Returns new MessageList."""
        # Uses the derived conversation_id field
        filtered = [m for m in self.messages if m.conversation_id == conversation_id]
        return MessageList(messages=filtered)

    def filter_by_group(self, group_id: str) -> MessageList:
        """Returns messages belonging to a specific group ID. Returns new MessageList."""
        filtered = [m for m in self.messages if m.group_id == group_id]
        return MessageList(messages=filtered)

    def filter_private_messages(self) -> MessageList:
        """Returns likely Private Messages (uses derived 'is_pm' flag). Returns new MessageList."""
        filtered = [m for m in self.messages if m.is_pm]
        return MessageList(messages=filtered)

    def filter_system_messages(self) -> MessageList:
        """Returns only system messages. Returns new MessageList."""
        filtered = [m for m in self.messages if m.is_system_message]
        return MessageList(messages=filtered)

    def filter_non_system_messages(self) -> MessageList:
        """Returns only non-system messages. Returns new MessageList."""
        filtered = [m for m in self.messages if not m.is_system_message]
        return MessageList(messages=filtered)

    # ... Add other filter methods from original code, ensuring they return MessageList ...

    def get_conversations(self) -> dict[str, list[Message]]:
        """Groups messages by their calculated conversation ID.

        Returns:
            A dictionary where keys are conversation IDs and values are lists
            of Message objects belonging to that conversation, sorted chronologically.
            Conversations themselves are ordered by the timestamp of the latest message.
        """
        grouped = defaultdict(list)
        valid_conv_ids = set()
        for msg in self.messages:
            # Group only messages that have a valid conversation_id
            if msg.conversation_id:
                grouped[msg.conversation_id].append(msg)
                valid_conv_ids.add(msg.conversation_id)  # Keep track of keys added

        # Sort by most recent activity (using timestamp of the last message in each group)
        default_time = datetime.min.replace(tzinfo=timezone.utc)
        sorted_ids = sorted(
            valid_conv_ids,  # Sort only the keys we actually added to grouped dict
            key=lambda cid: grouped[cid][-1].timestamp or default_time,
            reverse=True,  # Most recent conversations first
        )

        # Return ordered dictionary
        return {cid: grouped[cid] for cid in sorted_ids}

    def __repr__(self) -> str:
        """Simple representation."""
        return f"MessageList(count={len(self.messages)})"


# ──────────────────────────────────────────────────────────────────────────────
-e 
-------- END OF FILE models/message.py --------

-------- START OF FILE models/party.py --------
# pixabit/models/party.py

# ─── Model ────────────────────────────────────────────────────────────────────
#            Habitica Party & Quest Models
# ──────────────────────────────────────────────────────────────────────────────

# SECTION: MODULE DOCSTRING
"""Defines Pydantic models for Habitica Parties, including nested quest progress,
chat messages (via MessageList), and basic member information.
"""

# SECTION: IMPORTS
from __future__ import annotations

import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Any, Iterator  # Use standard lowercase etc.

# External Libs
import emoji_data_python
from pydantic import (
    BaseModel,
    ConfigDict,
    Field,
    FieldValidationInfo,
    PrivateAttr,  # For internal, non-dumped attributes
    ValidationError,
    ValidationInfo,  # For context
    field_validator,
    model_validator,
)

# Local Imports
try:
    from pixabit.api.client import HabiticaClient
    from pixabit.config import HABITICA_DATA_PATH, USER_ID  # Import USER_ID
    from pixabit.helpers._json import load_pydantic_model, save_json, save_pydantic_model
    from pixabit.helpers._logger import log
    from pixabit.helpers.DateTimeHandler import DateTimeHandler

    from .game_content import Quest as StaticQuestData  # Rename to avoid clash
    from .game_content import StaticContentManager

    # Import models used within Party
    from .message import Message, MessageList
except ImportError:
    # Fallbacks for isolated testing
    log = logging.getLogger(__name__)
    log.addHandler(logging.NullHandler())
    USER_ID = "fallback_user_id_from_config"
    HABITICA_DATA_PATH = Path("./pixabit_cache")

    def save_pydantic_model(m, p, **kwargs):
        pass

    def load_pydantic_model(cls, p, **kwargs):
        return None

    log.warning("party.py: Could not import dependencies. Using fallbacks.")

CACHE_SUBDIR = "party"
PARTY_RAW_FILENAME = "party_raw.json"
PARTY_PROCESSED_FILENAME = "party_processed.json"


# SECTION: PYDANTIC SUB-MODELS


# KLASS: QuestProgress
class QuestProgress(BaseModel):
    """Represents the progress within an active party quest."""

    model_config = ConfigDict(
        extra="ignore",  # Ignore fields like quest key/leader here
        populate_by_name=True,
        frozen=False,  # Progress changes
    )

    # Boss quest progress
    up: float = Field(0.0, description="Boss damage dealt or positive habit progress.")
    down: float = Field(0.0, description="Damage taken or negative habit progress.")
    hp: float | None = Field(None, description="Boss current HP (if applicable).")  # Boss HP can be None
    rage: float | None = Field(None, description="Boss current Rage (if applicable).")  # Rage can be None

    # Collection quest progress
    # Raw collect goals are dict like {item_key: count_needed} - Use extra='allow'? Or explicit model?
    # Pydantic can parse dicts directly, 'extra=allow' is simplest if keys vary widely
    # collect: dict[str, int] = Field(default_factory=dict, description="Item collection goals (key: count needed).")
    # Prefer defining if structure is consistent from API
    collect_goals: dict[str, int] = Field(default_factory=dict, alias="collect", description="Item collection goals (key: count needed).")

    # Actual collected count (often separate field from goals)
    collected_items_count: int = Field(0, alias="collectedItems", description="Items collected so far for collection quests.")

    # Validator for numeric fields
    @field_validator("up", "down", "hp", "rage", mode="before")
    @classmethod
    def ensure_float_or_none(cls, value: Any) -> float | None:
        """Ensures numeric progress fields are floats if present, else None."""
        if value is None:
            return None
        try:
            return float(value)
        except (ValueError, TypeError):
            log.debug(f"Could not parse quest progress value: {value!r}. Defaulting.")
            # Defaulting to 0 might be wrong if None meant "not applicable"
            # Return 0 for up/down, None for hp/rage might be better? Let's default to 0.0 for now.
            return 0.0  # Or decide based on field if None is more appropriate default

    @field_validator("collected_items_count", mode="before")
    @classmethod
    def ensure_int(cls, value: Any) -> int:
        """Ensures collected_items is an integer, defaulting to 0."""
        if value is None:
            return 0
        try:
            return int(value)
        except (ValueError, TypeError):
            log.debug(f"Could not parse collected_items value: {value!r}. Setting to 0.")
            return 0

    # Collect goals validation? Could ensure keys are str, values are int.
    # Pydantic dict validation usually handles basic types.

    def __repr__(self) -> str:
        """Concise representation."""
        parts = []
        if self.hp is not None:
            parts.append(f"hp={self.hp:.1f}")
        if self.rage is not None:
            parts.append(f"rage={self.rage:.1f}")
        # Only show up/down if non-zero or hp exists (relevant for boss quests)
        if self.hp is not None or self.up != 0.0:
            parts.append(f"up={self.up:.1f}")
        if self.hp is not None or self.down != 0.0:
            parts.append(f"down={self.down:.1f}")
        if self.collect_goals:
            goal_str = ",".join(f"{k}:{v}" for k, v in self.collect_goals.items())
            parts.append(f"collect={self.collected_items_count}/[{goal_str}]")
        progress_str = ", ".join(parts) if parts else "No Progress"
        return f"QuestProgress({progress_str})"


# KLASS: QuestInfo
class QuestInfo(BaseModel):
    """Represents the metadata about the party's current quest."""

    model_config = ConfigDict(
        extra="ignore",  # Ignore fields like webhook url etc.
        populate_by_name=True,
        frozen=False,  # Status changes (active, completed)
    )

    key: str | None = Field(None, description="Unique key for the quest (e.g., 'basilisk'). Null if no quest.")
    active: bool = Field(False, description="Is the quest active (invitation sent/accepted)?")
    # Use alias for API's `RSVPNeeded`
    rsvp_needed: bool = Field(False, alias="RSVPNeeded", description="Does leader need to accept invites?")
    # Completion status: Can be timestamp string, 'allGuilds', or null/absent. Store as string.
    completed_status: str | None = Field(None, alias="completed", description="Completion status or timestamp string.")
    leader_id: str | None = Field(None, alias="leader", description="User ID of the quest leader/inviter.")

    # Member RSVP status {userId: bool | null} - Null means pending? bool means accepted/declined?
    # API might use `true` for accepted, `false` for declined?, absence=pending?
    # Using bool | None for flexibility.
    member_rsvp: dict[str, bool | None] = Field(default_factory=dict, alias="members")

    # Nested progress model
    progress: QuestProgress = Field(default_factory=QuestProgress)

    # --- Derived Properties ---
    @property
    def completed_timestamp(self) -> datetime | None:
        """Parses completed_status into a datetime if possible."""
        if self.completed_status:
            handler = DateTimeHandler(timestamp=self.completed_status)
            return handler.utc_datetime  # Returns None if parsing fails
        return None

    @property
    def is_active_and_ongoing(self) -> bool:
        """Calculates if the quest is active AND not yet completed."""
        # Active flag must be true AND completed_status must be missing/null/empty
        return self.active and not self.completed_status

    # --- Representation ---
    def __repr__(self) -> str:
        """Concise representation."""
        status = "Inactive"
        if self.completed_status:
            completion_time = self.completed_timestamp
            status = f"Completed ({completion_time.strftime('%Y-%m-%d')})" if completion_time else f"Completed ({self.completed_status})"
        elif self.active:
            status = "Active/Ongoing" if self.is_active_and_ongoing else "Pending"  # Or Invited?

        key_str = f"key='{self.key}'" if self.key else "No Quest"
        return f"QuestInfo({key_str}, status={status})"


# KLASS: PartyMember (Basic info available directly in party data)
class PartyMember(BaseModel):
    """Represents basic info about a member as found in party data (usually just ID)."""

    # API `/groups/{groupId}` returns a list of member *IDs*.
    # Full member details require additional calls.
    # So this model primarily just holds the ID found in the party structure itself.
    # If the API endpoint DOES provide more nested details, expand this model.

    model_config = ConfigDict(extra="ignore", populate_by_name=True, frozen=True)
    # Assuming the list contains just IDs, not full member objects
    id: str  # This would be validated from the list of strings if members = ['id1', 'id2']

    # If API returns list of member objects:
    # id: str = Field(..., alias="_id") # Map from _id if needed
    # display_name: str | None = None # Often requires separate fetch
    # username: str | None = None     # Often requires separate fetch


# ──────────────────────────────────────────────────────────────────────────────


# SECTION: MAIN PARTY MODEL


# KLASS: Party
class Party(BaseModel):
    """Represents a Habitica Party group object, including quest and chat."""

    model_config = ConfigDict(
        extra="ignore",  # Ignore fields like leader message, leader object details
        populate_by_name=True,
        validate_assignment=True,  # Re-validate on assignment if needed
        arbitrary_types_allowed=True,  # Needed for MessageList initially, might be removable if MessageList validation works directly
    )

    # --- Core Identification & Info ---
    id: str = Field(..., description="Unique party ID (mapped from _id).")
    name: str = Field("Unnamed Party", description="Party name (parsed emoji).")
    description: str = Field("", description="Party description (parsed emoji).")
    # summary: str | None = Field(None, description="Party summary/tagline (parsed emoji).") # Less common

    # --- Leader & Members ---
    leader_id: str | None = Field(None, description="User ID of the party leader.")  # Extracted by validator
    # The raw API has `memberCount` and a separate `members` list (usually just IDs).
    # We store the count directly. Member objects would need separate loading/linking.
    member_count: int = Field(0, alias="memberCount", description="Number of members in the party.")
    # Optional: store member IDs if useful
    # member_ids: list[str] = Field(default_factory=list)

    # --- Quest ---
    quest: QuestInfo = Field(default_factory=QuestInfo, description="Details of the current party quest.")

    # --- Chat ---
    # Use the MessageList Pydantic model. Validation will happen here.
    # Context (`current_user_id`) is needed for MessageList validation.
    # Exclude from default serialization unless explicitly included.
    chat: MessageList | None = Field(None, description="Party chat messages.")

    # --- Sorting Info ---
    # order: str | None = Field(None, description="Field used for sorting members.") # Less commonly used?
    # order_ascending: bool | None = Field(None, alias="orderAscending")

    # --- Internal attribute for storing fetched static quest data ---
    _static_quest_details: StaticQuestData | None = PrivateAttr(default=None)

    # --- Validators ---

    @model_validator(mode="before")
    @classmethod
    def prepare_data(cls, data: Any) -> dict[str, Any]:
        """Prepare raw data: Map IDs, extract leader ID."""
        if not isinstance(data, dict):
            return data  # Let Pydantic handle type error

        values = data.copy()

        # Map _id -> id
        if "_id" in values and "id" not in values:
            values["id"] = values["_id"]

        # Extract leader ID from potentially nested structure
        leader_info = values.get("leader")
        if isinstance(leader_info, str):
            # Leader is just an ID string
            values["leader_id"] = leader_info
        elif isinstance(leader_info, dict):
            # Leader is an object, get ID from it
            values["leader_id"] = leader_info.get("_id") or leader_info.get("id")
        # 'leader' key itself might be absent

        # Handle potential direct 'chat' list/dict from API
        # Pydantic handles validating `chat` key against `MessageList` type hint

        # Extract member IDs? Assuming API provides `members` as list of IDs
        # member_data = values.get("members")
        # if isinstance(member_data, list) and all(isinstance(m, str) for m in member_data):
        #      values["member_ids"] = member_data

        return values

    # Ensure ID exists after potential mapping
    @field_validator("id", mode="after")
    @classmethod
    def check_id(cls, v: str) -> str:
        if not v or not isinstance(v, str):
            raise ValueError("Party ID (_id) is required and must be a string.")
        return v

    # Consolidate text parsing
    @field_validator("name", "description", mode="before")
    @classmethod
    def parse_text_emoji(cls, value: Any, info: FieldValidationInfo) -> str:
        """Parses text fields, replaces emoji, handles defaults."""
        default = "Unnamed Party" if info.field_name == "name" else ""
        if isinstance(value, str):
            parsed = emoji_data_python.replace_colons(value).strip()
            return parsed if parsed else default
        return default

    @field_validator("member_count", mode="before")
    @classmethod
    def ensure_int(cls, value: Any) -> int:
        """Ensure member_count is an integer."""
        if value is None:
            return 0
        try:
            return int(value)
        except (ValueError, TypeError):
            return 0

    # --- Methods ---

    @classmethod
    def create_from_raw_data(cls, raw_data: dict, current_user_id: str | None = None) -> Party:
        """Factory method to create Party, passing context for chat validation."""
        if not isinstance(raw_data, dict):
            log.error(f"Invalid raw data type for Party creation: Expected dict, got {type(raw_data)}")
            raise TypeError("Invalid input data for Party creation.")

        # Define the context required by MessageList validation
        validation_context = {"current_user_id": current_user_id}

        log.debug(f"Creating Party from raw data with context: {validation_context}")
        try:
            # Validate the raw data using the context
            # Pydantic will automatically pass this context down when validating nested models
            # like 'chat: MessageList' if the nested model's validator requests it.
            party_instance = cls.model_validate(raw_data, context=validation_context)
            log.debug(f"Party instance created successfully: {party_instance.id}")
            return party_instance
        except ValidationError as e:
            log.error(f"Validation failed creating Party model: {e}", exc_info=False)  # Log less verbose error
            log.debug(f"Failed raw data keys: {list(raw_data.keys())}")  # Log keys for debugging
            # Optionally log parts of the data that failed, be mindful of size/privacy
            # log.debug(f"Failing quest data: {raw_data.get('quest')}")
            # log.debug(f"Failing chat data sample: {raw_data.get('chat', [])[:2]}")
            raise  # Re-raise the specific Pydantic error
        except Exception as e:
            log.exception("Unexpected error creating Party from raw data.")  # Log full trace
            raise

    def get_chat_messages(self) -> list[Message]:
        """Returns the validated list of chat messages, or an empty list."""
        # Access validated messages from the nested MessageList model
        return self.chat.messages if self.chat else []

    async def fetch_and_set_static_quest_details(self, content_manager: StaticContentManager) -> StaticQuestData | None:
        """Fetches static quest details using the content manager and caches it internally.

        Args:
            content_manager: An instance of StaticContentManager.

        Returns:
            The fetched static quest data, or None if not found or no quest active.
        """
        if not self.quest or not self.quest.key:
            log.debug("Party has no active quest key. Cannot fetch static details.")
            self._static_quest_details = None
            return None

        log.debug(f"Fetching static quest details for key: '{self.quest.key}'")
        try:
            # Use the provided manager instance to get quest details
            static_data = await content_manager.get_quest(self.quest.key)
            if static_data:
                log.success(f"Successfully fetched static details for quest '{self.quest.key}'.")
                self._static_quest_details = static_data  # Store internally
                return static_data
            else:
                log.warning(f"Static details not found for quest key '{self.quest.key}'.")
                self._static_quest_details = None
                return None
        except Exception as e:
            log.exception(f"Error fetching static quest details for '{self.quest.key}': {e}")
            self._static_quest_details = None
            return None

    @property
    def static_quest_details(self) -> StaticQuestData | None:
        """Returns the internally cached static quest details (if fetched)."""
        return self._static_quest_details

    # --- Representation ---
    def __repr__(self) -> str:
        """Concise representation."""
        quest_repr = repr(self.quest) if self.quest else "No Quest"
        chat_len = len(self.chat.messages) if self.chat and self.chat.messages else 0
        name_preview = self.name[:30].replace("\n", " ") + ("..." if len(self.name) > 30 else "")

        return f"Party(id='{self.id}', name='{name_preview}', members={self.member_count}, chat={chat_len}, {quest_repr})"

    def __str__(self) -> str:
        """User-friendly representation (name)."""
        return self.name


# ──────────────────────────────────────────────────────────────────────────────

# SECTION: MAIN EXECUTION (Example/Test)


async def main():
    """Demo function to retrieve, process, and save party data."""
    log.info("--- Party Model Demo ---")
    party_instance: Party | None = None

    # Use USER_ID from config for context
    user_id_context = USER_ID
    if not user_id_context or user_id_context == "fallback_user_id_from_config":
        log.warning("Cannot run demo effectively: Valid USER_ID not found in config.")
        # Provide a dummy ID for testing if absolutely necessary
        user_id_context = "test-user-id-123"
        log.info(f"Using dummy USER_ID for context: {user_id_context}")

    try:
        # Ensure cache directory exists
        cache_dir = HABITICA_DATA_PATH / CACHE_SUBDIR
        cache_dir.mkdir(exist_ok=True, parents=True)
        raw_path = cache_dir / PARTY_RAW_FILENAME
        processed_path = cache_dir / PARTY_PROCESSED_FILENAME

        # 1. Get data from API
        log.info("Fetching party data from API...")
        api = HabiticaClient()  # Assumes configured
        raw_data = await api.get_party_data()

        if not raw_data:
            log.error("Failed to fetch party data from API. Exiting.")
            return None

        # Optionally save raw data
        save_json(raw_data, raw_path)

        # 2. Create Party model using the factory method with context
        log.info(f"Processing raw party data (using user ID '{user_id_context}' for chat context)...")
        party_instance = Party.create_from_raw_data(raw_data, current_user_id=user_id_context)
        log.success("Party model created successfully.")

        # 3. Example Data Access
        print(f"  Party Name: {party_instance.name}")
        print(f"  Party ID: {party_instance.id}")
        print(f"  Leader ID: {party_instance.leader_id}")
        print(f"  Member Count: {party_instance.member_count}")
        print(f"  Quest Info: {party_instance.quest}")
        print(f"  Quest Progress: {party_instance.quest.progress}")
        print(f"  Chat Message Count: {len(party_instance.get_chat_messages())}")

        # 4. Example: Fetch and store static quest data
        log.info("Attempting to fetch static quest details...")
        # Requires StaticContentManager to be instantiated
        content_manager = StaticContentManager()  # Use default paths
        static_details = await party_instance.fetch_and_set_static_quest_details(content_manager)
        if static_details:
            print(f"  Fetched Static Quest Title: {static_details.text}")  # Access field from StaticQuestData
        elif party_instance.quest.key:
            print(f"  Could not fetch static details for quest '{party_instance.quest.key}'.")
        else:
            print("  No active quest to fetch details for.")

        # 5. Save processed data (using pydantic helper)
        # Choose whether to include chat by controlling the dump excludes manually if needed,
        # or rely on the exclude=True in the Field definition. model_dump respects exclude=True by default.
        log.info(f"Saving processed party data to {processed_path}...")
        if save_pydantic_model(party_instance, processed_path):  # Chat excluded by default
            log.success("Processed party data saved.")
            # To explicitly include chat:
            data_to_save = party_instance.model_dump(mode="json", exclude_none=True)  # Pydantic handles exclude=True
            save_json(data_to_save, cache_dir / "party_with_chat.json")
        else:
            log.error("Failed to save processed party data.")

    except ValidationError as e:
        log.error(f"Pydantic validation error during party processing: {e}")
    except ConnectionError as e:
        log.error(f"API connection error fetching party data: {e}")
    except Exception as e:
        log.exception(f"An unexpected error occurred in the party demo: {e}")

    return party_instance


if __name__ == "__main__":
    import asyncio

    logging.basicConfig(level=logging.DEBUG, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")
    asyncio.run(main())

# ──────────────────────────────────────────────────────────────────────────────
-e 
-------- END OF FILE models/party.py --------

-------- START OF FILE models/tag.py --------
# pixabit/models/tag.py

# ─── Model ────────────────────────────────────────────────────────────────────
#            Habitica Tag Models (Simple Version)
# ──────────────────────────────────────────────────────────────────────────────

# SECTION: MODULE DOCSTRING
"""Defines basic Pydantic models for representing Habitica Tags."""

# SECTION: IMPORTS
from __future__ import annotations

import json
from pathlib import Path
from typing import Any, Iterator  # Changed List -> list etc below

import emoji_data_python
from pydantic import (
    BaseModel,
    ConfigDict,
    Field,
    ValidationError,
    field_validator,
    model_validator,
)

from pixabit.api.client import HabiticaClient
from pixabit.helpers._json import load_json, save_json, save_pydantic_model

# Local Imports (assuming helpers and api are accessible)
# Assuming logger, json helper, client, and config are setup
try:
    from pixabit.config import CACHE_DIR
    from pixabit.helpers._logger import log
except ImportError:
    import logging

    log = logging.getLogger(__name__)
    log.addHandler(logging.NullHandler())
    # Provide simple fallbacks if imports fail during refactoring/testing
    CACHE_DIR = Path("./pixabit_cache")
    log.warning("tag.py: Could not import helpers/api/config. Using fallbacks.")

CACHE_SUBDIR = "content"
TAGS_FILENAME = "tags.json"
PROCESSED_TAGS_FILENAME = "processed_tags.json"


# SECTION: TAG MODEL


# KLASS: Tag
class Tag(BaseModel):
    """Represents a single Habitica Tag."""

    model_config = ConfigDict(
        extra="ignore",  # Ignore extra fields from API
        frozen=False,  # Tags might be editable (name, etc.)
        validate_assignment=True,  # Re-validate on attribute assignment
    )

    id: str = Field(..., description="Unique tag ID.")  # Make ID mandatory
    name: str = Field("", description="Tag name (parsed emoji).")  # Default to empty string
    challenge: bool = Field(False, description="True if tag is associated with a challenge.")
    # group: str | None = Field(None, description="Group ID if it's a group tag?") # Check API if needed
    # user: str | None = Field(None, description="User ID of creator?") # Check API if needed

    # Optional internal tracking, not directly from API typically
    position: int | None = Field(None, description="Order/position within the tag list.", exclude=True)  # Exclude from dumps

    # --- Validators ---
    @field_validator("name", mode="before")
    @classmethod
    def parse_name_emoji(cls, value: Any) -> str:
        """Parses tag name and replaces emoji shortcodes."""
        if isinstance(value, str):
            parsed = emoji_data_python.replace_colons(value)
            # Optional: Strip leading/trailing whitespace
            return parsed.strip()
        log.debug(f"Received non-string value for tag name: {value!r}. Using empty string.")
        return ""  # Return empty string if name is not a string or None

    # --- Methods ---
    def __repr__(self) -> str:
        """Concise representation."""
        chal_flag = " (Challenge)" if self.challenge else ""
        name_preview = self.name.replace("\n", " ")  # Avoid newlines in repr
        return f"Tag(id='{self.id}', name='{name_preview}'{chal_flag})"

    def __str__(self) -> str:
        """User-friendly string representation (often just the name)."""
        return self.name


# ──────────────────────────────────────────────────────────────────────────────


# SECTION: TAG LIST MODEL


# KLASS: TagList
class TagList(BaseModel):
    """Container for managing a list of Tag objects, adhering to Pydantic."""

    model_config = ConfigDict(
        extra="forbid",  # No extra fields expected on the list itself
        frozen=False,  # Allow adding/removing tags
        arbitrary_types_allowed=False,
    )

    tags: list[Tag] = Field(default_factory=list, description="The list of Tag objects.")

    # Optional: Add validation context if needed later
    # current_user_id: str | None = Field(None, description="Contextual user ID", exclude=True)

    @model_validator(mode="after")
    def update_tag_positions(self) -> TagList:
        """Updates the position attribute for each tag based on its order."""
        for i, tag in enumerate(self.tags):
            tag.position = i  # Assign position based on current order
        return self

    # --- Factory Methods ---
    @classmethod
    def from_raw_data(cls, raw_list: list[dict[str, Any]]) -> TagList:
        """Creates a TagList by validating raw dictionary data."""
        if not isinstance(raw_list, list):
            log.error(f"Invalid input for TagList.from_raw_data: Expected list, got {type(raw_list)}.")
            return cls(tags=[])  # Return empty list on error

        validated_tags: list[Tag] = []
        for i, tag_data in enumerate(raw_list):
            if not isinstance(tag_data, dict):
                log.warning(f"Skipping invalid item at index {i} in raw tag data (expected dict, got {type(tag_data)}).")
                continue
            try:
                # Validate each item into a Tag model
                tag_instance = Tag.model_validate(tag_data)
                validated_tags.append(tag_instance)
            except ValidationError as e:
                tag_id = tag_data.get("id", "N/A")
                log.error(f"Validation failed for tag data (ID: {tag_id}) at index {i}: {e}")
                # Optionally skip invalid tags or raise error depending on strictness

        # Instantiate TagList with validated tags (positions are handled by model_validator)
        return cls(tags=validated_tags)

    # --- List-like Methods ---
    def __len__(self) -> int:
        """Return the number of tags."""
        return len(self.tags)

    def __iter__(self) -> Iterator[Tag]:
        """Return an iterator over the tags."""
        return iter(self.tags)

    def __getitem__(self, index: int | slice) -> Tag | list[Tag]:
        """Get tag(s) by index or slice."""
        return self.tags[index]

    def __contains__(self, item: Tag | str) -> bool:
        """Check if a tag (by instance or ID) is in the list."""
        if isinstance(item, str):  # Check by ID
            return any(tag.id == item for tag in self.tags)
        elif isinstance(item, Tag):  # Check by instance (object equality)
            return item in self.tags
        return False

    # --- Mutating Methods ---
    def add_tag(self, tag: Tag) -> None:
        """Adds a tag to the list if it doesn't exist (by ID). Updates positions."""
        if not isinstance(tag, Tag):
            log.warning(f"Attempted to add invalid type to TagList: {type(tag)}")
            return
        if tag.id not in self:
            self.tags.append(tag)
            self.update_tag_positions()  # Recalculate positions after add
        else:
            log.debug(f"Tag with ID '{tag.id}' already exists. Skipping add.")

    def remove_tag(self, tag_id: str) -> bool:
        """Removes a tag by ID and updates positions. Returns True if removed."""
        initial_len = len(self.tags)
        self.tags = [tag for tag in self.tags if tag.id != tag_id]
        removed = len(self.tags) < initial_len
        if removed:
            self.update_tag_positions()  # Recalculate positions after remove
        return removed

    def update_tag(self, tag_id: str, update_data: dict[str, Any]) -> bool:
        """Update a tag by ID. Returns True if updated."""
        tag = self.get_by_id(tag_id)
        try:
            updated_tag = tag.model_validate(update_data, update=True)
            if not updated_tag:
                log.warning(f"Failed to process metadata for edited tag {tag_id[:8]}")
            log.info(f"Edited task: {tag_id[:8]})")

            return updated_tag

        except ValidationError as e:
            log.error(f"Validation error editing tag {tag_id[:8]}: {e}")
            return None
        except Exception as e:
            log.exception(f"Error editing tag {tag_id[:8]}: {e}")
            return None

    def reorder_tags(self, tag_id: str, new_position: int) -> bool:
        """Moves a tag to a new position index and updates all positions."""
        tag_to_move = self.get_by_id(tag_id)
        if not tag_to_move:
            log.warning(f"Cannot reorder: Tag with ID '{tag_id}' not found.")
            return False

        try:
            current_index = self.tags.index(tag_to_move)
            # Ensure position is within bounds (0 to len)
            new_position_clamped = max(0, min(new_position, len(self.tags) - 1))

            # Remove and insert at the new position
            self.tags.pop(current_index)
            self.tags.insert(new_position_clamped, tag_to_move)

            # Update all positions after reordering
            self.update_tag_positions()
            return True
        except ValueError:  # Should not happen if get_by_id worked, but safety
            log.error(f"Error finding index for tag ID '{tag_id}' during reorder.")
            return False

    # --- Filtering/Access Methods ---
    def get_by_id(self, tag_id: str) -> Tag | None:
        """Finds a tag by its unique ID."""
        return next((tag for tag in self.tags if tag.id == tag_id), None)

    def get_user_tags(self) -> list[Tag]:
        """Returns only the user-created tags (non-challenge tags)."""
        return [tag for tag in self.tags if not tag.challenge]

    def get_challenge_tags(self) -> list[Tag]:
        """Returns only the challenge-associated tags."""
        return [tag for tag in self.tags if tag.challenge]

    def filter_by_name(self, name_part: str, case_sensitive: bool = False) -> list[Tag]:
        """Filters tags by name containing a substring."""
        if not case_sensitive:
            name_part_lower = name_part.lower()
            return [tag for tag in self.tags if name_part_lower in tag.name.lower()]
        else:
            return [tag for tag in self.tags if name_part in tag.name]

    # --- Serialization ---
    # No custom save_to_json needed, use the helper function
    # No custom model_dump needed, Pydantic handles it

    def save(self, filename: str = PROCESSED_TAGS_FILENAME, folder: str | Path = CACHE_DIR / CACHE_SUBDIR) -> bool:
        """Saves the TagList model to a JSON file using the helper."""
        log.info(f"Saving {len(self.tags)} tags...")
        return save_pydantic_model(self, filename, folder=folder, indent=2)

    # --- Representation ---
    def __repr__(self) -> str:
        """Detailed representation showing counts."""
        user_count = len(self.get_user_tags())
        chal_count = len(self.get_challenge_tags())
        return f"TagList(total={len(self.tags)}, user={user_count}, challenge={chal_count})"


# ──────────────────────────────────────────────────────────────────────────────


# SECTION: MAIN EXECUTION (Example/Test)
async def main():
    """Demo function to retrieve, process, and save tags."""
    log.info("Starting Tag Processing Demo...")
    tag_list_instance: TagList | None = None
    try:
        # Ensure cache directory exists
        cache_path = Path(CACHE_DIR) / CACHE_SUBDIR
        cache_path.mkdir(exist_ok=True, parents=True)
        raw_json_path = cache_path / TAGS_FILENAME
        processed_json_path = cache_path / PROCESSED_TAGS_FILENAME

        # 1. Fetch tags from API
        log.info("Fetching tags from API...")
        api = HabiticaClient()  # Assumes client is configured
        raw_tags = await api.get_tags()
        log.success(f"Fetched {len(raw_tags)} tags from API.")

        # Optionally save raw data (using generic save_json)
        save_json(raw_tags, raw_json_path)
        log.info(f"Raw tag data saved to {raw_json_path}")

        # 2. Process raw data into TagList model
        log.info("Processing raw data into TagList model...")
        tag_list_instance = TagList.from_raw_data(raw_tags)
        log.success(f"Processed into TagList: {tag_list_instance}")

        # 3. Example: Accessing data
        print(f"  - User tags count: {len(tag_list_instance.get_user_tags())}")
        if tag_list_instance.tags:
            first_tag = tag_list_instance.tags[0]
            print(f"  - First tag: {first_tag}")
            found_tag = tag_list_instance.get_by_id(first_tag.id)
            print(f"  - Found by ID: {found_tag is not None}")

        # 4. Save the processed TagList model
        log.info(f"Saving processed TagList to {processed_json_path}...")
        if tag_list_instance.save(filename=processed_json_path.name, folder=processed_json_path.parent):
            log.success("Processed tags saved successfully.")
        else:
            log.error("Failed to save processed tags.")

    except ValidationError as e:
        log.error(f"Pydantic Validation Error during tag processing: {e}")
    except ConnectionError as e:  # Example API error
        log.error(f"API Connection Error: Failed to fetch tags - {e}")
    except Exception as e:
        log.exception(f"An unexpected error occurred in the tag processing demo: {e}")  # Log full trace

    log.info("Tag Processing Demo Finished.")
    return tag_list_instance  # Return for potential further use


if __name__ == "__main__":
    import asyncio

    asyncio.run(main())

# ──────────────────────────────────────────────────────────────────────────────
-e 
-------- END OF FILE models/tag.py --------

-------- START OF FILE models/tag_factory.py --------
# pixabit/models/tag_factory.py

# ─── Model ────────────────────────────────────────────────────────────────────
#       Advanced Habitica Tag Models & Factory (Config-Driven)
# ──────────────────────────────────────────────────────────────────────────────

# SECTION: MODULE DOCSTRING
"""Defines advanced Pydantic models for representing Habitica Tags with hierarchy
(Parent/Subtag) and a factory (`TagFactory`) to create them based on a TOML
configuration file mapping specific tag IDs/symbols to attributes and types.
Provides an enhanced `TagList` optimized for these hierarchical tags.
"""

# SECTION: IMPORTS
from __future__ import annotations

# --- Python Standard Library Imports ---
import json
import re
from collections import defaultdict
from contextlib import contextmanager
from functools import lru_cache
from pathlib import Path
from typing import Any, ClassVar, Iterator, Literal  # Use standard lowercase list etc below

import emoji_data_python
import tomllib  # Python 3.11+, use tomli library for < 3.11

# --- Third-Party Library Imports ---
from pydantic import (
    BaseModel,
    ConfigDict,
    Field,
    TypeAdapter,  # For direct JSON parsing if needed
    ValidationError,
    field_validator,
    model_validator,
)

from pixabit.api.client import HabiticaClient
from pixabit.helpers._json import load_json, save_json, save_pydantic_model

# Local Imports (assuming helpers and api are accessible)
# Assuming logger, json helper, client, and config are setup
try:
    from pixabit.config import CACHE_DIR
    from pixabit.helpers._logger import log
except ImportError:
    import logging

    log = logging.getLogger(__name__)
    log.addHandler(logging.NullHandler())
    log.warning("tag_factory.py: Could not import helpers. Using fallbacks.")
    # Provide simple fallbacks if imports fail during refactoring/testing
    CACHE_DIR = Path("./pixabit_cache")
    log.warning("tag.py: Could not import helpers/api/config. Using fallbacks.")
from pathlib import Path
from typing import Dict, Optional

from pydantic_settings import BaseSettings, SettingsConfigDict

CACHE_SUBDIR = "content"
TAGS_FILENAME = "tags.json"
PROCESSED_TAGS_FILENAME = "processed_tags.json"
from pathlib import Path

# Maps special symbols found in tag text to short attribute names
ATTRIBUTE_SYMBOL_MAP: dict[str, str] = {
    "🜄": "con",  # Water symbol maps to Constitution
    "🜂": "str",  # Fire symbol maps to Strength
    "🜁": "int",  # Air symbol maps to Intelligence
    "🜃": "per",  # Earth symbol maps to Perception
    "᛭": "legacy",  # Nordic cross symbol maps to Legacy
}

# Maps configuration *keys* from the TOML file to internal attribute short names
# These MUST match the keys used in your `tags.toml` file
ATTRIBUTE_CONFIG_KEY_MAP: dict[str, str] = {
    "TAG_ID_ATTR_STR": "str",
    "TAG_ID_ATTR_INT": "int",
    "TAG_ID_ATTR_CON": "con",
    "TAG_ID_ATTR_PER": "per",
    "TAG_ID_LEGACY": "legacy",
    "TAG_ID_CHALLENGE": "challenge",
    "TAG_ID_PERSONAL": "personal",
    # Add other specific tag IDs you want to map if needed
}

# Precompile regex for efficiency (matches any *single* symbol from the map)
# Ensures symbols are treated individually if multiple appear in text
ATTRIBUTE_SYMBOL_REGEX = re.compile(f"({'|'.join(re.escape(s) for s in ATTRIBUTE_SYMBOL_MAP.keys())})")

DEFAULT_ATTRIBUTE = "str"  # Default attribute if none detected


class TagsConfig(BaseSettings):
    # Mapeos de ID de tag a atributo
    TAG_ID_ATTR_STR: str
    TAG_ID_ATTR_INT: str
    TAG_ID_ATTR_CON: str
    TAG_ID_ATTR_PER: str
    TAG_ID_NO_ATTR: str
    TAG_ID_LEGACY: Optional[str] = None
    TAG_ID_CHALLENGE: Optional[str] = None
    TAG_ID_PERSONAL: Optional[str] = None

    # Configuración adicional
    DEFAULT_ATTRIBUTE: str = "str"
    CACHE_DIR: Path = Path("./pixabit_cache")

    # Configuración para cargar desde archivo
    model_config = SettingsConfigDict(env_file=".env", env_file_encoding="utf-8", extra="ignore")

    def get_id_to_attribute_map(self) -> Dict[str, str]:
        """Genera el mapeo de ID a atributo basado en la configuración actual."""
        mapping = {}
        # Agrega mappings para todos los campos TAG_ID_*
        for field_name, field_value in self.model_dump().items():
            if field_name.startswith("TAG_ID_") and field_value:
                # Extrae la parte después de TAG_ID_ y conviértela a lowercase
                attr_name = field_name[7:].lower()
                mapping[field_value] = attr_name
        return mapping


# SECTION: BASE MODELS (for TagFactory context)


# KLASS: BaseTag
class BaseTag(BaseModel):
    """Base model for tags created by the TagFactory."""

    model_config = ConfigDict(
        extra="ignore",  # Ignore extra fields from raw data
        frozen=False,  # Allow modification (e.g., position)
        validate_assignment=True,  # Validate on assignment
    )

    id: str = Field(..., description="Unique tag ID.")
    name: str = Field("", description="Tag name (parsed emoji).")  # Renamed from text for consistency
    challenge: bool = Field(False, description="Is this a challenge tag?")
    group: str | None = Field(None, description="Associated group ID (if any).")  # If API provides it

    # --- Factory-assigned fields ---
    tag_type: Literal["base", "parent", "subtag"] = Field("base", description="Type determined by factory (base, parent, subtag).")
    parent_id: str | None = Field(None, description="ID of the parent tag if this is a subtag.")
    attribute: str | None = Field(None, description="Associated attribute (str, con, etc.) determined by factory.")
    # --- End Factory-assigned ---

    position: int | None = Field(None, description="Calculated position within the list.", exclude=True)

    # --- Validators ---
    @field_validator("name", mode="before")
    @classmethod
    def parse_name_emoji(cls, value: Any) -> str:
        """Parses tag name, replaces emoji shortcodes, strips whitespace."""
        if isinstance(value, str):
            parsed = emoji_data_python.replace_colons(value)
            return parsed.strip()
        log.debug(f"Received non-string for tag name: {value!r}. Using empty string.")
        return ""

    # --- Properties & Methods ---
    @property
    def display_name(self) -> str:
        """Returns the primary name for display."""
        return self.name  # Can be overridden in subclasses

    def is_parent(self) -> bool:
        """Checks if this tag is classified as a parent."""
        return self.tag_type == "parent"

    def is_subtag(self) -> bool:
        """Checks if this tag is classified as a subtag."""
        return self.tag_type == "subtag"

    def __repr__(self) -> str:
        """Concise representation."""
        props = f"type={self.tag_type}"
        if self.parent_id:
            props += f", parent={self.parent_id}"
        if self.attribute:
            props += f", attr={self.attribute}"
        chal = " (Chal)" if self.challenge else ""
        return f"BaseTag(id='{self.id}', name='{self.name}'{chal}, {props})"

    def __str__(self) -> str:
        """User-friendly representation."""
        return self.display_name


# KLASS: ParentTag
class ParentTag(BaseTag):
    """Represents a parent tag (e.g., an attribute category), determined by TagFactory."""

    tag_type: Literal["parent"] = "parent"  # Override default

    @property
    def display_name(self) -> str:
        """Display name for Parent tag (usually just its name)."""
        # Optionally add prefix/suffix like "[ATTR] Name" if needed
        return self.name


# KLASS: SubTag
class SubTag(BaseTag):
    """Represents a subtag associated with a ParentTag, determined by TagFactory."""

    tag_type: Literal["subtag"] = "subtag"  # Override default

    @property
    def display_name(self) -> str:
        """Display name for SubTag. Maybe remove prefix symbol if present."""
        # Example: Remove the first symbol if it's mapped
        match = ATTRIBUTE_SYMBOL_REGEX.match(self.name)
        if match and match.group(1) in ATTRIBUTE_SYMBOL_MAP:
            # Return name after the symbol, stripped
            return self.name[len(match.group(1)) :].strip()
        return self.name  # Return original name otherwise


# Type alias for clarity
AnyTag = ParentTag | SubTag | BaseTag

# ──────────────────────────────────────────────────────────────────────────────


# SECTION: TAG FACTORY


class TagFactory:
    """Factory for creating specialized Tag objects (ParentTag, SubTag, BaseTag)
    based on rules defined in a TOML configuration file and detected symbols/IDs.
    """

    def __init__(self, config: TagsConfig = None):
        """Initialize factory with configuration from a TOML file.

        The TOML file should contain a [tags] section mapping
        configuration keys (like 'TAG_ID_ATTR_STR') to actual Habitica tag IDs.

        Args:
            config_path: Path to the TOML configuration file.

        Raises:
            FileNotFoundError: If the config file doesn't exist.
            tomllib.TOMLDecodeError: If the config file is invalid TOML.
            KeyError: If essential keys are missing in the config.
            TypeError: If config values are of unexpected types.
        """
        # Cargar config si no se proporciona
        self.config = config or TagsConfig()

        # Configura los mapeos usando los métodos de la config
        self.id_to_attribute = self.config.get_id_to_attribute_map()

        # 2. attribute_to_parent_id: Maps attribute names back to their *primary* Tag ID (for finding parents)
        self.attribute_to_parent_id: dict[str, str] = {v: k for k, v in self.id_to_attribute.items()}

        # 3. all_configured_ids: Set of all tag IDs mentioned in the config mapping.
        self.all_configured_ids = set(self.id_to_attribute.keys())

        log.info(f"TagFactory initialized. Mapped {len(self.id_to_attribute)} attributes to tag IDs.")
        log.debug(f"Attribute Map: {self.id_to_attribute}")

    def _detect_attribute_from_symbol(self, tag_name: str) -> str | None:
        """Detects attribute based on the *first* recognized symbol in the name."""
        match = ATTRIBUTE_SYMBOL_REGEX.search(tag_name)
        return ATTRIBUTE_SYMBOL_MAP.get(match.group(1)) if match else None

    def determine_tag_properties(self, tag_id: str, tag_name: str) -> tuple[Literal["parent", "subtag", "base"], str | None, str | None]:
        """Determines tag type, parent ID, and attribute based on factory rules.

        Returns:
            tuple: (tag_type, parent_id, attribute)
        """
        tag_type: Literal["parent", "subtag", "base"] = "base"
        parent_id: str | None = None
        attribute: str | None = None

        # 1. Check if the ID directly maps to a configured attribute (potential parent)
        if tag_id in self.id_to_attribute:
            tag_type = "parent"
            attribute = self.id_to_attribute[tag_id]
            parent_id = None  # Parents don't have parents
            return tag_type, parent_id, attribute

        # 2. If not a parent, check for symbols in the name to infer attribute and find parent
        else:
            detected_attribute = self._detect_attribute_from_symbol(tag_name)
            if detected_attribute:
                # Find the parent ID corresponding to this attribute
                parent_id = self.attribute_to_parent_id.get(detected_attribute)
                if parent_id:
                    tag_type = "subtag"
                    attribute = detected_attribute
                else:
                    # Symbol found, but no configured parent for that attribute
                    log.warning(f"Tag '{tag_name}' (ID:{tag_id}) has symbol for '{detected_attribute}' but no parent tag configured for that attribute.")
                    attribute = detected_attribute  # Still assign attribute, but maybe type='base'
                    tag_type = "base"  # Or keep 'subtag' without parent? Let's make it base.

        # 3. Assign default attribute if none found so far
        if attribute is None:
            attribute = DEFAULT_ATTRIBUTE

        return tag_type, parent_id, attribute

    def create_tag(self, raw_data: dict[str, Any], position: int | None = None) -> AnyTag:
        """Creates the appropriate Tag (ParentTag, SubTag, BaseTag) from raw data.

        Args:
            raw_data: A dictionary containing raw tag data (must have 'id', should have 'name').
            position: Optional list index for the tag.

        Returns:
            An instance of ParentTag, SubTag, or BaseTag.

        Raises:
            ValidationError: If raw_data fails validation against the base model.
            KeyError: If 'id' is missing in raw_data.
        """
        if "id" not in raw_data:
            raise KeyError("Tag data must contain an 'id' field.")

        tag_id = raw_data["id"]
        # Prioritize 'name', fallback to 'text' if Habitica API uses that sometimes
        tag_name = raw_data.get("name", raw_data.get("text", ""))
        raw_data["name"] = tag_name  # Ensure 'name' field exists for validation

        tag_type, parent_id, attribute = self.determine_tag_properties(tag_id, tag_name)

        # Prepare data for model validation
        model_data = {
            **raw_data,
            "tag_type": tag_type,
            "parent_id": parent_id,
            "attribute": attribute,
            "position": position,
        }

        # Validate and instantiate the correct model type
        try:
            if tag_type == "parent":
                return ParentTag.model_validate(model_data)
            elif tag_type == "subtag":
                return SubTag.model_validate(model_data)
            else:  # 'base'
                return BaseTag.model_validate(model_data)
        except ValidationError as e:
            log.error(f"Validation failed creating tag ID '{tag_id}' with type '{tag_type}': {e}")
            raise  # Re-raise validation error


# ──────────────────────────────────────────────────────────────────────────────


# SECTION: TAG LIST (for TagFactory context)


# KLASS: TagList (Factory-Aware)
class TagList(BaseModel):
    """A Pydantic-based list-like collection for managing advanced Tag objects
    (ParentTag, SubTag, BaseTag) created by a TagFactory. Provides methods
    for hierarchical access and manipulation.
    """

    model_config = ConfigDict(
        extra="forbid",
        frozen=False,  # List itself can be modified
        arbitrary_types_allowed=True,  # Needed because list contains Union[ParentTag, ...]
    )

    # Holds the tags created by the factory
    tags: list[AnyTag] = Field(default_factory=list)
    factory: TagFactory | None = Field(None, exclude=True)  # Keep reference if needed

    # --- Model Lifecycle ---
    @model_validator(mode="after")
    def _update_positions(self) -> TagList:
        """Ensure positions are set correctly after validation/modification."""
        for i, tag in enumerate(self.tags):
            # Only update if BaseTag (factory adds position during creation,
            # but this ensures it's updated after potential list manipulation)
            if isinstance(tag, BaseTag):
                tag.position = i
        return self

    # --- Factory Methods ---
    @classmethod
    def from_raw_data(cls, raw_list: list[dict], factory: TagFactory) -> TagList:
        """Creates TagList from raw API data using the provided TagFactory."""
        if not isinstance(raw_list, list):
            log.error(f"Invalid input for TagList.from_raw_data: Expected list, got {type(raw_list)}.")
            return cls(tags=[], factory=factory)

        processed_tags: list[AnyTag] = []
        for i, item in enumerate(raw_list):
            if not isinstance(item, dict):
                log.warning(f"Skipping invalid item at index {i} in raw tag data (expected dict).")
                continue
            try:
                tag_instance = factory.create_tag(item, position=i)
                processed_tags.append(tag_instance)
            except (ValidationError, KeyError) as e:
                # create_tag logs the specific error
                log.warning(f"Skipping tag at index {i} due to creation error: {e}")
                continue  # Skip invalid items

        # Create TagList instance (positions set by model_validator)
        return cls(tags=processed_tags, factory=factory)

    @classmethod
    def from_json_file(cls, json_path: str | Path, factory: TagFactory) -> TagList:
        """Loads TagList directly from a JSON file using the factory."""
        json_path = Path(json_path)
        log.debug(f"Loading tags from JSON file: {json_path}")
        raw_data = load_json(json_path)  # Use helper

        if raw_data is None or not isinstance(raw_data, list):
            log.error(f"Failed to load valid tag list data from {json_path}.")
            return cls(tags=[], factory=factory)  # Return empty on failure

        return cls.from_raw_data(raw_data, factory)

    # --- Accessors & Filters ---
    @property
    def parents(self) -> list[ParentTag]:
        """Get all ParentTag instances."""
        # Need to filter by instance type
        return [tag for tag in self.tags if isinstance(tag, ParentTag)]

    @property
    def subtags(self) -> list[SubTag]:
        """Get all SubTag instances."""
        return [tag for tag in self.tags if isinstance(tag, SubTag)]

    @property
    def base_tags(self) -> list[BaseTag]:
        """Get all BaseTag instances (not Parent or SubTag)."""
        # Exclude ParentTag and SubTag instances
        return [tag for tag in self.tags if type(tag) is BaseTag]

    def get_subtags_for_parent(self, parent_id: str) -> list[SubTag]:
        """Get all subtags explicitly linked to a specific parent ID."""
        return [tag for tag in self.tags if isinstance(tag, SubTag) and tag.parent_id == parent_id]

    def get_subtags_for_parent_attribute(self, attribute: str) -> list[SubTag]:
        """Get all subtags associated with a specific attribute (e.g., 'str')."""
        # Find parent ID for attribute first
        if not self.factory:
            return []  # Factory needed
        parent_id = self.factory.attribute_to_parent_id.get(attribute)
        if not parent_id:
            return []
        return self.get_subtags_for_parent(parent_id)

    def group_by_parent_id(self) -> dict[str, list[SubTag]]:
        """Groups SubTags by their parent ID."""
        grouped = defaultdict(list)
        for tag in self.subtags:  # Iterate only over subtags
            if tag.parent_id:
                grouped[tag.parent_id].append(tag)
        return dict(grouped)

    def group_by_attribute(self) -> dict[str, list[AnyTag]]:
        """Groups all tags by their detected attribute."""
        grouped = defaultdict(list)
        for tag in self.tags:
            if isinstance(tag, BaseTag) and tag.attribute:  # Check type and attribute existence
                grouped[tag.attribute].append(tag)
        return dict(grouped)

    @lru_cache(maxsize=128)
    def get_tag_by_id(self, tag_id: str) -> AnyTag | None:
        """Finds a tag by ID (cached for performance)."""
        # Ensure tag is BaseTag or subclass before checking id
        return next((tag for tag in self.tags if isinstance(tag, BaseTag) and tag.id == tag_id), None)

    def filter_by_challenge(self, is_challenge: bool) -> list[AnyTag]:
        """Filter tags by the 'challenge' flag."""
        return [tag for tag in self.tags if isinstance(tag, BaseTag) and tag.challenge == is_challenge]

    def filter_by_type(self, tag_type: Literal["parent", "subtag", "base"]) -> list[AnyTag]:
        """Filter tags by their factory-determined type."""
        return [tag for tag in self.tags if isinstance(tag, BaseTag) and tag.tag_type == tag_type]

    def sorted_by_position(self) -> list[AnyTag]:
        """Returns tags sorted by their calculated position."""
        # Ensure tags have a position and handle None gracefully
        return sorted([t for t in self.tags if isinstance(t, BaseTag)], key=lambda t: t.position if t.position is not None else float("inf"))

    # --- List-like Methods ---
    def __len__(self) -> int:
        return len(self.tags)

    def __iter__(self) -> Iterator[AnyTag]:
        return iter(self.tags)

    def __getitem__(self, index: int | slice) -> AnyTag | list[AnyTag]:
        return self.tags[index]

    def __contains__(self, item: AnyTag | str) -> bool:
        if isinstance(item, str):
            return self.get_tag_by_id(item) is not None
        elif isinstance(item, BaseTag):
            return item in self.tags  # Instance check
        return False

    # --- Mutating Methods ---
    def add_tag(self, tag: AnyTag) -> None:
        """Adds a tag to the list if it doesn't exist (by ID). Updates positions."""
        if not isinstance(tag, BaseTag):
            log.warning(f"Attempted to add invalid type to TagList: {type(tag)}")
            return
        if tag.id not in self:
            self.tags.append(tag)
            self._update_positions()  # Recalculate positions after add
        else:
            log.debug(f"Tag with ID '{tag.id}' already exists. Skipping add.")

    def remove_tag(self, tag_id: str) -> bool:
        """Removes a tag by ID and updates positions. Returns True if removed."""
        initial_len = len(self.tags)
        self.tags = [tag for tag in self.tags if tag.id != tag_id]
        removed = len(self.tags) < initial_len
        if removed:
            self._update_positions()  # Recalculate positions after remove
        return removed

    def update_tag(self, tag_id: str, update_data: dict[str, Any]) -> bool:
        """Update a tag by ID. Returns True if updated."""
        tag = self.get_by_id(tag_id)
        try:
            updated_tag = tag.model_validate(update_data, update=True)
            if not updated_tag:
                log.warning(f"Failed to process metadata for edited tag {tag_id[:8]}")
            log.info(f"Edited task: {tag_id[:8]})")

            return updated_tag

        except ValidationError as e:
            log.error(f"Validation error editing tag {tag_id[:8]}: {e}")
            return None
        except Exception as e:
            log.exception(f"Error editing tag {tag_id[:8]}: {e}")
            return None

    def reorder_tags(self, tag_id: str, new_position: int) -> bool:
        """Moves a tag to a new position index and updates all positions."""
        tag_to_move = self.get_by_id(tag_id)
        if not tag_to_move:
            log.warning(f"Cannot reorder: Tag with ID '{tag_id}' not found.")
            return False

        try:
            current_index = self.tags.index(tag_to_move)
            # Ensure position is within bounds (0 to len)
            new_position_clamped = max(0, min(new_position, len(self.tags) - 1))

            # Remove and insert at the new position
            self.tags.pop(current_index)
            self.tags.insert(new_position_clamped, tag_to_move)

            # Update all positions after reordering
            self._update_positions()
            return True
        except ValueError:  # Should not happen if get_by_id worked, but safety
            log.error(f"Error finding index for tag ID '{tag_id}' during reorder.")
            return False

    # --- Filtering/Access Methods ---
    def get_by_id(self, tag_id: str) -> AnyTag | None:
        """Finds a tag by its unique ID."""
        return next((tag for tag in self.tags if tag.id == tag_id), None)

    def get_user_tags(self) -> list[AnyTag]:
        """Returns only the user-created tags (non-challenge tags)."""
        return [tag for tag in self.tags if not tag.challenge]

    def get_challenge_tags(self) -> list[AnyTag]:
        """Returns only the challenge-associated tags."""
        return [tag for tag in self.tags if tag.challenge]

    def filter_by_name(self, name_part: str, case_sensitive: bool = False) -> list[AnyTag]:
        """Filters tags by name containing a substring."""
        if not case_sensitive:
            name_part_lower = name_part.lower()
            return [tag for tag in self.tags if name_part_lower in tag.name.lower()]
        else:
            return [tag for tag in self.tags if name_part in tag.name]

    # --- Serialization ---
    def save(self, filename: str = PROCESSED_TAGS_FILENAME, folder: str | Path = CACHE_DIR / CACHE_SUBDIR) -> bool:
        """Saves the TagList model to a JSON file using the helper."""
        log.info(f"Saving {len(self.tags)} tags...")
        return save_pydantic_model(self, filename, folder=folder, indent=2)

    # --- Representation ---
    def __repr__(self) -> str:
        """Detailed representation including counts by type."""
        counts = defaultdict(int)
        for tag in self.tags:
            if isinstance(tag, BaseTag):
                counts[tag.tag_type] += 1
        counts_str = ", ".join(f"{k}={v}" for k, v in counts.items())
        return f"TagList(total={len(self.tags)}, {counts_str})"


# ──────────────────────────────────────────────────────────────────────────────


# SECTION: LOADING FUNCTION & CONTEXT MANAGER


# FUNC: load_tags_from_json_with_factory
def load_tags_from_json_with_factory(json_path: str | Path, config_path: str | Path) -> TagList:
    """Loads and processes tags from a JSON file using a TagFactory based on TOML config."""
    log.info(f"Loading tags from '{json_path}' using config '{config_path}'...")
    try:
        factory = TagFactory(config_path=config_path)
        taglist = TagList.from_json_file(json_path, factory)
        log.success(f"Successfully loaded and processed {len(taglist)} tags.")
        return taglist
    except FileNotFoundError as e:
        log.error(f"File not found during tag loading: {e}")
        raise  # Re-raise for caller to handle
    except (json.JSONDecodeError, tomllib.TOMLDecodeError, ValidationError, KeyError, TypeError) as e:
        log.error(f"Error loading or processing tags: {e}")
        raise  # Re-raise for caller to handle
    except Exception as e:
        log.exception(f"An unexpected error occurred during tag loading: {e}")
        raise


@contextmanager
def tag_loading_context(json_path: str | Path, config_path: str | Path):
    """Context manager for safely loading tags via factory with error handling."""
    factory: TagFactory | None = None
    taglist: TagList | None = None
    error: Exception | None = None
    try:
        log.info(f"Entering tag loading context for '{json_path}'...")
        factory = TagFactory(config_path=config_path)
        taglist = TagList.from_json_file(json_path, factory)
        yield taglist, factory  # Yield the results
    except (FileNotFoundError, json.JSONDecodeError, tomllib.TOMLDecodeError, ValidationError, KeyError, TypeError) as e:
        log.error(f"Error within tag loading context: {e}")
        error = e
        yield None, factory  # Yield None on error, factory might be partially init
    except Exception as e:
        log.exception(f"An unexpected error occurred in tag loading context: {e}")
        error = e
        yield None, factory  # Yield None on unexpected error
    finally:
        if error:
            log.warning("Tag loading context finished with errors.")
        else:
            log.success("Tag loading context finished successfully.")


# ──────────────────────────────────────────────────────────────────────────────


# SECTION: MAIN EXECUTION (Example/Test)
async def main():
    """Demo function showing usage patterns of TagFactory and TagList."""
    # Define file paths relative to this script or using absolute paths
    log.info("Starting Tag Processing Demo...")

    # Configuration paths
    EXAMPLE_TAGS_CONFIG = Path("./tags_config.toml")  # Assumes file exists
    OUTPUT_PROCESSED_JSON = Path("./output/processed_tags.json")

    tag_list_instance: TagList | None = None

    # Ensure cache directory exists
    cache_path = Path(CACHE_DIR) / CACHE_SUBDIR
    cache_path.mkdir(exist_ok=True, parents=True)
    raw_json_path = cache_path / TAGS_FILENAME
    processed_json_path = cache_path / PROCESSED_TAGS_FILENAME

    # 1. Fetch tags from API
    log.info("Fetching tags from API...")
    api = HabiticaClient()  # Assumes client is configured
    raw_tags = await api.get_tags()
    log.success(f"Fetched {len(raw_tags)} tags from API.")

    # Save raw data
    save_json(raw_tags, raw_json_path)
    log.info(f"Raw tag data saved to {raw_json_path}")

    # 2. Create factory and process raw data into TagList model
    log.info("Creating TagFactory and processing raw data...")
    factory = TagFactory(EXAMPLE_TAGS_CONFIG)
    tag_list_instance = TagList.from_raw_data(raw_list=raw_tags, factory=factory)
    log.success(f"Processed into TagList: {tag_list_instance}")
    tag_list_instance.save()
    # 3. Example: Accessing data
    user_tags = tag_list_instance.get_user_tags()
    print(f"User tags count: {len(user_tags)}")

    if tag_list_instance.tags:
        first_tag = tag_list_instance.tags[0]
        print(f"First tag: {first_tag}")
        found_tag = tag_list_instance.get_by_id(first_tag.id)
        print(f"Found by ID: {found_tag is not None}")


if __name__ == "__main__":
    import asyncio

    asyncio.run(main())
-e 
-------- END OF FILE models/tag_factory.py --------

-------- START OF FILE models/task.py --------
# pixabit/models/task.py

# ─── Title ────────────────────────────────────────────────────────────────────
#         Habitica Task Models (Habits, Dailies, Todos, Rewards)
# ──────────────────────────────────────────────────────────────────────────────


# SECTION: MODULE DOCSTRING
"""Defines Pydantic models for representing Habitica Tasks (Habits, Dailies,
Todos, Rewards), including nested structures like ChecklistItem and
ChallengeLinkData. Provides a TaskList container for managing and processing
collections of Task objects.
"""

# SECTION: IMPORTS
from __future__ import annotations

import json
import logging
import math
import uuid
from collections import defaultdict
from datetime import datetime, timezone
from enum import Enum
from pathlib import Path
from typing import TYPE_CHECKING, Any, ClassVar, Iterator, Literal

import emoji_data_python
from pydantic import (
    BaseModel,
    ConfigDict,
    Field,
    PrivateAttr,
    ValidationError,
    ValidationInfo,
    computed_field,
    field_validator,
    model_validator,
)
from pydantic_core import PydanticCustomError

from pixabit.api.client import HabiticaClient
from pixabit.config import HABITICA_DATA_PATH
from pixabit.helpers._json import save_json
from pixabit.helpers._logger import log
from pixabit.helpers._md_to_rich import MarkdownRenderer
from pixabit.helpers._rich import Text
from pixabit.helpers.DateTimeHandler import DateTimeHandler

if TYPE_CHECKING:
    from .game_content import Quest as StaticQuestData
    from .game_content import StaticContentManager
    from .tag import TagList
    from .user import User

md_renderer = MarkdownRenderer()


# SECTION: NESTED DATA MODELS


# KLASS: ChecklistItem
class ChecklistItem(BaseModel):
    """Represents a single item within a task's checklist."""

    model_config = ConfigDict(extra="ignore", populate_by_name=True, frozen=False)
    id: str
    text: str = ""
    completed: bool = False

    @model_validator(mode="before")
    @classmethod
    def ensure_id(cls, data: Any) -> dict[str, Any]:
        if not isinstance(data, dict):
            return {}
        if "id" not in data:
            generated_id = str(uuid.uuid4())
            log.warning(f"Checklist item missing 'id': {data.get('text', 'N/A')}")
            data["id"] = generated_id
        return data

    @field_validator("text", mode="before")
    @classmethod
    def parse_text_emoji(cls, value: Any) -> str:
        if isinstance(value, str):
            return emoji_data_python.replace_colons(value).strip()
        return ""

    def __repr__(self) -> str:
        status = "[x]" if self.completed else "[ ]"
        text_preview = self.text[:30].replace("\n", " ")
        if len(self.text) > 30:
            text_preview += "..."
        id_preview = self.id[:8] if self.id else "NoID"
        return f"ChecklistItem(id={id_preview}, status='{status}', text='{text_preview}')"

    def __str__(self) -> str:
        return f"{'[x]' if self.completed else '[ ]'} {self.text}"


# KLASS: ChallengeLinkData
class ChallengeLinkData(BaseModel):
    model_config = ConfigDict(extra="ignore", populate_by_name=True, frozen=True)
    task_id: str | None = Field(None, alias="taskId")
    challenge_id: str = Field(..., alias="id")
    short_name: str | None = Field(None, alias="shortName")
    broken_reason: str | None = Field(None, alias="broken")
    is_broken: bool = False
    broken_status: Literal["task_deleted", "challenge_deleted", "unsubscribed", "challenge_closed", "unknown"] | None = None

    @field_validator("short_name", mode="before")
    @classmethod
    def parse_name_emoji(cls, value: Any) -> str | None:
        if isinstance(value, str):
            return emoji_data_python.replace_colons(value).strip()
        return None

    @model_validator(mode="before")
    @classmethod
    def process_broken_status(cls, data: Any) -> dict[str, Any]:
        if not isinstance(data, dict):
            return {} if not data else data
        values = data.copy()
        broken_raw = values.get("broken")
        is_broken_flag = bool(broken_raw)
        broken_status_val = None
        if is_broken_flag and isinstance(broken_raw, str):
            reason = broken_raw.upper().strip()
            match reason:
                case "TASK_DELETED" | "CHALLENGE_TASK_NOT_FOUND":
                    broken_status_val = "task_deleted"
                case "CHALLENGE_DELETED":
                    broken_status_val = "challenge_deleted"
                case "UNSUBSCRIBED":
                    broken_status_val = "unsubscribed"
                case "CHALLENGE_CLOSED":
                    broken_status_val = "challenge_closed"
                case _:
                    broken_status_val = "unknown"
                    log.debug(f"Unknown challenge 'broken' reason: {broken_raw}")
        values["is_broken"] = is_broken_flag
        values["broken_status"] = broken_status_val
        return values

    def __repr__(self) -> str:
        status = f", BROKEN='{self.broken_status}' ({self.broken_reason})" if self.is_broken else ""
        name = f", name='{self.short_name}'" if self.short_name else ""
        chid = self.challenge_id[:8] if self.challenge_id else "NoChalID"
        return f"ChallengeLinkData(challenge_id={chid}{name}{status})"


# ──────────────────────────────────────────────────────────────────────────────


# SECTION: BASE TASK MODEL


# KLASS: Task
class Task(BaseModel):
    model_config = ConfigDict(
        extra="allow",
        populate_by_name=True,
        validate_assignment=True,
        arbitrary_types_allowed=True,
    )
    id: str = Field(..., alias="_id")
    text: str = ""
    notes: str = ""
    type: Literal["habit", "daily", "todo", "reward"] = Field(..., alias="type")
    tags_id: list[str] = Field(default_factory=list, alias="tags")
    value: float = 0.0
    priority: float = 1.0
    attribute: Literal["str", "int", "con", "per"] | None = "str"
    created_at: datetime | None = Field(None, alias="createdAt")
    updated_at: datetime | None = Field(None, alias="updatedAt")
    challenge: ChallengeLinkData | None = None
    alias: str | None = None
    position: int | None = Field(None, exclude=True)
    calculated_status: str = "unknown"
    _styled_text: Text | None = PrivateAttr(default=None)
    _styled_notes: Text | None = PrivateAttr(default=None)
    _tag_names: list[str] = PrivateAttr(default_factory=list)

    @model_validator(mode="before")
    @classmethod
    def prepare_data(cls, data: Any) -> dict[str, Any]:
        if not isinstance(data, dict):
            return data
        values = data.copy()
        # Asegurar que siempre tengamos un ID
        if "_id" in values and "id" not in values:
            values["id"] = values["_id"]
        if "id" not in values or not values.get("id"):
            if "_id" not in values:
                values["id"] = str(uuid.uuid4())
                log.debug(f"Generated temporary ID: {values['id'][:8]}")
            else:
                log.warning(f"Task with _id but missing id: {values.get('_id')}")
        # Verificar el tipo de tarea
        if "type" not in values or not isinstance(values.get("type"), str):
            task_id_preview = values.get("id", values.get("_id", "unknown ID"))[:8]
            log.warning(f"Task {task_id_preview} missing valid 'type' field")
        return values

    @field_validator("id", mode="after")
    @classmethod
    def check_id(cls, v: str) -> str:
        if not v or not isinstance(v, str):
            raise PydanticCustomError("value_error", "Task ID (_id) is required and must be a non-empty string", {"value": v})
        return v

    @field_validator("text", "notes", "alias", mode="before")
    @classmethod
    def parse_text_emoji(cls, value: Any, info: ValidationInfo) -> str | None:
        is_optional = info.field_name == "alias"
        default = None if is_optional else ""
        if value is None or (isinstance(value, str) and not value.strip()):
            return default
        if isinstance(value, str):
            return emoji_data_python.replace_colons(value).strip()
        log.warning(f"Field '{info.field_name}': Expected string, got {type(value).__name__}")
        return default

    @field_validator("created_at", "updated_at", mode="before")
    @classmethod
    def parse_datetime_utc(cls, value: Any) -> datetime | None:
        if isinstance(value, str) and not value.strip():
            return None
        handler = DateTimeHandler(timestamp=value)
        if value is not None and handler.utc_datetime is None and value != "":
            log.warning(f"Could not parse timestamp: {value!r}")
        return handler.utc_datetime

    @field_validator("value", "priority", mode="before")
    @classmethod
    def ensure_float(cls, value: Any, info: ValidationInfo) -> float:
        default = 0.0 if info.field_name == "value" else 1.0
        try:
            if value is None or (isinstance(value, str) and not value.strip()):
                return default
            return float(value)
        except (ValueError, TypeError):
            log.debug(f"Could not parse {info.field_name} as float: {value!r}")
            return default

    @field_validator("attribute", mode="before")
    @classmethod
    def validate_attribute(cls, value: Any) -> str | None:
        allowed = {"str", "int", "con", "per"}
        if value is None or (isinstance(value, str) and not value.strip()):
            return None
        if value in allowed:
            return value
        log.warning(f"Invalid attribute value '{value}'")
        return None

    @field_validator("challenge", mode="before")
    @classmethod
    def validate_challenge_data(cls, value: Any) -> dict[str, Any] | None:
        if value is None or not value:
            return None
        if isinstance(value, dict):
            return value
        log.warning(f"Unexpected 'challenge' type: {type(value).__name__}")
        return None

    @computed_field(repr=False)
    @property
    def styled_text(self) -> Text:
        if self._styled_text is None:
            self._styled_text = md_renderer.markdown_to_rich_text(self.text or "")
        return self._styled_text

    @computed_field(repr=False)
    @property
    def styled_notes(self) -> Text:
        if self._styled_notes is None:
            self._styled_notes = md_renderer.markdown_to_rich_text(self.notes or "")
        return self._styled_notes

    @computed_field
    @property
    def tag_names(self) -> list[str]:
        return self._tag_names

    def set_tag_names_from_provider(self, tags_provider: TagList | None) -> None:
        resolved_names = []
        if tags_provider and hasattr(tags_provider, "get_by_id"):
            for tag_id in self.tags_id:
                tag = tags_provider.get_by_id(tag_id)
                if tag and hasattr(tag, "name"):
                    resolved_names.append(tag.name)
                else:
                    # log.debug(f"Tag ID '{tag_id}' not found for task {self.id}")
                    resolved_names.append(f"Unknown:{tag_id[:6]}")
        elif self.tags_id:
            # log.debug(f"No tags_provider for task {self.id[:8]}")
            resolved_names = [f"ID:{tid[:6]}" for tid in self.tags_id]
        self._tag_names = resolved_names

    def process_status_and_metadata(
        self, user: User | None = None, tags_provider: TagList | None = None, content_manager: StaticContentManager | None = None
    ) -> bool:
        try:
            self.set_tag_names_from_provider(tags_provider)
            self.update_calculated_status()
            return True
        except Exception as e:
            log.exception(f"Error processing task {self.id}: {e}")
            return False

    @staticmethod
    def calculate_checklist_progress(checklist: list[ChecklistItem]) -> float:
        if not checklist:
            return 0.0
        completed_count = sum(1 for item in checklist if getattr(item, "completed", False))
        total_count = len(checklist)
        return completed_count / total_count if total_count > 0 else 0.0

    def calculate_value_color(self) -> str:
        value = self.value
        if value <= -20:
            return "red"
        elif value <= -10:
            return "orange"
        elif value < 0:
            return "yellow"
        elif value == 0:
            return "grey"
        elif value < 5:
            return "bright_blue"
        elif value <= 10:
            return "blue"
        else:
            return "green"

    def update_calculated_status(self):
        # Método implementado en subclases
        pass

    def __repr__(self) -> str:
        text_preview = self.text[:25].replace("\n", " ")
        if len(self.text) > 25:
            text_preview += "..."
        prio = f"P{self.priority}" if self.priority != 1.0 else ""
        attr = f"A:{self.attribute or '?'}"
        status = f"S:{self.calculated_status}"
        return f"{self.__class__.__name__}(id='{self.id[:8]}', {prio} {attr} {status} text='{text_preview}')"

    def __str__(self) -> str:
        return self.text


# ──────────────────────────────────────────────────────────────────────────────


# SECTION: TASK SUBCLASSES


# KLASS: Habit
class Habit(Task):
    model_config = ConfigDict(extra="ignore", populate_by_name=True)
    type: Literal["habit"] = Field("habit", frozen=True)
    up: bool = True
    down: bool = True
    counter_up: int = Field(0, alias="counterUp")
    counter_down: int = Field(0, alias="counterDown")
    frequency: str = "daily"  # 'daily', 'weekly', 'monthly'

    @field_validator("counter_up", "counter_down", mode="before")
    @classmethod
    def ensure_int(cls, value: Any) -> int:
        if value is None:
            return 0
        try:
            return int(float(value))
        except (ValueError, TypeError):
            return 0

    def update_calculated_status(self):
        if self.up and self.down:
            self.calculated_status = "good_bad"
        elif self.up:
            self.calculated_status = "good"
        elif self.down:
            self.calculated_status = "bad"
        else:
            self.calculated_status = "neutral"

    def __repr__(self) -> str:
        up_str = f"⬆️{self.counter_up}" if self.up else ""
        down_str = f"⬇️{self.counter_down}" if self.down else ""
        counters = f"{up_str} / {down_str}" if self.up and self.down else (up_str or down_str or "No Score")
        text_preview = self.text[:20].replace("\n", " ")
        if len(self.text) > 20:
            text_preview += "..."
        prio = f"P{self.priority}" if self.priority != 1.0 else ""
        return f"Habit(id='{self.id[:8]}' {prio} ctr='{counters}', text='{text_preview}')"


# KLASS: Daily
class Daily(Task):
    model_config = ConfigDict(extra="ignore", populate_by_name=True)
    type: Literal["daily"] = Field("daily", frozen=True)
    completed: bool = False
    is_due: bool = Field(False, alias="isDue")
    streak: int = 0
    yesterday_completed: bool = Field(False, alias="yesterDaily")
    collapse_checklist: bool = Field(False, alias="collapseChecklist")
    checklist: list[ChecklistItem] = Field(default_factory=list)
    frequency: Literal["daily", "weekly", "monthly", "yearly"] = "weekly"
    every_x: int = Field(1, alias="everyX")
    repeat: dict[Literal["m", "t", "w", "th", "f", "s", "su"], bool] = Field(default_factory=dict)
    days_of_month: list[int] = Field(default_factory=list, alias="daysOfMonth")
    weeks_of_month: list[int] = Field(default_factory=list, alias="weeksOfMonth")
    start_date: datetime | None = Field(None, alias="startDate")
    _calculated_user_damage: float | None = PrivateAttr(default=None)
    _calculated_party_damage: float | None = PrivateAttr(default=None)

    @field_validator("streak", "every_x", mode="before")
    @classmethod
    def ensure_int(cls, value: Any) -> int:
        default = 1 if value is None else 0
        try:
            val = int(float(value))
            return max(0, val)
        except (ValueError, TypeError):
            return default

    @field_validator("start_date", mode="before")
    @classmethod
    def parse_start_date_utc(cls, value: Any) -> datetime | None:
        handler = DateTimeHandler(timestamp=value)
        return handler.utc_datetime

    def update_calculated_status(self):
        if self.completed:
            self.calculated_status = "complete"
        elif self.is_due:
            self.calculated_status = "due"
        else:
            self.calculated_status = "not_due"

    @computed_field
    @property
    def user_damage(self) -> float | None:
        """Potential damage to user if missed."""
        return self._calculated_user_damage

    @computed_field
    @property
    def party_damage(self) -> float | None:
        """Potential damage to party/boss if missed."""
        return self._calculated_party_damage

    def calculate_and_store_damage(self, user: User, static_content: StaticContentManager | None = None) -> None:
        self._calculated_user_damage = None
        self._calculated_party_damage = None
        # Early returns para casos que no requieren cálculo
        if not self.is_due or self.completed or not user:
            return
        if getattr(user, "is_sleeping", True):
            return
        stealth = getattr(user, "stealth", 0)
        if stealth > 0:
            return
        try:
            eff_stats = getattr(user, "effective_stats", {})
            if not eff_stats:
                log.warning("Cannot calculate damage: No effective stats for user")
                return
            effective_con = eff_stats.get("con", 0.0)
            # Limitar y calcular valor base
            value = max(-47.27, min(self.value, 21.27))
            base_delta = math.pow(0.9747, value)
            # Mitigación por progreso de checklist
            checklist_progress = self.calculate_checklist_progress(self.checklist)
            checklist_mitigation = 1.0 - checklist_progress
            # Delta efectivo y mitigación por CON
            effective_delta = base_delta * checklist_mitigation
            con_mitigation = max(0.1, 1.0 - (effective_con / 250.0))
            # Daño HP calculado
            hp_damage = effective_delta * con_mitigation * self.priority * 2.0
            self._calculated_user_damage = max(0.0, round(hp_damage, 1))
            # Cálculo de daño de grupo si es aplicable
            party_info = getattr(user, "party", None)
            if party_info:
                party_quest_info = getattr(party_info, "quest", None)
                if party_quest_info and getattr(party_quest_info, "is_active_and_ongoing", False):
                    quest_key = getattr(party_quest_info, "key", None)
                    static_quest = getattr(party_info, "_static_quest_details", None)
                    if static_quest is None and quest_key and static_content:
                        log.warning(f"Static quest details not pre-fetched for {quest_key}")
                    if static_quest and getattr(static_quest, "is_boss_quest", False):
                        boss_stats = getattr(static_quest, "boss", None)
                        if boss_stats:
                            boss_strength = getattr(boss_stats, "strength", 0.0)
                            if boss_strength > 0:
                                party_delta = effective_delta * self.priority
                                party_damage = party_delta * boss_strength
                                self._calculated_party_damage = max(0.0, round(party_damage, 1))
        #       log.debug(
        #          f"Daily {self.id}: UserDmg={self._calculated_user_damage}, "
        #         f"PartyDmg={self._calculated_party_damage}, Value:{value:.1f}, "
        #        f"Prio:{self.priority:.1f}, Chk:{checklist_progress:.2f}"
        #   )
        except AttributeError as ae:
            log.error(f"AttributeError during damage calc for Daily {self.id}: {ae}")
        except Exception as e:
            log.exception(f"Error calculating damage for daily {self.id}: {e}")
            self._calculated_user_damage = None
            self._calculated_party_damage = None

    def process_status_and_metadata(
        self, user: User | None = None, tags_provider: TagList | None = None, content_manager: StaticContentManager | None = None
    ) -> bool:
        result = super().process_status_and_metadata(user, tags_provider, content_manager)
        if result and user:
            try:
                self.calculate_and_store_damage(user, content_manager)
            except Exception as e:
                log.exception(f"Error calculating damage: {e}")
                return False
        return result

    def __repr__(self) -> str:
        status = self.calculated_status.upper()
        streak_str = f" (Strk:{self.streak})" if self.streak > 0 else ""
        dmg_str = f" (DmgU:{self.user_damage or 0:.1f}|P:{self.party_damage or 0:.1f})" if self.is_due and not self.completed else ""
        checklist_str = f" Chk:{len(self.checklist)}" if self.checklist else ""
        text_preview = self.text[:15].replace("\n", " ")
        if len(self.text) > 15:
            text_preview += "..."
        prio = f"P{self.priority}" if self.priority != 1.0 else ""
        return f"Daily(id='{self.id[:8]}' {prio} S:{status}{streak_str}" f"{checklist_str}{dmg_str}, text='{text_preview}')"


# KLASS: Todo
class Todo(Task):
    model_config = ConfigDict(extra="ignore", populate_by_name=True)
    type: Literal["todo"] = Field("todo", frozen=True)
    completed: bool = False
    completed_date: datetime | None = Field(None, alias="dateCompleted")
    due_date: datetime | None = Field(None, alias="date")
    collapse_checklist: bool = Field(False, alias="collapseChecklist")
    checklist: list[ChecklistItem] = Field(default_factory=list)

    @field_validator("completed_date", "due_date", mode="before")
    @classmethod
    def parse_todo_datetime_utc(cls, value: Any) -> datetime | None:
        if value == "":
            return None
        handler = DateTimeHandler(timestamp=value)
        return handler.utc_datetime

    @property
    def is_past_due(self) -> bool:
        if self.completed or not self.due_date:
            return False
        now_utc = datetime.now(timezone.utc)
        return self.due_date < now_utc

    def update_calculated_status(self):
        if self.completed:
            self.calculated_status = "complete"
        elif not self.due_date:
            self.calculated_status = "no_due_date"
        elif self.is_past_due:
            self.calculated_status = "past_due"
        else:
            self.calculated_status = "due"

    def calculate_progress(self) -> float:
        if self.completed:
            return 1.0
        return self.calculate_checklist_progress(self.checklist)

    def __repr__(self) -> str:
        status = self.calculated_status.upper()
        due_str = f", due={self.due_date.strftime('%y-%m-%d')}" if self.due_date else ""
        checklist_str = f" Chk:{len(self.checklist)}" if self.checklist else ""
        progress = self.calculate_progress()
        prog_str = f" Prg:{progress:.0%}" if progress < 1.0 and self.checklist else ""
        text_preview = self.text[:15].replace("\n", " ")
        if len(self.text) > 15:
            text_preview += "..."
        prio = f"P{self.priority}" if self.priority != 1.0 else ""
        return f"Todo(id='{self.id[:8]}' {prio} S:{status}{due_str}" f"{checklist_str}{prog_str}, text='{text_preview}')"


class Reward(Task):
    model_config = ConfigDict(extra="ignore", populate_by_name=True)
    type: Literal["reward"] = Field("reward", frozen=True)
    value: float  # Gold cost of the reward

    def update_calculated_status(self):
        self.calculated_status = "available"

    def __repr__(self) -> str:
        cost_str = f"(Cost: {self.value:.1f} GP)"
        text_preview = self.text[:25].replace("\n", " ")
        if len(self.text) > 25:
            text_preview += "..."
        return f"Reward(id='{self.id[:8]}' {cost_str} text='{text_preview}')"


# Tipo unión para cualquier tarea
AnyTask = Habit | Daily | Todo | Reward

# ──────────────────────────────────────────────────────────────────────────────


# SECTION: TASK LIST CONTAINER / MANAGER


# KLASS: TaskList
class TaskList(BaseModel):
    model_config = ConfigDict(
        extra="ignore",
        populate_by_name=True,
        validate_assignment=True,
        arbitrary_types_allowed=True,
    )

    tasks: list[Task] = Field(default_factory=list, description="List of all Task objects.")

    _raw_tasks_data: list[dict[str, Any]] | None = PrivateAttr(default=None)
    _tasks_by_id: dict[str, Task] = PrivateAttr(default_factory=dict)
    _tasks_by_type: dict[Literal["habit", "daily", "todo", "reward"], list[Task]] = PrivateAttr(default_factory=lambda: defaultdict(list))
    _tags_provider: TagList | None = PrivateAttr(default=None)
    _user_data: User | None = PrivateAttr(default=None)
    _content_manager: StaticContentManager | None = PrivateAttr(default=None)

    @classmethod
    def from_raw_api_list(
        cls,
        raw_data: list[dict[str, Any]],
        user: User | None = None,
        tags_provider: TagList | None = None,
        content_manager: StaticContentManager | None = None,
    ) -> TaskList:
        """Create a TaskList from raw API data with error handling and logging."""
        parsed_tasks: list[Task] = []
        errors: list[tuple[str, str, Exception]] = []

        log.info(f"Parsing {len(raw_data)} raw task entries from API")

        type_map: dict[str, type[Task]] = {
            "habit": Habit,
            "daily": Daily,
            "todo": Todo,
            "reward": Reward,
        }

        for i, item in enumerate(raw_data):
            type_str = str(item.get("type", "")).lower()
            task_model = type_map.get(type_str)

            if not task_model:
                task_id = item.get("_id", item.get("id", "unknown"))[:8]
                log.warning(f"Skipping item {i} (ID: {task_id}) with unknown type '{type_str}'")
                continue

            try:
                task = task_model(**item)
                task.position = i
                parsed_tasks.append(task)
            except ValidationError as e:
                task_id = item.get("_id", item.get("id", "unknown"))[:8]
                log.error(f"Validation error for task {task_id} (Type: {type_str}): {e}")
                errors.append((item.get("_id", "N/A"), type_str, e))
            except Exception as e:
                task_id = item.get("_id", item.get("id", "unknown"))[:8]
                log.exception(f"Error parsing task {task_id} (Type: {type_str}): {e}")
                errors.append((item.get("_id", "N/A"), type_str, e))

        log.info(f"Successfully parsed {len(parsed_tasks)} tasks. Encountered {len(errors)} errors")

        instance = cls(tasks=parsed_tasks)
        instance._raw_tasks_data = raw_data
        instance._user_data = user
        instance._tags_provider = tags_provider
        instance._content_manager = content_manager
        instance.process_tasks(user=user, tags_provider=tags_provider, content_manager=content_manager)

        return instance

    @classmethod
    def from_processed_dicts(cls, processed_task_dicts: list[dict[str, Any]]) -> TaskList:
        """Create a TaskList from pre-processed task dictionaries."""
        if not isinstance(processed_task_dicts, list):
            log.error(f"Invalid input: Expected list, got {type(processed_task_dicts)}")
            return cls(tasks=[])

        log.debug(f"Processing {len(processed_task_dicts)} task dictionaries")

        validated_tasks: list[AnyTask] = []
        task_map = {"habit": Habit, "daily": Daily, "todo": Todo, "reward": Reward}

        for index, task_data in enumerate(processed_task_dicts):
            if not isinstance(task_data, dict):
                continue

            type = task_data.get("type")
            model_class = task_map.get(type)

            if not model_class:
                task_id = task_data.get("id", f"index_{index}")
                log.warning(f"Unknown task type '{type!r}' for task ID '{task_id}'. Skipping")
                continue

            try:
                task = model_class.model_validate(task_data)
                validated_tasks.append(task)
            except Exception as e:
                task_id = task_data.get("id", f"index_{index}")
                log.exception(f"Error validating task '{task_id}' (Type: {type}): {e}")

        log.info(f"Validated {len(validated_tasks)} tasks from processed dictionary data")
        return cls(tasks=validated_tasks)

    def process_tasks(
        self,
        user: User | None = None,
        tags_provider: TagList | None = None,
        content_manager: StaticContentManager | None = None,
    ) -> None:
        """Process metadata for all tasks and organize them by ID and type."""
        log.info(f"Processing {len(self.tasks)} tasks")

        # Update instance attributes if provided
        if user:
            self._user_data = user
        if tags_provider:
            self._tags_provider = tags_provider
        if content_manager:
            self._content_manager = content_manager

        # Reset internal dictionaries
        self._tasks_by_id = {}
        self._tasks_by_type = defaultdict(list)

        # Process each task
        for i, task in enumerate(self.tasks):
            task.position = i
            self._tasks_by_id[task.id] = task
            self._tasks_by_type[task.type].append(task)
            task.process_status_and_metadata(user=self._user_data, tags_provider=self._tags_provider, content_manager=self._content_manager)

        log.info("Task processing complete")

    def get_task_by_id(self, task_id: str) -> Task | None:
        """Get a task by its ID."""
        return self._tasks_by_id.get(task_id)

    def get_tasks_by_type(self, type: Literal["habit", "daily", "todo", "reward"]) -> list[Task]:
        """Get all tasks of a specific type."""
        return self._tasks_by_type.get(type, [])

    def add_task(self, task_data: dict[str, Any] | Task) -> Task | None:
        """Add a new task to the list and all internal data structures."""
        # Determine task type and model
        if isinstance(task_data, Task):
            type_str = task_data.type
            new_task = task_data
        elif isinstance(task_data, dict):
            type_str = str(task_data.get("type", "")).lower()

            type_map: dict[str, type[Task]] = {
                "habit": Habit,
                "daily": Daily,
                "todo": Todo,
                "reward": Reward,
            }

            task_model = type_map.get(type_str)
            if not task_model:
                log.error(f"Cannot add task with unknown type '{type_str}'")
                return None

            try:
                new_task = task_model(**task_data)
            except ValidationError as e:
                task_id = task_data.get("_id", task_data.get("id", "unknown"))[:8]
                log.error(f"Validation error adding task {task_id}: {e}")
                return None
            except Exception as e:
                task_id = task_data.get("_id", task_data.get("id", "unknown"))[:8]
                log.exception(f"Error adding task {task_id}: {e}")
                return None
        else:
            log.error(f"Invalid input type: {type(task_data).__name__}. Expected dict or Task")
            return None

        # Check for duplicate ID
        if new_task.id in self._tasks_by_id:
            log.warning(f"Task ID {new_task.id[:8]} already exists")
            return self._tasks_by_id[new_task.id]

        # Add to collections
        self.tasks.append(new_task)
        self._tasks_by_id[new_task.id] = new_task
        self._tasks_by_type[new_task.type].append(new_task)
        new_task.position = len(self.tasks) - 1

        # Process task metadata
        success = new_task.process_status_and_metadata(user=self._user_data, tags_provider=self._tags_provider, content_manager=self._content_manager)

        if not success:
            log.warning(f"Failed to process metadata for task {new_task.id[:8]}")

        log.info(f"Added task: {new_task.id[:8]} (Type: {new_task.type})")
        return new_task

    def edit_task(self, task_id: str, update_data: dict[str, Any]) -> Task | None:
        """Update an existing task with new data."""
        task = self.get_task_by_id(task_id)
        if not task:
            log.warning(f"Task ID {task_id[:8]} not found for editing")
            return None
        try:
            # Use Pydantic's update mechanism
            updated_task = task.model_validate(update_data, update=True)

            # Process updated task metadata
            success = updated_task.process_status_and_metadata(user=self._user_data, tags_provider=self._tags_provider, content_manager=self._content_manager)

            if not success:
                log.warning(f"Failed to process metadata for edited task {task_id[:8]}")

            log.info(f"Edited task: {task_id[:8]} (Type: {updated_task.type})")
            return updated_task
        except ValidationError as e:
            log.error(f"Validation error editing task {task_id[:8]}: {e}")
            return None
        except Exception as e:
            log.exception(f"Error editing task {task_id[:8]}: {e}")
            return None

    def delete_task(self, task_id: str) -> Task | None:
        """Remove a task from the list and all internal data structures."""
        task = self.get_task_by_id(task_id)
        if not task:
            log.warning(f"Task ID {task_id[:8]} not found for deletion")
            return None

        try:
            # Remove from main list
            try:
                self.tasks.remove(task)
            except ValueError:
                log.warning(f"Task {task_id[:8]} not found in main tasks list")

            # Remove from type-specific list
            type = task.type
            if type in self._tasks_by_type:
                try:
                    self._tasks_by_type[type].remove(task)
                except ValueError:
                    log.warning(f"Task {task_id[:8]} not found in type list for '{type}'")

            # Remove from ID dictionary
            if task_id in self._tasks_by_id:
                del self._tasks_by_id[task_id]
            else:
                log.warning(f"Task ID {task_id[:8]} not found in ID dictionary")

            log.info(f"Deleted task: {task_id[:8]} (Type: {task.type})")
            return task
        except Exception as e:
            log.exception(f"Error deleting task {task_id[:8]}: {e}")
            return None

    def reorder_tasks(self, type: Literal["habit", "daily", "todo", "reward"], new_order_ids: list[str]) -> None:
        """Reorder tasks of a specific type according to a new ID order."""
        if type not in self._tasks_by_type:
            log.warning(f"No tasks found for type '{type}'")
            return

        # Get current tasks of this type
        current_tasks = self.get_tasks_by_type(type)
        tasks_dict = {task.id: task for task in current_tasks}

        # Create new ordered list
        reordered_tasks = []
        seen_ids = set()

        # First add tasks that are in the new order
        for task_id in new_order_ids:
            if task_id in tasks_dict and task_id not in seen_ids:
                reordered_tasks.append(tasks_dict[task_id])
                seen_ids.add(task_id)
            elif task_id in seen_ids:
                log.warning(f"Duplicate task ID '{task_id[:8]}' in new_order_ids")
            else:
                log.warning(f"Task ID '{task_id[:8]}' not found in current tasks")

        # Add any remaining tasks at the end
        missing_ids = set(tasks_dict.keys()) - seen_ids
        if missing_ids:
            log.warning(f"Tasks not in new_order_ids will be appended: {list(missing_ids)[:5]}")
            for task_id in missing_ids:
                reordered_tasks.append(tasks_dict[task_id])

        # Update type-specific list
        self._tasks_by_type[type] = reordered_tasks

        # Rebuild global task list in the desired display order
        log.info(f"Rebuilding global task list after reordering '{type}'")
        new_global_tasks = []
        processed_ids = set()

        # Define preferred display order
        type_display_order = ["daily", "todo", "habit", "reward"]
        all_types = set(self._tasks_by_type.keys())
        ordered_types = []

        # First add types in preferred order
        for t in type_display_order:
            if t in all_types:
                ordered_types.append(t)
                all_types.remove(t)

        # Then add any remaining types
        ordered_types.extend(sorted(all_types))

        # Build new global list in the correct order
        for current_type in ordered_types:
            for task in self._tasks_by_type.get(current_type, []):
                if task.id not in processed_ids:
                    new_global_tasks.append(task)
                    processed_ids.add(task.id)
                else:
                    log.warning(f"Duplicate task ID '{task.id[:8]}' while rebuilding global list")

        # Update global list and reprocess
        self.tasks = new_global_tasks
        self.process_tasks(user=self._user_data, tags_provider=self._tags_provider, content_manager=self._content_manager)

        log.info(f"Reordering complete for tasks of type '{type}'")

    # Collection-like methods
    def __len__(self) -> int:
        return len(self.tasks)

    def __iter__(self) -> Iterator[AnyTask]:
        return iter(self.tasks)

    def __getitem__(self, index: int | slice) -> AnyTask | list[AnyTask]:
        if isinstance(index, int) and not 0 <= index < len(self.tasks):
            raise IndexError("TaskList index out of range")
        return self.tasks[index]

    def __contains__(self, item: AnyTask | str) -> bool:
        if isinstance(item, str):
            return item in self._tasks_by_id
        return item in self.tasks

    def __repr__(self) -> str:
        counts = defaultdict(int)
        for task in self.tasks:
            counts[task.type] += 1
        count_str = ", ".join(f"{t}:{c}" for t, c in sorted(counts.items()))
        return f"TaskList(count={len(self.tasks)}, types=[{count_str}])"

    def __str__(self) -> str:
        task_summary = [f"{type.capitalize()}s: {len(self.get_tasks_by_type(type))}" for type in self._tasks_by_type.keys() if self.get_tasks_by_type(type)]
        return f"TaskList contains: {', '.join(task_summary) or 'No tasks'}"

    # Helper methods for filtering
    def get_by_id(self, task_id: str) -> AnyTask | None:
        """Get a task by its ID - alias for get_task_by_id."""
        return self._tasks_by_id.get(task_id)

    def filter(self, criteria_func: callable[[AnyTask], bool]) -> TaskList:
        """Filter tasks based on a custom criteria function."""
        filtered_tasks = [task for task in self.tasks if criteria_func(task)]
        return TaskList(tasks=filtered_tasks)

    def filter_by_type(self, type: Literal["habit", "daily", "todo", "reward"]) -> TaskList:
        """Filter tasks by type."""
        return TaskList(tasks=self.get_tasks_by_type(type))

    def filter_by_status(self, status: str) -> TaskList:
        """Filter tasks by calculated status."""
        return self.filter(lambda task: task.calculated_status == status)

    def filter_by_tag_id(self, tag_id: str) -> TaskList:
        """Filter tasks by tag ID."""
        return self.filter(lambda task: tag_id in task.tags_id)

    def filter_by_tag_name(self, tag_name: str, case_sensitive: bool = False) -> TaskList:
        """Filter tasks by tag name."""
        if case_sensitive:
            return self.filter(lambda task: tag_name in task.tag_names)

        tag_name_lower = tag_name.lower()
        return self.filter(lambda task: any(tag_name_lower in tn.lower() for tn in task.tag_names))

    def filter_by_text(self, text_part: str, case_sensitive: bool = False) -> TaskList:
        """Filter tasks by text content."""
        if case_sensitive:
            return self.filter(lambda task: text_part in task.text)

        text_part_lower = text_part.lower()
        return self.filter(lambda task: text_part_lower in task.text.lower())

    # Convenience methods for common task types
    def get_habits(self) -> TaskList:
        """Get all habits."""
        return self.filter_by_type("habit")

    def get_dailies(self) -> TaskList:
        """Get all dailies."""
        return self.filter_by_type("daily")

    def get_todos(self) -> TaskList:
        """Get all todos."""
        return self.filter_by_type("todo")

    def get_rewards(self) -> TaskList:
        """Get all rewards."""
        return self.filter_by_type("reward")

    # Serialization methods
    def to_dicts(self) -> list[dict[str, Any]]:
        """Convert all tasks to dictionaries."""
        return [task.model_dump(mode="json", exclude={"styled_text", "styled_notes"}) for task in self.tasks]

    def save_to_json(self, filename: str, folder: Path) -> bool:
        """Save tasks to a JSON file in the specified folder."""
        data = self.to_dicts()
        log.info(f"Saving {len(data)} tasks to {folder / filename}")

        try:
            return save_json(data, filename, folder=folder)
        except Exception as e:
            log.exception(f"Error saving tasks to JSON: {e}")
            return False

    @classmethod
    def load_from_json(
        cls,
        file_path: Path | str,
        user: User | None = None,
        tags_provider: TagList | None = None,
        content_manager: StaticContentManager | None = None,
    ) -> TaskList | None:
        """Load tasks from a JSON file."""
        path = Path(file_path)
        try:
            with open(path, encoding="utf-8") as f:
                raw_data = json.load(f)

            if not isinstance(raw_data, list):
                log.error(f"Expected JSON list of tasks in {path}, got {type(raw_data).__name__}")
                return None

            task_list = cls.from_raw_api_list(raw_data, user, tags_provider, content_manager)
            log.info(f"TaskList loaded from {path}")
            return task_list

        except FileNotFoundError:
            log.error(f"JSON file not found: {path}")
            return None
        except json.JSONDecodeError as e:
            log.error(f"Error decoding JSON from {path}: {e}")
            return None
        except Exception as e:
            log.exception(f"Error loading TaskList from {path}: {e}")
            return None


# ──────────────────────────────────────────────────────────────────────────────

# SECTION: MAIN EXECUTION (Example/Test)


async def main():
    """Demo function to retrieve, process, and save tasks."""
    log.info("--- Task Models Demo ---")
    tasks_list_instance: TaskList | None = None
    try:
        cache_dir = HABITICA_DATA_PATH / "tasks"
        cache_dir.mkdir(exist_ok=True, parents=True)
        raw_path = cache_dir / "tasks_raw.json"
        processed_path = cache_dir / "tasks_processed.json"

        # 1. Fetch raw data
        log.info("Fetching tasks from API...")
        api = HabiticaClient()  # Assumes configured
        raw_data = await api.get_tasks()
        log.success(f"Fetched {len(raw_data)} raw task items.")
        save_json(raw_data, raw_path.name, folder=raw_path.parent)  # Save raw data

        # 2. Validate and create TaskList
        log.info("Validating raw data into TaskList...")
        tasks_list_instance = TaskList.from_raw_api_list()
        log.success(f"Created TaskList: {tasks_list_instance}")

        # 3. Process Tasks (Requires User, TagList, ContentManager - Mocked for demo if needed)
        log.info("Processing task statuses (requires User/Tags/Content)...")

        # 4. Example Access/Filtering
        dailies = tasks_list_instance.get_dailies()
        print(f"  - Number of Dailies: {len(dailies)}")
        if dailies:
            first_daily = dailies[0]
        #           print(f"  - First Daily: {first_daily}")
        # Access calculated damage (will be None if User was missing)
        #            print(f"    -> Calculated User Damage: {getattr(first_daily, 'user_damage', 'N/A')}")

        # 5. Save processed data
        log.info(f"Saving processed tasks to {processed_path}...")
        if tasks_list_instance.save_to_json(processed_path.name, folder=processed_path.parent):
            log.success("Processed tasks saved.")
        else:
            log.error("Failed to save processed tasks.")

    except ConnectionError as e:
        log.error(f"API Connection error: {e}")
    except ValidationError as e:
        log.error(f"Pydantic Validation Error: {e}")
    except Exception as e:
        log.exception(f"An unexpected error occurred in the task demo: {e}")

    return tasks_list_instance


if __name__ == "__main__":
    import asyncio

    logging.basicConfig(level=logging.DEBUG, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")
    asyncio.run(main())

# ──────────────────────────────────────────────────────────────────────────────
-e 
-------- END OF FILE models/task.py --------

-------- START OF FILE models/user.py --------
# pixabit/models/user.py

# ─── Model ────────────────────────────────────────────────────────────────────
#            Habitica User Model and Subcomponents
# ──────────────────────────────────────────────────────────────────────────────

# SECTION: MODULE DOCSTRING
"""Defines Pydantic models for the Habitica User object and its complex nested
structures like profile, auth, preferences, stats, items, achievements, etc.
Includes methods for calculating derived stats like effective attributes and max HP/MP.
"""

# SECTION: IMPORTS
from __future__ import annotations  # Allow forward references

import json
import logging
import math
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Literal  # Use standard types

# External Libs
import emoji_data_python
from pydantic import (
    BaseModel,
    ConfigDict,
    Field,
    PrivateAttr,  # For internal storage
    ValidationError,
    ValidationInfo,
    computed_field,  # For calculated fields in dumps
    field_validator,
    model_validator,
)

# Local Imports
try:
    from pixabit.api.client import HabiticaClient
    from pixabit.config import HABITICA_DATA_PATH, USER_ID  # If USER_ID needed as fallback
    from pixabit.helpers._json import load_pydantic_model, save_json, save_pydantic_model
    from pixabit.helpers._logger import log
    from pixabit.helpers.DateTimeHandler import DateTimeHandler

    # Dependent models (use TYPE_CHECKING if circularity is a risk)
    from .game_content import Gear, StaticContentManager  # Need Gear model + manager for stats
    from .message import Message, MessageList  # For inbox
    from .party import QuestInfo  # For user's view of party quest
    from .tag import Tag, TagList  # For user tags
except ImportError:
    # Fallbacks
    log = logging.getLogger(__name__)
    log.addHandler(logging.NullHandler())
    USER_ID = "fallback_user_id"
    HABITICA_DATA_PATH = Path("./pixabit_cache")

    def save_json(d, p, **k):
        pass

    def save_pydantic_model(m, p, **k):
        pass

    def load_pydantic_model(cls, p, **k):
        return None

    class DateTimeHandler:
        def __init__(self, timestamp):
            self._ts = timestamp

        @property
        def utc_datetime(self):
            return None

    log.warning("user.py: Could not import dependencies. Using fallbacks.")

# SECTION: USER SUBCOMPONENT MODELS


# KLASS: UserProfile
class UserProfile(BaseModel):
    """Represents user profile information like display name and blurb."""

    model_config = ConfigDict(extra="ignore", populate_by_name=True)

    name: str = Field("", description="User's display name (parsed).")
    blurb: str | None = Field(None, description="User's profile description (parsed).")  # Blurb can be absent

    @field_validator("name", "blurb", mode="before")
    @classmethod
    def parse_text_emoji(cls, value: Any) -> str | None:
        """Parse text and replace emoji shortcodes, strips whitespace."""
        if isinstance(value, str):
            parsed = emoji_data_python.replace_colons(value).strip()
            return parsed  # Return potentially empty string or None
        # Handle name default elsewhere if needed
        return None


# KLASS: UserAuthLocal (Nested under UserAuth)
class UserAuthLocal(BaseModel):
    """Nested local authentication details."""

    model_config = ConfigDict(extra="ignore")
    username: str | None = Field(None)
    # email: str | None = None # Ignore email for privacy unless needed


# KLASS: UserAuthTimestamps (Nested under UserAuth)
class UserAuthTimestamps(BaseModel):
    """Nested timestamp information for authentication."""

    model_config = ConfigDict(extra="ignore")
    created: datetime | None = None
    updated: datetime | None = None
    loggedin: datetime | None = None

    @field_validator("created", "updated", "loggedin", mode="before")
    @classmethod
    def parse_datetime_utc(cls, value: Any) -> datetime | None:
        """Parses timestamp using DateTimeHandler."""
        # Allow null values through
        if value is None:
            return None
        handler = DateTimeHandler(timestamp=value)
        if handler.utc_datetime is None and value is not None:
            log.warning(f"Could not parse auth timestamp: {value!r}")
        return handler.utc_datetime


# KLASS: UserAuth
class UserAuth(BaseModel):
    """Represents user authentication details (wrapping local and timestamps)."""

    model_config = ConfigDict(extra="ignore")

    local: UserAuthLocal | None = Field(default_factory=UserAuthLocal)
    timestamps: UserAuthTimestamps | None = Field(default_factory=UserAuthTimestamps)

    # Convenience properties for easier access
    @property
    def username(self) -> str | None:
        return self.local.username if self.local else None

    @property
    def created_at(self) -> datetime | None:
        return self.timestamps.created if self.timestamps else None

    @property
    def updated_at(self) -> datetime | None:
        return self.timestamps.updated if self.timestamps else None

    @property
    def logged_in_at(self) -> datetime | None:
        return self.timestamps.loggedin if self.timestamps else None


# KLASS: UserPreferences
class UserPreferences(BaseModel):
    """User-specific preferences and settings."""

    model_config = ConfigDict(extra="ignore", populate_by_name=True)

    sleep: bool = Field(False, description="Whether user is resting in the Inn.")
    day_start: int = Field(0, alias="dayStart", description="User's preferred day start hour (0–23).")
    timezone_offset: int | None = Field(None, alias="timezoneOffset", description="User's current timezone offset from UTC in minutes.")
    timezone_offset_at_last_cron: int | None = Field(None, alias="timezoneOffsetAtLastCron", description="User's timezone offset at the time of the last cron.")
    # Other preferences like: email, language, chatRevoked, background, costume, shirt etc. ignored by default

    @field_validator("day_start", mode="before")
    @classmethod
    def parse_day_start(cls, value: Any) -> int:
        """Validate and clamp day start hour."""
        try:
            ds = int(value)
            return max(0, min(23, ds))  # Clamp between 0 and 23
        except (ValueError, TypeError):
            log.debug(f"Invalid dayStart value '{value}'. Using default 0.")
            return 0


# KLASS: Buffs (Used within UserStats)
class Buffs(BaseModel):
    """Temporary stat increases/decreases from spells, food, etc."""

    model_config = ConfigDict(extra="allow", populate_by_name=True)  # Allow extra buffs like seafoam

    # Use aliases for fields conflicting with built-ins/types
    con: float = Field(0.0)  # Buffs can be floats (gear sets)
    int_: float = Field(0.0, alias="int")
    per: float = Field(0.0)
    str_: float = Field(0.0, alias="str")
    stealth: int = Field(0)  # Stealth usually integer buff stacks

    @field_validator("con", "int_", "per", "str_", mode="before")
    @classmethod
    def ensure_float(cls, value: Any) -> float:
        try:
            return float(value)
        except (ValueError, TypeError):
            return 0.0

    @field_validator("stealth", mode="before")
    @classmethod
    def ensure_int(cls, value: Any) -> int:
        try:
            return int(value)
        except (ValueError, TypeError):
            return 0


# KLASS: Training (Used within UserStats)
class Training(BaseModel):
    """Permanent stat increases from leveling/resets (can have fractions)."""

    model_config = ConfigDict(extra="ignore", populate_by_name=True)  # Ignore rev, assigned etc.

    con: float = Field(0.0)
    int_: float = Field(0.0, alias="int")
    per: float = Field(0.0)
    str_: float = Field(0.0, alias="str")

    @field_validator("con", "int_", "per", "str_", mode="before")
    @classmethod
    def ensure_float(cls, value: Any) -> float:
        try:
            return float(value)
        except (ValueError, TypeError):
            return 0.0


# KLASS: EquippedGear (Used within UserItems)
class EquippedGear(BaseModel):
    """Represents gear currently equipped by the user."""

    model_config = ConfigDict(extra="allow")  # Allow other potential slots

    # Define known slots explicitly
    weapon: str | None = Field(None)
    armor: str | None = Field(None)
    head: str | None = Field(None)
    shield: str | None = Field(None)  # Can be None if weapon is two-handed
    headAccessory: str | None = Field(None, alias="headAccessory")
    eyewear: str | None = Field(None)
    body: str | None = Field(None)  # e.g., Robe
    back: str | None = Field(None)  # e.g., Cape

    def get_equipped_item_keys(self) -> list[str]:
        """Returns a list of non-None gear keys currently equipped."""
        # Use model_dump to get values respecting aliases, exclude None
        return [val for val in self.model_dump(exclude_none=True).values() if isinstance(val, str)]

    def calculate_total_bonus(self, user_class: str | None, gear_data: dict[str, Gear]) -> dict[str, float]:
        """Calculates total stat bonuses from equipped gear, considering class match.

        Args:
             user_class: User's character class (e.g., 'warrior').
             gear_data: Dict mapping gear keys to validated Gear objects.

        Returns:
             Dictionary {'str': bonus, 'con': bonus, 'int': bonus, 'per': bonus}.
        """
        total_bonus = {"str": 0.0, "con": 0.0, "int": 0.0, "per": 0.0}
        # Note: Gear model fields are already corrected (strength, intelligence etc.)
        # Mapping direct gear field names to the bonus keys
        stats_map_gear_to_bonus = {"strength": "str", "constitution": "con", "intelligence": "int", "perception": "per"}

        for gear_key in self.get_equipped_item_keys():
            item: Gear | None = gear_data.get(gear_key)
            # Check if item exists and is a validated Gear model
            if item and isinstance(item, Gear):
                # --- CORRECTED CLASS CHECK ---
                # Class bonus multiplier (1.5x if item class matches user class)
                # Habitica defines 'base' sometimes for general gear? Including 'base' match here too.
                # Check if item.special_class is not None before comparing
                is_class_match = item.special_class is not None and (item.special_class == user_class)
                bonus_multiplier = 1.5 if is_class_match else 1.0
                # --- END CORRECTION ---

                # Add item stats to total, applying multiplier
                for gear_stat_field, bonus_key in stats_map_gear_to_bonus.items():
                    # Get stat value directly from the item model field
                    stat_value = getattr(item, gear_stat_field, 0.0)
                    total_bonus[bonus_key] += stat_value * bonus_multiplier
            # else: log.debug(f"Gear key '{gear_key}' not found in provided gear_data or is not a Gear object.")
        print(total_bonus)
        return total_bonus


# KLASS: UserItems
class UserItems(BaseModel):
    """Holds user's inventory: gear, consumables, pets, mounts, etc."""

    model_config = ConfigDict(extra="ignore", populate_by_name=True)

    gear_equipped: EquippedGear = Field(default_factory=EquippedGear, alias="equipped")
    gear_costume: EquippedGear = Field(default_factory=EquippedGear, alias="costume")
    # gear.owned maps key -> True/False
    # gear_owned: dict[str, bool] = Field(default_factory=dict, alias="owned")
    # Consumables
    # eggs: dict[str, int] = Field(default_factory=dict)
    # food: dict[str, int] = Field(default_factory=dict)
    # hatchingPotions: dict[str, int] = Field(default_factory=dict, alias="hatchingPotions")
    # Companions
    # pets: dict[str, int] = Field(default_factory=dict, description="{Pet-SpeciesKey: feed_count}")  # e.g. {"BearCub-Base": 5}
    # mounts: dict[str, bool] = Field(default_factory=dict, description="{Mount-SpeciesKey: True}")  # e.g. {"BearCub-Base": True}
    # Special items & Quest scrolls
    # special: dict[str, int] = Field(default_factory=dict)  # Orbs, Cards etc. {itemKey: count}
    quests: dict[str, int] = Field(default_factory=dict)  # {questKey: count}

    @model_validator(mode="before")
    @classmethod
    def structure_gear_data(cls, data: Any) -> dict[str, Any]:
        """Ensure gear keys (equipped, costume, owned) exist from gear sub-dict."""
        if not isinstance(data, dict):
            return data
        values = data.copy()
        # Check if fields are already top-level (might happen if processing elsewhere)
        if "equipped" not in values and "gear" in values and isinstance(values["gear"], dict):
            gear_data = values["gear"]
            values["equipped"] = gear_data.get("equipped", {})
            values["costume"] = gear_data.get("costume", {})
            values["owned"] = gear_data.get("owned", {})
            # Optionally remove the original 'gear' key if desired
            # values.pop("gear")
        # Ensure defaults if keys are still missing
        for key in ["equipped", "costume", "owned"]:
            if key not in values:
                values[key] = {}

        # Validate counts for items? Pydantic handles dict[str, int] usually
        return values


# KLASS: UserAchievements
class UserAchievements(BaseModel):
    """Holds user's achievements progress."""

    model_config = ConfigDict(extra="ignore", populate_by_name=True)

    challenges: list[str] = Field(default_factory=list, description="List of challenge IDs completed.")
    quests: dict[str, int] = Field(default_factory=dict, description="{quest_key: completion_count}.")
    perfect_days: int = Field(0, alias="perfect", description="Count of perfect days.")
    streak: int = Field(0, description="Max consecutive perfect days streak.")
    loginIncentives: int = Field(0, alias="loginIncentives", description="Count of login incentives claimed for achievements.")
    ultimateGearSets: dict[str, bool] = Field(default_factory=dict, alias="ultimateGearSets")
    # Store other achievements found directly under 'achievements'
    other_achievements: dict[str, Any] = Field(default_factory=dict)

    @model_validator(mode="before")
    @classmethod
    def separate_other_achievements(cls, data: Any) -> dict[str, Any]:
        """Separates known fields from arbitrary others under 'achievements'."""
        if not isinstance(data, dict):
            return data
        values = {}
        known_keys = {"challenges", "quests", "perfect", "streak", "loginIncentives", "ultimateGearSets"}
        other_achievements_dict = {}
        for k, v in data.items():
            if k in known_keys:
                values[k] = v  # Keep known keys
            else:
                other_achievements_dict[k] = v  # Put others into separate dict
        values["other_achievements"] = other_achievements_dict
        return values

    @field_validator("perfect_days", "streak", "loginIncentives", mode="before")
    @classmethod
    def ensure_int(cls, value: Any) -> int:
        try:
            return int(value)
        except (ValueError, TypeError):
            return 0


# KLASS: UserPartyInfo (User's perspective)
class UserPartyInfo(BaseModel):
    """Holds information about the user's current party membership and quest status."""

    model_config = ConfigDict(extra="ignore", populate_by_name=True)

    party_id: str | None = Field(None, alias="_id", description="The unique ID of the party the user is in.")
    # The user object often contains a snapshot of the party quest status relevant *to the user*.
    quest: QuestInfo | None = Field(None, description="User's view of the active party quest status and progress.")

    @model_validator(mode="before")
    @classmethod
    def ensure_party_id_mapping(cls, data: Any) -> dict[str, Any]:
        """Map party._id to party_id if structure is party:{_id: ...}."""
        if not isinstance(data, dict):
            return data  # Or {}
        values = data.copy()
        # If raw data looks like user.party = {_id: ..., quest: ...}
        if "_id" in values and "party_id" not in values:
            values["party_id"] = values["_id"]
        return values


# KLASS: UserInboxInfo
class UserInboxInfo(BaseModel):
    """Holds information about the user's inbox."""

    model_config = ConfigDict(extra="ignore", populate_by_name=True)

    newMessages: int = Field(0, alias="newMessages", description="Count of unread private messages.")
    hasNew: bool = Field(False, alias="hasNew", description="Flag indicating a new gift message.")  # Less informative, count is better
    optOut: bool = Field(False, alias="optOut", description="Whether the user has opted out of receiving new PMs.")
    blocks: list[str] = Field(default_factory=list, description="List of user IDs blocked by this user.")
    # Optional: Can embed received messages directly if API provides them here
    messages: MessageList | None = None  # Type hint

    @field_validator("newMessages", mode="before")
    @classmethod
    def ensure_int(cls, value: Any) -> int:
        try:
            return int(value)
        except (ValueError, TypeError):
            return 0

    @field_validator("messages", mode="before")  # Example if API embeds messages as dict {msgId: data}
    @classmethod
    def messages_dict_to_list(cls, value: Any) -> list[dict] | None:
        """Converts message dict to list for MessageList validation."""
        if isinstance(value, dict):
            return list(value.values())
        elif isinstance(value, list):
            return value  # Already a list
        return None  # Return None or empty list if invalid format


# KLASS: UserStats (Main Stats Container) - Continuing from previous section
class UserStats(BaseModel):
    """Represents the user's core numerical stats and attributes."""

    model_config = ConfigDict(extra="ignore", populate_by_name=True)

    # --- Resources ---
    hp: float = Field(default=50.0)
    mp: float = Field(default=0.0)
    exp: float = Field(default=0.0)
    gp: float = Field(default=0.0)
    lvl: int = Field(default=1)
    klass: Literal["warrior", "rogue", "healer", "wizard"] | None = Field(None, alias="class")

    # --- Base Attributes (Allocated Points) ---
    base_con: int = Field(0, alias="con")
    base_int_: int = Field(0, alias="int")
    base_per: int = Field(0, alias="per")
    base_str_: int = Field(0, alias="str")

    # --- Base Max Values (Before CON/INT bonuses) ---
    max_hp: int = Field(50, alias="maxHealth")
    max_mp: int = Field(10, alias="maxMP")  # Base before INT bonus
    max_exp: int = Field(0, alias="toNextLevel")

    # --- Modifiers (Nested Models) ---
    buffs: Buffs = Field(default_factory=Buffs)
    training: Training = Field(default_factory=Training)

    # --- Validators ---
    @field_validator("hp", "mp", "exp", "gp", mode="before")
    @classmethod
    def ensure_float_resources(cls, value: Any) -> float:
        try:
            return float(value)
        except (ValueError, TypeError):
            return 0.0

    @field_validator("lvl", "max_hp", "max_mp", "max_exp", "base_con", "base_int_", "base_per", "base_str_", mode="before")  # Add base stats here
    @classmethod
    def ensure_int(cls, value: Any) -> int:
        """Ensures integer fields are parsed correctly."""
        try:
            return int(value)
        except (ValueError, TypeError):
            return 0

    # No separate calculation class needed. Calculations happen on User or here if self-contained.

    def calculate_level_bonus(self) -> float:
        """Calculate stat bonus from level."""
        return min(50.0, math.floor(self.lvl / 2.0))  # Max bonus of +50 from levels

    def calculate_stats_before_gear(self) -> dict[str, float]:
        """Calculates stats combining base, buffs, and level bonus."""
        level_bonus = self.calculate_level_bonus()
        return {
            "con": float(self.base_con) + self.buffs.con + level_bonus,
            "int": float(self.base_int_) + self.buffs.int_ + level_bonus,
            "per": float(self.base_per) + self.buffs.per + level_bonus,
            "str": float(self.base_str_) + self.buffs.str_ + level_bonus,
        }


# ──────────────────────────────────────────────────────────────────────────────


# SECTION: MAIN USER MODEL


# KLASS: User
class User(BaseModel):
    """Represents the complete Habitica User object, aggregating data from the API."""

    model_config = ConfigDict(extra="ignore", populate_by_name=True, validate_assignment=True)

    # --- Top-Level Fields ---
    id: str = Field(..., alias="_id", description="Unique User UUID.")
    balance: float = Field(default=0.0, description="User's gem balance / 4.")
    needs_cron: bool = Field(False, alias="needsCron", description="Flag indicating cron needs to run.")
    last_cron: datetime | None = Field(None, alias="lastCron", description="Timestamp of the last cron run (UTC).")
    login_incentives: int = Field(0, alias="loginIncentives", description="Current login incentive count for rewards.")

    # --- Nested Subcomponent Models ---
    profile: UserProfile = Field(default_factory=UserProfile)
    auth: UserAuth = Field(default_factory=UserAuth)
    preferences: UserPreferences = Field(default_factory=UserPreferences)
    items: UserItems = Field(default_factory=UserItems)
    achievements: UserAchievements = Field(default_factory=UserAchievements)
    party: UserPartyInfo = Field(default_factory=UserPartyInfo)
    inbox: UserInboxInfo = Field(default_factory=UserInboxInfo)
    stats: UserStats = Field(default_factory=UserStats)
    tags: TagList | None = Field(default_factory=TagList, description="User's defined tags.")  # Populate from separate endpoint typically
    challenges: list[str] | None = Field([], description="List of Challenges")
    # --- Calculated Fields (Storage) ---
    # Store results of expensive calculations here, mark Private or Exclude
    _calculated_stats: dict[str, Any] = PrivateAttr(default_factory=dict)

    # --- Validators ---
    @model_validator(mode="before")
    @classmethod
    def ensure_id_mapping(cls, data: Any) -> dict[str, Any]:
        """Map _id to id if needed."""
        if isinstance(data, dict):
            if "_id" in data and "id" not in data:
                data["id"] = data["_id"]
            # Handle tags being directly in user data vs loaded separately
            if "tags" in data and isinstance(data["tags"], list):
                # Pydantic will validate the list against the TagList model
                pass
            else:
                data["tags"] = []  # Ensure tags field exists for TagList parsing

        return data if isinstance(data, dict) else {}

    @field_validator("last_cron", mode="before")
    @classmethod
    def parse_last_cron_utc(cls, value: Any) -> datetime | None:
        """Parse lastCron timestamp using DateTimeHandler."""
        handler = DateTimeHandler(timestamp=value)
        return handler.utc_datetime

    @field_validator("balance", mode="before")
    @classmethod
    def ensure_float_balance(cls, value: Any) -> float:
        try:
            return float(value)
        except (ValueError, TypeError):
            return 0.0

    @field_validator("login_incentives", mode="before")
    @classmethod
    def ensure_int_login(cls, value: Any) -> int:
        try:
            return int(value)
        except (ValueError, TypeError):
            return 0

    @field_validator("tags", mode="before")
    @classmethod
    def tags_list_to_taglist(cls, value: Any) -> dict[str, list]:
        """Ensure tags input is structured correctly for TagList model."""
        if isinstance(value, list):
            # Wrap list in a dict for Pydantic to parse into TagList(tags=...)
            return {"tags": value}
        elif isinstance(value, dict) and "tags" in value:
            return value  # Already structured
        return {"tags": []}  # Default empty

    # --- Convenience Properties (Accessing nested data) ---
    @property
    def username(self) -> str | None:
        return self.auth.username

    @property
    def display_name(self) -> str:
        return self.profile.name or self.username or "N/A"  # Fallback display

    @property
    def level(self) -> int:
        return self.stats.lvl

    @property
    def klass(self) -> str | None:
        return self.stats.klass

    @property
    def hp(self) -> float:
        return self.stats.hp

    @property
    def mp(self) -> float:
        return self.stats.mp

    @property
    def gp(self) -> float:
        return self.stats.gp

    @property
    def exp(self) -> float:
        return self.stats.exp

    @property
    def party_id(self) -> str | None:
        return self.party.party_id

    @property
    def is_sleeping(self) -> bool:
        return self.preferences.sleep

    @property
    def max_exp(self) -> int:
        return self.stats.max_exp

    @computed_field(description="Calculated Gem count.")
    @property
    def gems(self) -> int:
        """Calculated gem count from balance (balance = gems / 4)."""
        return int(self.balance * 4) if self.balance > 0 else 0

    @property
    def stealth(self) -> int:
        """Shortcut to the stealth buff value."""
        return self.stats.buffs.stealth

    @property
    def is_on_boss_quest(self) -> bool:
        """Check if user is currently on an active boss quest."""
        if self.party.quest and self.party.quest.is_active_and_ongoing:
            # Requires fetching static data and checking quest type
            static_details = self.party.static_quest_details  # Access cached static data
            if static_details and getattr(static_details, "is_boss_quest", False):
                return True
        return False

    # --- Accessing Calculated Stats ---

    @property
    def effective_stats(self) -> dict[str, float]:
        """Returns the pre-calculated effective stats (STR, CON, INT, PER). Call `calculate_effective_stats` first."""
        return self._calculated_stats.get("effective_stats", {})

    @property
    def max_hp(self) -> float:
        """Returns the pre-calculated maximum HP. Call `calculate_effective_stats` first."""
        return self._calculated_stats.get("max_hp", 50.0)  # Default 50

    @property
    def max_mp(self) -> float:
        """Returns the pre-calculated maximum MP. Call `calculate_effective_stats` first."""
        # Base MP depends on class - needs logic or rely on calculated value
        return self._calculated_stats.get("max_mp", 10.0)  # Default base

    # --- Calculation Method ---

    def calculate_effective_stats(self, gear_data: dict[str, Gear] | None = None) -> None:
        """Calculates total effective stats (base, buffs, training, level, gear)
        and max HP/MP. Stores the results in the internal `_calculated_stats` dict.

        Args:
            gear_data: A dictionary mapping gear keys to **validated Gear objects**.
                       Required for accurate calculations. If None, gear bonus will be 0.
        """
        log.debug(f"Calculating effective stats for user {self.id}...")
        if gear_data is None:
            log.warning("No gear_data provided to calculate_effective_stats. Gear bonus will be zero.")
            gear_data = {}  # Use empty dict

        # 1. Get stats before gear bonus (base + buffs + training + level)
        stats_before_gear = self.stats.calculate_stats_before_gear()

        # 2. Calculate gear bonus
        gear_bonus = self.items.gear_equipped.calculate_total_bonus(self.klass, gear_data)

        # 3. Combine for final effective stats
        eff_stats: dict[str, float] = {}
        for stat in ["str", "con", "int", "per"]:
            eff_stats[stat] = stats_before_gear.get(stat, 0.0) + gear_bonus.get(stat, 0.0)
        log.debug(f" -> StatsBeforeGear: {stats_before_gear}")
        log.debug(f" -> GearBonus: {gear_bonus}")
        log.debug(f" -> EffectiveStats: {eff_stats}")

        # 4. Calculate Max HP/MP using effective CON/INT
        effective_con = eff_stats.get("con", 0.0)
        effective_int = eff_stats.get("int", 0.0)
        # Base HP + 2HP per Effective CON point (floor CON first?) Habitica math can be subtle. Assume direct multiplier for now.
        max_hp_calc = float(self.stats.max_hp + (effective_con * 2.0))
        # Base MP + 2MP per Effective INT point? (or mana multiplier based on class?) Need accurate formula. Using +2/INT for now.
        # Example: Wizard MP: 30 + 2.5*INT + Lvl/2; Healer MP: 30 + 2*INT + Lvl/4 ? Research needed.
        # Using a simplified base + INT multiplier
        max_mp_calc = float(self.stats.max_mp + (effective_int * 2.0))
        log.debug(f" -> Calculated MaxHP: {max_hp_calc} (Base: {self.stats.max_hp}, EffCON: {effective_con:.1f})")
        log.debug(f" -> Calculated MaxMP: {max_mp_calc} (Base: {self.stats.max_mp}, EffINT: {effective_int:.1f})")

        # 5. Store all calculated values internally
        self._calculated_stats["effective_stats"] = eff_stats
        self._calculated_stats["max_hp"] = round(max_hp_calc, 1)
        self._calculated_stats["max_mp"] = round(max_mp_calc, 1)
        # Can store other derived values here too if needed
        # self._calculated_stats["gems"] = self.gems

    # --- Factory & Serialization ---
    @classmethod
    def create_from_raw_data(cls, raw_data: dict) -> User | None:
        """Validates raw API data into a User object."""
        if not isinstance(raw_data, dict):
            log.error("Invalid raw data type for User creation: Expected dict.")
            return None
        try:
            user_instance = cls.model_validate(raw_data)
            log.info(f"User model created successfully for ID: {user_instance.id}")
            return user_instance
        except ValidationError as e:
            log.error(f"Validation failed creating User model: {e}", exc_info=False)
            return None
        except Exception as e:
            log.exception("Unexpected error creating User from raw data.")
            return None

    # Default model_dump/model_dump_json are sufficient now


# ──────────────────────────────────────────────────────────────────────────────


# SECTION: MAIN EXECUTION (Example/Test)
async def main():
    """Demo function to retrieve, process, and display user data."""
    log.info("--- User Model Demo ---")
    user_instance: User | None = None

    try:
        cache_dir = HABITICA_DATA_PATH / "user"
        cache_dir.mkdir(exist_ok=True, parents=True)
        raw_path = cache_dir / "user_raw.json"
        processed_path = cache_dir / "user_processed.json"

        # 1. Fetch raw data
        log.info("Fetching user data from API...")
        api = HabiticaClient()  # Assumes configured
        raw_data = await api.get_user_data()
        if not raw_data:
            log.error("Failed to fetch user data. Exiting demo.")
            return None
        log.success("Fetched raw user data.")
        save_json(raw_data, raw_path.name, folder=raw_path.parent)  # Save raw

        # 2. Create User model
        log.info("Validating raw data into User model...")
        user_instance = User.create_from_raw_data(raw_data)
        if not user_instance:
            log.error("Failed to create User model from raw data.")
            return None

        # 3. Load Static Gear Data (needed for calculation)
        log.info("Loading static gear data for stat calculation...")
        content_manager = StaticContentManager()  # Assumes it can load/cache
        static_gear_data = await content_manager.get_gear()  # Returns dict[str, Gear]
        if not static_gear_data:
            log.warning("Could not load static gear data. Effective stats will exclude gear bonus.")

        # 4. Calculate Effective Stats
        log.info("Calculating effective stats...")
        user_instance.calculate_effective_stats(gear_data=static_gear_data)
        log.success("Effective stats calculated.")

        # 5. Display Data
        print("\n--- Basic Info ---")
        print(f"ID          : {user_instance.id}")
        print(f"Username    : {user_instance.username}")
        print(f"Display Name: {user_instance.display_name}")
        print(f"Level       : {user_instance.level}")
        print(f"Class       : {user_instance.klass}")
        print(f"Sleeping    : {user_instance.is_sleeping}")
        print(f"Party ID    : {user_instance.party_id}")
        print(f"Gems        : {user_instance.gems}")

        print("\n--- Core Resources ---")
        # Use calculated max values now
        print(f"HP          : {user_instance.hp:.1f} / {user_instance.max_hp:.1f}")
        print(f"MP          : {user_instance.mp:.1f} / {user_instance.max_mp:.1f}")
        print(f"EXP         : {user_instance.exp:.0f} / {user_instance.max_exp}")
        print(f"Gold        : {user_instance.gp:.2f}")

        print("\n--- Effective Stats (incl. Gear) ---")
        for stat, value in user_instance.effective_stats.items():
            # Access base/training/buff for breakdown (optional)
            base_stat = getattr(user_instance.stats, f"base_{'int_' if stat=='int' else ('str_' if stat=='str' else stat)}", 0)
            train_stat = getattr(user_instance.stats.training, f"{'int_' if stat=='int' else ('str_' if stat=='str' else stat)}", 0.0)
            buff_stat = getattr(user_instance.stats.buffs, f"{'int_' if stat=='int' else ('str_' if stat=='str' else stat)}", 0.0)
            # Gear bonus = effective - (base + train + buff + level)
            level_bonus = user_instance.stats.calculate_level_bonus()
            gear_b = value - (base_stat + buff_stat + level_bonus)
            print(f"{stat.upper():<4}: {value:>5.1f}   (Base:{base_stat} Train:{train_stat:.1f} Buff:{buff_stat:.1f} Lvl:{level_bonus:.0f} Gear:{gear_b:+.1f})")

        print("\n--- User Tags ---")
        if user_instance.tags:
            print(f"Tag Count   : {len(user_instance.tags.tags)}")
            print(f"Sample Tags : {[t.name for t in user_instance.tags.tags[:5]]}...")
        else:
            print("No tags found/loaded.")

        # 6. Save processed data (including calculated stats if needed)
        # By default, PrivateAttr (_calculated_stats) isn't dumped.
        # If you want to save them, make _calculated_stats a regular Field or modify dumping.
        log.info(f"Saving processed user data to {processed_path}...")
        # save_pydantic_model(user_instance, processed_path) # Standard dump
        # Or to include calculated stats:
        user_data_dict = user_instance.model_dump(mode="json")
        user_data_dict["calculated"] = user_instance._calculated_stats  # Manually add private attr
        save_json(user_data_dict, processed_path.name, folder=processed_path.parent)  # Save merged dict
        log.success("Processed user data (with calculated) saved.")

    except ConnectionError as e:
        log.error(f"API Connection error: {e}")
    except ValidationError as e:
        log.error(f"Pydantic Validation Error: {e}")
    except Exception as e:
        log.exception(f"An unexpected error occurred in the user demo: {e}")

    return user_instance


if __name__ == "__main__":
    import asyncio

    logging.basicConfig(level=logging.DEBUG, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")
    asyncio.run(main())

# ──────────────────────────────────────────────────────────────────────────────
-e 
-------- END OF FILE models/user.py --------

-------- START OF FILE models/__init__.py --------
-e 
-------- END OF FILE models/__init__.py --------

-------- START OF FILE services/archiver.py --------
# pixabit/services/archiver.py

# ─── Title ────────────────────────────────────────────────────────────────────
#            Persistent Data Archiver (SQLite Backend)
# ──────────────────────────────────────────────────────────────────────────────

# SECTION: MODULE DOCSTRING
"""
Provides the Archiver class for long-term persistent storage of Habitica data
(e.g., Challenges, completed Todos) using an SQLite database. Separates archival
logic from the main live data management.
"""

# SECTION: IMPORTS
from __future__ import annotations

import sqlite3
import json
from pathlib import Path
from datetime import datetime, timezone
from typing import List, Dict, Optional

# Project Imports (Ensure these resolve)
try:
    # Import necessary MODELS that will be archived/retrieved
    from pixabit.models.challenge import Challenge
    from pixabit.models.task import Task, AnyTask, TaskList # For deserializing tasks/todos
    # Import Pydantic specifically for Validation Error if validating on load
    from pydantic import ValidationError
    from pixabit.helpers._logger import log
    from pixabit.helpers.DateTimeHandler import DateTimeHandler # For parsing dates from DB
    # Add other models here if they will be archived (e.g., Todo)
except ImportError as e:
     import logging
     log = logging.getLogger(__name__); log.addHandler(logging.NullHandler())
     log.critical(f"Archiver failed imports: {e}. Check structure."); raise

# SECTION: CONSTANTS & CONFIG

ARCHIVE_DB_FILENAME = "persistent_archive.db" # Default filename

# SECTION: ARCHIVER CLASS

# KLASS: Archiver
class Archiver:
    """
    Handles persistent storage and retrieval of Habitica data using SQLite.
    Manages challenges, and potentially completed tasks like Todos.
    """
    def __init__(self, db_path: Path):
        """
        Initializes the Archiver and connects to the database.

        Args:
            db_path: The full path to the SQLite database file.
        """
        self.db_path = db_path
        self._conn: sqlite3.Connection | None = None
        log.info(f"Initializing Archiver with DB: {self.db_path}")
        try:
            self._ensure_tables_exist() # Creates DB and tables if needed
        except Exception as e:
            log.critical(f"CRITICAL: Failed to initialize Archiver DB at {db_path}: {e}", exc_info=True)
            # Application might need to handle this critical failure

    def _get_conn(self) -> sqlite3.Connection | None:
        """Gets or establishes a connection to the archive SQLite database."""
        # Basic connection management (re-establish if closed)
        try:
             # Check if connection exists and is usable
            if self._conn:
                 # Quick check if connection is still active (may not be fully reliable)
                 try:
                    self._conn.execute("SELECT 1")
                    return self._conn
                 except sqlite3.Error:
                     self._close_db_conn() # Close broken connection

            # Establish new connection
            self.db_path.parent.mkdir(parents=True, exist_ok=True)
            self._conn = sqlite3.connect(self.db_path, timeout=10)
            self._conn.row_factory = sqlite3.Row
            self._conn.execute("PRAGMA foreign_keys = ON;")
            self._conn.execute("PRAGMA journal_mode=WAL;")
            log.debug(f"Established connection to DB: {self.db_path}")
            return self._conn
        except sqlite3.Error as e:
            log.error(f"Failed to connect to archive DB {self.db_path}: {e}")
            self._conn = None # Ensure conn is None on failure
            return None

    def _close_db_conn(self):
        """Closes the database connection if open."""
        if self._conn:
            log.debug(f"Closing Archiver DB connection to {self.db_path}")
            try:
                self._conn.close()
            except sqlite3.Error as e:
                log.error(f"Error closing archiver DB: {e}")
            self._conn = None

    def _ensure_tables_exist(self):
        """Creates database tables if they don't already exist."""
        conn = self._get_conn()
        if not conn: return # Cannot proceed

        try:
            cursor = conn.cursor()
            log.debug("Ensuring Archiver tables exist...")

            # --- Challenges Table ---
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS challenges_archive (
                    id TEXT PRIMARY KEY,
                    name TEXT, short_name TEXT, summary TEXT, description TEXT,
                    leader_id TEXT, leader_name TEXT,
                    group_id TEXT, group_name TEXT, group_type TEXT, group_privacy TEXT,
                    prize INTEGER, official INTEGER, created_at TEXT, broken TEXT,
                    tasks_json TEXT, last_archived_at TEXT NOT NULL
                )
            """)
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_challenges_archive_id ON challenges_archive(id)")

            # --- Add other tables as needed (e.g., Todos) ---
            # cursor.execute(""" CREATE TABLE IF NOT EXISTS todos_archive (...) """)
            # cursor.execute("CREATE INDEX IF NOT EXISTS ...")

            conn.commit()
            log.debug("Archiver tables ensured.")
        except sqlite3.Error as e:
            log.exception(f"Archiver failed to ensure tables: {e}")
            raise # Critical error
        # Keep connection open briefly? Or close? Close for simplicity unless bulk ops common.
        # self._close_db_conn()

    def _deserialize_challenge(self, row: sqlite3.Row) -> Challenge | None:
         """Helper to convert a DB row into a Challenge object."""
         if not row: return None
         try:
             challenge_data = dict(row)
             challenge_id = challenge_data.get("id")

             # Deserialize tasks from JSON
             tasks_list: list[AnyTask] = []
             tasks_json_str = challenge_data.pop("tasks_json", None)
             if tasks_json_str:
                  try:
                       raw_task_list = json.loads(tasks_json_str)
                       if isinstance(raw_task_list, list):
                            # Assuming TaskList class and from_processed_dicts are available
                            tasks_list = TaskList.from_processed_dicts(raw_task_list).tasks
                  except json.JSONDecodeError: log.warning(f"Invalid tasks JSON in archive for {challenge_id}")
                  except Exception as e: log.error(f"Error parsing tasks for {challenge_id} from archive: {e}")

             # Reconstruct nested models (if stored flat) and convert types
             challenge_data["official"] = bool(challenge_data.get("official", 0))
             challenge_data["created_at"] = DateTimeHandler(challenge_data.get("created_at")).utc_datetime
             # Rebuild leader dict if stored flat
             leader_data = None
             lid = challenge_data.pop('leader_id', None)
             if lid: leader_data = {'id': lid, 'name': challenge_data.pop('leader_name', None)}
             challenge_data['leader'] = leader_data
             # Rebuild group dict
             group_data = None
             gid = challenge_data.pop('group_id', None)
             if gid: group_data = {'id': gid, 'name': challenge_data.pop('group_name', None),
                                     'type': challenge_data.pop('group_type', None),
                                     'privacy': challenge_data.pop('group_privacy', None)}
             challenge_data['group'] = group_data

             # Validate dictionary against Challenge model
             challenge_instance = Challenge.model_validate(challenge_data)
             challenge_instance.tasks = tasks_list # Assign deserialized tasks
             return challenge_instance
         except ValidationError as e:
             log.warning(f"Skipping invalid archived challenge {challenge_id}: {e}")
             return None
         except Exception as e:
             log.exception(f"Error deserializing archived challenge {challenge_id} from DB")
             return None


    def archive_challenges(self, challenges: list[Challenge]) -> int:
        """Adds or replaces multiple challenges in the archive database. Returns count archived."""
        if not challenges: return 0
        conn = self._get_conn();
        if not conn: log.error("Cannot archive challenges, DB connection failed."); return 0
        log.info(f"Archiving {len(challenges)} challenges to DB...")
        success_count = 0
        try:
            cursor = conn.cursor()
            conn.execute("BEGIN TRANSACTION")
            now_iso = datetime.now(timezone.utc).isoformat()

            for challenge in challenges:
                if not isinstance(challenge, Challenge) or not challenge.id: continue
                try:
                    tasks_dump = [t.model_dump(mode='json', exclude={'styled_text','styled_notes'}) for t in challenge.tasks]
                    tasks_json_str = json.dumps(tasks_dump) if tasks_dump else None # Store null if no tasks

                    db_data = (
                        challenge.id, challenge.name, challenge.short_name, challenge.summary, challenge.description,
                        challenge.leader.id if challenge.leader else None, challenge.leader.name if challenge.leader else None,
                        challenge.group.id if challenge.group else None, challenge.group.name if challenge.group else None,
                        challenge.group.type if challenge.group else None, challenge.group.privacy if challenge.group else None,
                        challenge.prize, int(challenge.official),
                        challenge.created_at.isoformat() if challenge.created_at else None,
                        challenge.broken, tasks_json_str, now_iso
                    )
                    cursor.execute("""
                        INSERT OR REPLACE INTO challenges_archive VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """, db_data)
                    success_count += 1
                except Exception as item_e:
                     log.error(f"Failed preparing/saving chal {challenge.id} to archive DB: {item_e}")

            conn.commit()
            log.info(f"Archived {success_count}/{len(challenges)} challenges.")

        except sqlite3.Error as e:
            log.exception(f"Failed saving challenges archive batch: {e}")
            if conn: try: conn.rollback() except: pass
            success_count = 0 # Indicate failure
        finally:
             self._close_db_conn() # Close after operation

        return success_count

    def get_archived_challenge(self, challenge_id: str) -> Challenge | None:
        """Retrieves a single challenge from the archive DB."""
        conn = self._get_conn();
        if not conn: return None
        challenge_instance = None
        try:
            cursor = conn.cursor()
            cursor.execute("SELECT * FROM challenges_archive WHERE id = ?", (challenge_id,))
            row = cursor.fetchone()
            if row:
                 challenge_instance = self._deserialize_challenge(row)
                 log.debug(f"Retrieved challenge '{challenge_id}' from archive.")
            else:
                 log.debug(f"Challenge '{challenge_id}' not found in archive.")
        except sqlite3.Error as e:
            log.exception(f"Failed get archived challenge {challenge_id}: {e}")
        # finally: self._close_db_conn() # Keep open potentially? Close for now.
        return challenge_instance

    def load_all_challenges_from_archive(self) -> Dict[str, Challenge]:
         """Loads all challenges from the archive DB into a dictionary."""
         log.info("Loading all challenges from archive DB...")
         all_challenges = {}
         conn = self._get_conn()
         if not conn:
             return {}
         try:
              cursor = conn.cursor()
              cursor.execute("SELECT * FROM challenges_archive ORDER BY last_archived_at DESC")
              rows = cursor.fetchall()
              for row in rows:
                   challenge = self._deserialize_challenge(row)
                   if challenge and challenge.id:
                       all_challenges[challenge.id] = challenge
              log.info(f"Loaded {len(all_challenges)} challenges from archive DB.")
         except sqlite3.Error as e:
              log.exception("Failed to load all challenges from archive DB.")
         finally:
              self._close_db_conn() # Close after potentially large load
         return all_challenges


    # --- Add methods for archiving/retrieving other items (e.g., Todos) ---
    # def archive_todo(self, todo: Todo) -> bool: ...
    # def get_archived_todos(self, ...) -> List[Todo]: ...

    def close(self):
        """Closes database connections and performs cleanup."""
        log.info("Closing DataManager resources...")
        self._close_db_conn()
        # Add other cleanup if needed
        log.info("DataManager closed.")


# ──────────────────────────────────────────────────────────────────────────────
-e 
-------- END OF FILE services/archiver.py --------

-------- START OF FILE services/challenge_service.py --------
from __future__ import annotations

import asyncio
from typing import TYPE_CHECKING, Any, Dict, List, Literal, Optional

from pixabit.helpers._logger import log
from pixabit.models.challenge import Challenge, ChallengeList
from pixabit.models.task import Task, TaskList

if TYPE_CHECKING:
    from pixabit.api.client import HabiticaClient
    from pixabit.api.mixin.challenge_mixin import ChallengeKeepOption, TaskKeepOption
    from pixabit.models.challenge import Challenge, ChallengeList
    from pixabit.models.task import Task, TaskList

    from .data_manager import DataManager


class ChallengeService:
    """Service layer for interacting with Habitica Challenges.
    Coordinates API calls and updates the local data managed by DataManager.
    """

    def __init__(self, api_client: HabiticaClient, data_manager: DataManager):
        """Initializes the ChallengeService.

        Args:
            api_client: The Habitica API client instance.
            data_manager: The data manager instance holding live data models.
        """
        self.api = api_client
        self.dm = data_manager
        log.debug("ChallengeService initialized.")

    # --- Read Operations (Synchronous - access cached data) ---

    def get_cached_challenges(self) -> ChallengeList | None:
        """Returns the cached ChallengeList instance from the DataManager.

        Returns:
            The cached ChallengeList or None if not loaded.
        """
        if not self.dm.challenges:
            log.warning("Attempted to get challenges, but ChallengeList is not loaded in DataManager.")
        return self.dm.challenges

    def get_challenge_by_id(self, challenge_id: str) -> Challenge | None:
        """Gets a specific challenge by its ID from the cached ChallengeList.

        Args:
            challenge_id: The ID of the challenge to retrieve.

        Returns:
            The Challenge object if found, otherwise None.
        """
        challenge_list = self.get_cached_challenges()
        if challenge_list:
            return challenge_list.get_by_id(challenge_id)
        return None

    # --- Direct API Operations (Asynchronous) ---

    async def fetch_challenges(self, member_only: bool = False, page: int = 0) -> ChallengeList | None:
        """Fetches challenges directly from the API without relying on cache.

        Args:
            member_only: If True, only return challenges the user is a member of.
            page: Page number for paginated results.

        Returns:
            A ChallengeList containing the challenges or None if the API call failed.
        """
        log.info(f"Fetching challenges from API (member_only={member_only}, page={page})...")
        try:
            challenges_data = await self.api.get_challenges(member_only=member_only, page=page)
            if not challenges_data:
                log.warning("API returned no challenge data.")
                return None

            # Create validation context for the challenge list
            user_id_context = self.dm.user.id if self.dm.user else None
            validation_context = {"current_user_id": user_id_context}

            # Create challenge list from API data
            challenge_list = ChallengeList.from_raw_data(challenges_data, context=validation_context)
            for challenge in challenge_list:
                if challenge.id in self.dm.user.challenges:
                    challenge.joined = True
                else:
                    challenge.joined = False
            log.info(f"Successfully fetched {len(challenge_list.challenges) if challenge_list else 0} challenges from API.")
            return challenge_list
        except Exception as e:
            log.exception(f"Failed to fetch challenges from API: {e}")
            return None

    async def fetch_challenge_details(self, challenge_id: str) -> Challenge | None:
        """Fetches detailed information about a specific challenge from the API.

        Args:
            challenge_id: The ID of the challenge to fetch.

        Returns:
            The Challenge object with full details if successful, None otherwise.
        """
        log.info(f"Fetching challenge details for '{challenge_id}' from API...")
        try:
            challenge_data = await self.api.get_challenge(challenge_id=challenge_id)
            if not challenge_data:
                log.warning(f"API returned no data for challenge '{challenge_id}'.")
                return None

            # Create validation context
            user_id_context = self.dm.user.id if self.dm.user else None
            validation_context = {"current_user_id": user_id_context}

            # Create a temporary list to parse the challenge
            temp_list = ChallengeList.from_raw_data([challenge_data], context=validation_context)
            challenge = temp_list.challenges[0] if temp_list and temp_list.challenges else None

            if challenge:
                log.info(f"Successfully fetched challenge details for '{challenge_id}'.")
                return challenge
            else:
                log.error(f"Failed to parse challenge data for '{challenge_id}'.")
                return None
        except Exception as e:
            log.exception(f"Failed to fetch challenge details for '{challenge_id}': {e}")
            return None

    async def fetch_challenge_tasks(self, challenge_id: str) -> List[Task] | None:
        """Fetches tasks belonging to a specific challenge from the API.

        Args:
            challenge_id: The ID of the challenge to fetch tasks for.

        Returns:
            A list of Task objects if successful, None otherwise.
        """
        log.info(f"Fetching tasks for challenge '{challenge_id}' from API...")
        try:
            tasks_data = await self.api.get_challenge_tasks(challenge_id=challenge_id)
            if not tasks_data:
                log.warning(f"API returned no tasks for challenge '{challenge_id}'.")
                return None

            # We need to parse raw task data into Task objects
            task_list = TaskList.from_raw_api_list(tasks_data)
            if not task_list:
                log.error("Task list not available in DataManager.")
                return None

            # Create Task objects from raw data
            # tasks = []
            # for task_data in tasks_data:
            #     task = task_list.add_task(task_data, update_existing=True)
            #     if task:
            #         tasks.append(task)

            # log.info(f"Successfully fetched {len(tasks)} tasks for challenge '{challenge_id}'.")
            return task_list
        except Exception as e:
            log.exception(f"Failed to fetch tasks for challenge '{challenge_id}': {e}")
            return None

    # --- Write Operations (Asynchronous) ---

    async def join_challenge(self, challenge_id: str) -> bool:
        """Joins a challenge via API and updates the local cache.

        Args:
            challenge_id: The ID of the challenge to join.

        Returns:
            True if successful, False otherwise.

        Raises:
            ValueError: If challenge_id is invalid.
            HabiticaAPIError: If the API call fails.
        """
        log.info(f"Attempting to join challenge '{challenge_id}'...")
        try:
            # 1. Call API
            api_success = await self.api.join_challenge(challenge_id=challenge_id)
            if not api_success:
                log.error(f"API call to join challenge '{challenge_id}' failed.")
                return False

            # 2. Update local cache if challenge exists
            challenge_list = self.get_cached_challenges()
            if challenge_list:
                challenge = challenge_list.get_by_id(challenge_id)
                if challenge:
                    challenge.joined = True
                    log.info(f"Updated local cache: challenge '{challenge_id}' is now joined.")
                else:
                    log.info(f"Challenge '{challenge_id}' not found in local cache. Will be updated on next fetch.")

            # 3. Fetch and merge challenge tasks into user tasks
            tasks = await self.fetch_challenge_tasks(challenge_id)
            if tasks:
                log.info(f"Added {len(tasks)} tasks from challenge '{challenge_id}' to local task list.")

            return True
        except ValueError as ve:
            log.error(f"Input validation error joining challenge: {ve}")
            raise
        except Exception as e:
            log.exception(f"Failed to join challenge '{challenge_id}': {e}")
            raise

    async def leave_challenge(self, challenge_id: str, keep: str) -> bool:
        """Leaves a challenge via API and updates the local cache.

        Args:
            challenge_id: The ID of the challenge to leave.
            keep: Option whether to keep tasks ("keep-all" or "remove-all").

        Returns:
            True if successful, False otherwise.

        Raises:
            ValueError: If challenge not found.
            HabiticaAPIError: If the API call fails.
        """
        challenge_list = self.get_cached_challenges()
        if not challenge_list:
            log.warning("Challenge list not available, proceeding with API call only.")
        else:
            existing_challenge = challenge_list.get_by_id(challenge_id)
            if not existing_challenge:
                log.warning(f"Challenge with ID '{challenge_id}' not found in cache.")

        log.info(f"Attempting to leave challenge '{challenge_id}' (keep={keep})...")
        try:
            # 1. Call API
            api_success = await self.api.leave_challenge(challenge_id=challenge_id, keep=keep)
            if not api_success:
                log.error(f"API call to leave challenge '{challenge_id}' failed.")
                return False

            # 2. Update local cache if challenge exists
            if challenge_list:
                existing_challenge = challenge_list.get_by_id(challenge_id)
                if existing_challenge:
                    existing_challenge.joined = False
                    log.info(f"Updated local cache: challenge '{challenge_id}' is now left.")

            # 3. Remove challenge tasks from task list if keep is "remove-all"
            if keep == "remove-all" and self.dm.tasks:
                # This would require a method to identify and remove challenge tasks
                # Consider adding this functionality to the TaskList class
                log.info(f"Tasks from challenge '{challenge_id}' should be removed from local task list.")

            return True
        except ValueError as ve:
            log.error(f"Input validation error leaving challenge: {ve}")
            raise
        except Exception as e:
            log.exception(f"Failed to leave challenge '{challenge_id}': {e}")
            raise

    async def create_challenge(self, data: dict[str, Any]) -> Challenge | None:
        """Creates a new challenge via the API and adds it to the local ChallengeList.

        Args:
            data: Dictionary containing challenge data (requires 'name', 'shortName', 'group').

        Returns:
            The created Challenge object, or None if creation failed.

        Raises:
            ValueError: If required fields are missing.
            HabiticaAPIError: If the API call fails.
        """
        # Basic validation - API mixin already does this, but good practice here too
        if not data.get("name") or not data.get("shortName") or not data.get("group"):
            log.error("Challenge creation failed: Missing required fields (name, shortName, group).")
            raise ValueError("Challenge creation requires 'name', 'shortName', and 'group' ID.")

        log.info(f"Attempting to create challenge '{data.get('name')}'...")
        try:
            # 1. Call API
            challenge_data = await self.api.create_challenge(data=data)
            if not challenge_data:
                log.error("API call to create challenge did not return data.")
                return None

            # 2. Add to local cache
            challenge_list = self.get_cached_challenges()
            if challenge_list:
                # Create Challenge instance with context
                user_id_context = self.dm.user.id if self.dm.user else None
                context = {"current_user_id": user_id_context}
                temp_list = ChallengeList.from_raw_data([challenge_data], context=context)
                new_challenge = temp_list.challenges[0] if temp_list.challenges else None

                if new_challenge:
                    challenge_list.add_challenge(new_challenge)  # Assumes implementation of add_challenge
                    log.info(f"Successfully created and cached challenge: {new_challenge}")
                    return new_challenge
                else:
                    log.error("Failed to validate/create Challenge object from API response.")
                    return None
            else:
                log.warning("Cannot add created challenge to cache: ChallengeList not loaded.")
                # Still return the created challenge even if we couldn't cache it
                return Challenge.model_validate(challenge_data)

        except ValueError as ve:
            log.error(f"Input validation error creating challenge: {ve}")
            raise
        except Exception as e:
            log.exception(f"Failed to create challenge: {e}")
            raise

    async def update_challenge(self, challenge_id: str, data: dict[str, Any]) -> Challenge | None:
        """Updates an existing challenge via the API and updates the local cache.

        Args:
            challenge_id: The ID of the challenge to update.
            data: Dictionary containing the fields to update.

        Returns:
            The updated Challenge object, or None if update failed.

        Raises:
            ValueError: If challenge not found or data is empty.
            HabiticaAPIError: If the API call fails.
        """
        if not data:
            raise ValueError("Update data cannot be empty.")

        challenge_list = self.get_cached_challenges()
        existing_challenge = None
        if challenge_list:
            existing_challenge = challenge_list.get_by_id(challenge_id)
            if not existing_challenge:
                log.warning(f"Challenge with ID '{challenge_id}' not found in local cache.")

        log.info(f"Attempting to update challenge '{challenge_id}'...")
        try:
            # 1. Call API
            updated_data = await self.api.update_challenge(challenge_id=challenge_id, data=data)
            if not updated_data:
                log.error(f"API call to update challenge '{challenge_id}' did not return data.")
                return None

            # 2. Update local cache if challenge exists
            if existing_challenge:
                existing_challenge.model_validate(updated_data, update=True)
                log.info(f"Successfully updated cached challenge: {existing_challenge}")
                return existing_challenge
            else:
                # Create a new Challenge object with the updated data
                user_id_context = self.dm.user.id if self.dm.user else None
                context = {"current_user_id": user_id_context}
                temp_list = ChallengeList.from_raw_data([updated_data], context=context)
                new_challenge = temp_list.challenges[0] if temp_list.challenges else None

                if new_challenge and challenge_list:
                    challenge_list.add_challenge(new_challenge)  # Assumes implementation of add_challenge
                    log.info(f"Added updated challenge to cache: {new_challenge}")

                return new_challenge

        except ValueError as ve:
            log.error(f"Input validation error updating challenge: {ve}")
            raise
        except Exception as e:
            log.exception(f"Failed to update challenge '{challenge_id}': {e}")
            raise

    async def create_challenge_task(self, challenge_id: str, task_data: dict[str, Any]) -> Task | None:
        """Creates a task within a challenge via API and updates local caches.

        Args:
            challenge_id: ID of the challenge to add the task to.
            task_data: Dictionary containing task data (requires 'text', 'type').

        Returns:
            The created Task object, or None if failed.

        Raises:
            ValueError: If input data is invalid or challenge not found.
            HabiticaAPIError: If the API call fails.
        """
        # Validate inputs (API mixin also validates)
        if not task_data.get("text") or not task_data.get("type"):
            raise ValueError("Task data requires 'text' and 'type'.")
        if task_data["type"] not in {"habit", "daily", "todo", "reward"}:
            raise ValueError("Invalid task type.")

        challenge_list = self.get_cached_challenges()
        task_list = self.dm.tasks

        if not task_list:
            raise ValueError("Task list not available.")

        challenge = None
        if challenge_list:
            challenge = challenge_list.get_by_id(challenge_id)
            if not challenge:
                log.warning(f"Challenge with ID '{challenge_id}' not found in local cache.")

        log.info(f"Attempting to create task in challenge '{challenge_id}'...")
        try:
            # 1. Call API
            new_task_data = await self.api.create_challenge_task(challenge_id=challenge_id, task_data=task_data)
            if not new_task_data:
                log.error("API call to create challenge task returned no data.")
                return None

            # 2. Add task to main TaskList cache
            new_task_instance = task_list.add_task(new_task_data)
            if not new_task_instance:
                log.error("Failed to add created challenge task to local TaskList.")
                # API created task, but local cache failed. Inconsistent.
                return None

            # 3. Add task reference to Challenge's task list if challenge exists in cache
            if challenge:
                # Ensure challenge tasks are initialized
                if not hasattr(challenge, "tasks") or not challenge.tasks:
                    challenge.tasks = []
                # Avoid duplicates if already added via some other means
                if new_task_instance.id not in [task.id for task in challenge.tasks]:
                    challenge.tasks.append(new_task_instance)

            log.info(f"Successfully created task '{new_task_instance.id}' in challenge '{challenge_id}' and cached.")
            return new_task_instance

        except ValueError as ve:
            log.error(f"Input validation error creating challenge task: {ve}")
            raise
        except Exception as e:
            log.exception(f"Failed to create challenge task for challenge '{challenge_id}': {e}")
            raise

    # --- Helper Methods ---

    async def refresh_challenges(self, member_only: bool = False) -> ChallengeList | None:
        """Refreshes the locally cached ChallengeList from the API.

        Args:
            member_only: If True, only fetch challenges the user is a member of.

        Returns:
            The updated ChallengeList or None if the API call failed.
        """
        log.info(f"Refreshing challenges (member_only={member_only})...")
        try:
            # Fetch challenges from API
            challenges_data = await self.api.get_challenges(member_only=member_only)
            if not challenges_data:
                log.warning("API returned no challenge data.")
                return None

            # Create validation context
            user_id_context = self.dm.user.id if self.dm.user else None
            validation_context = {"current_user_id": user_id_context}

            # Create challenge list from API data
            challenge_list = ChallengeList.from_raw_data(challenges_data, context=validation_context)

            # Update data manager's challenge list
            self.dm.challenges = challenge_list
            log.info(f"Successfully refreshed {len(challenge_list.challenges) if challenge_list else 0} challenges in cache.")

            return challenge_list
        except Exception as e:
            log.exception(f"Failed to refresh challenges: {e}")
            return None
-e 
-------- END OF FILE services/challenge_service.py --------

-------- START OF FILE services/data_manager.py --------
# pixabit/services/data_manager.py

# ─── Title ────────────────────────────────────────────────────────────────────
#          Habitica Data Orchestration Manager (Live Data Focus)
# ──────────────────────────────────────────────────────────────────────────────

# SECTION: MODULE DOCSTRING
"""Provides a DataManager class for loading, caching (live data as files), processing,
and accessing current Habitica Pydantic models (User, Tasks, Tags, Party, Challenges).
Coordinates API client, static content manager, and models. Persistent archiving
is handled by a separate Archiver class.
"""

# SECTION: IMPORTS
from __future__ import annotations

import asyncio
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Any, Coroutine, Dict, Type

from pydantic import ValidationError

from pixabit.api.client import HabiticaClient
from pixabit.config import DEFAULT_CACHE_DURATION_DAYS, HABITICA_DATA_PATH, USER_ID
from pixabit.helpers._json import load_json, load_pydantic_model, save_json, save_pydantic_model
from pixabit.helpers._logger import log
from pixabit.helpers.DateTimeHandler import DateTimeHandler
from pixabit.models.challenge import Challenge, ChallengeList
from pixabit.models.game_content import Gear, Quest, StaticContentManager
from pixabit.models.party import Party
from pixabit.models.tag import Tag, TagList
from pixabit.models.task import AnyTask, Task, TaskList  # Need Task for user calc type hints if used
from pixabit.models.user import User

# SECTION: CONSTANTS & CONFIG

DEFAULT_LIVE_CACHE_TIMEOUT = timedelta(minutes=5)
DEFAULT_CHALLENGE_CACHE_TIMEOUT = timedelta(hours=2)
CACHE_SUBDIR_RAW = "raw"
CACHE_SUBDIR_PROCESSED = "processed"


# SECTION: DATA MANAGER CLASS
class DataManager:
    """Centralized manager for fetching, caching, processing, and accessing
    Habitica data models. Orchestrates API interaction and model processing.
    """

    def __init__(
        self,
        api_client: HabiticaClient,
        static_content_manager: StaticContentManager,
        cache_dir: Path = HABITICA_DATA_PATH,
        live_cache_timeout: timedelta = DEFAULT_LIVE_CACHE_TIMEOUT,
        challenge_cache_timeout: timedelta = DEFAULT_CHALLENGE_CACHE_TIMEOUT,
    ):
        """Initializes the DataManager.

        Args:
            api_client: An instance of HabiticaClient.
            static_content_manager: An instance of StaticContentManager.
            cache_dir: Base directory for caching data.
            live_cache_timeout: Duration for which cached live data is considered fresh.
        """
        self.api = api_client
        self.static_content_manager = static_content_manager
        self.cache_dir = cache_dir
        self.live_cache_timeout = live_cache_timeout
        self.challenge_cache_timeout = challenge_cache_timeout

        # Standard Cache Dirs
        self.raw_cache_dir = self.cache_dir / CACHE_SUBDIR_RAW
        self.processed_cache_dir = self.cache_dir / CACHE_SUBDIR_PROCESSED
        self.raw_cache_dir.mkdir(parents=True, exist_ok=True)
        self.processed_cache_dir.mkdir(parents=True, exist_ok=True)

        # Internal storage for LIVE data models
        self._user: User | None = None
        self._tasks: TaskList | None = None
        self._tags: TagList | None = None
        self._party: Party | None = None
        self._challenges: ChallengeList | None = None

        self._last_refresh_times: dict[str, datetime | None] = {
            "user": None,
            "tasks": None,
            "tags": None,
            "party": None,
            "challenges": None,
        }
        log.info(f"DataManager initialized. Cache Dir: {self.cache_dir}")

    # --- Cache Helper ---
    def _is_live_cache_stale(self, data_key: str) -> bool:
        """Checks if the LIVE cache for a specific data key is stale."""
        last_refresh = self._last_refresh_times.get(data_key)
        if last_refresh is None:
            return True
        timeout = self.challenge_cache_timeout if data_key == "challenges" else self.live_cache_timeout
        now_utc = datetime.now(timezone.utc)
        if last_refresh.tzinfo is None:
            last_refresh = last_refresh.replace(tzinfo=timezone.utc)
        is_stale = (now_utc - last_refresh) > timeout
        log.debug(f"Live cache check for '{data_key}': Stale={is_stale}")
        return is_stale

    def _get_cache_path(self, filename: str, processed: bool) -> Path:
        """Gets the full path for a cached file."""
        dir_path = self.processed_cache_dir if processed else self.raw_cache_dir
        return dir_path / filename

    def _update_refresh_time(self, data_key: str):
        """Updates the last refresh time for a given key."""
        self._last_refresh_times[data_key] = datetime.now(timezone.utc)

    # --- Properties for Accessing Data ---
    # Provides controlled access to the managed models

    @property
    def user(self) -> User | None:
        """Returns the loaded User object."""
        return self._user

    @property
    def tasks(self) -> TaskList | None:
        """Returns the loaded TaskList object."""
        return self._tasks

    @property
    def tags(self) -> TagList | None:  # Or AdvancedTagList
        """Returns the loaded TagList object."""
        return self._tags

    @property
    def party(self) -> Party | None:
        """Returns the loaded Party object."""
        return self._party

    @property
    def challenges(self) -> ChallengeList | None:
        return self._challenges

    # --- Static data access (unchanged) ---
    @property
    def static_gear_data(self) -> dict[str, Gear] | None:
        """Returns cached static gear data directly from the StaticContentManager (if loaded)."""
        # Note: This doesn't trigger loading, assumes load_all_data or equivalent called first.
        if self.static_content_manager._content:
            return self.static_content_manager._content.gear
        return None

    @property
    def static_quest_data(self) -> dict[str, Quest] | None:
        """Returns cached static quest data directly from the StaticContentManager (if loaded)."""
        if self.static_content_manager._content:
            return self.static_content_manager._content.quests
        return None

    # --- Loading Methods for LIVE Data ---

    async def load_user(self, force_refresh: bool = False) -> User | None:
        """Loads User data from cache or API."""
        data_key = "user"
        filename = "user.json"
        model_class = User

        if not force_refresh and self._user and not self._is_live_cache_stale(data_key):
            log.debug("Using in-memory user data.")
            return self._user

        # Try loading from processed cache first
        processed_path = self._get_cache_path(filename, processed=True)
        if not force_refresh and processed_path.exists():
            # Check modification time against timeout for file cache
            mtime = datetime.fromtimestamp(processed_path.stat().st_mtime, timezone.utc)
            if (datetime.now(timezone.utc) - mtime) <= self.live_cache_timeout:
                log.debug(f"Attempting to load user from processed cache: {processed_path}")
                cached_model = load_pydantic_model(model_class, processed_path)
                if cached_model:
                    self._user = cached_model
                    self._update_refresh_time(data_key)  # Update timestamp based on cache load
                    log.info("User loaded from fresh processed cache.")
                    return self._user
                else:
                    log.warning(f"Failed to load user model from presumably fresh cache file: {processed_path}")
            else:
                log.info("User processed cache file is stale.")

        # Fetch from API
        log.info(f"{'Forcing refresh for' if force_refresh else 'Fetching'} user data from API...")
        try:
            raw_data = await self.api.get_user_data()
            if not raw_data:
                log.error("Received empty data from user API endpoint.")
                # Should we return stale cache if available? Or None? Return None for now.
                self._user = None  # Clear potentially stale user
                return None

            # Validate raw data into model
            self._user = model_class.model_validate(raw_data)
            self._update_refresh_time(data_key)

            # Save raw and processed data
            save_json(raw_data, filename, folder=self.raw_cache_dir)
            save_pydantic_model(self._user, filename, folder=self.processed_cache_dir)
            log.success("User data fetched and processed.")
            return self._user

        except Exception as e:
            log.exception("Failed to fetch or process user data from API.")
            # Fallback to potentially stale in-memory data? Or clear? Clear is safer.
            self._user = None
            return None

    async def load_tasks(self, force_refresh: bool = False) -> TaskList | None:
        """Loads TaskList from cache or API."""
        data_key = "tasks"
        filename = "tasks.json"
        # Note: TaskList is a class, not a Pydantic model itself for saving/loading directly.
        # We cache the *list* of task dictionaries after TaskList validation.

        if not force_refresh and self._tasks and not self._is_live_cache_stale(data_key):
            log.debug("Using in-memory tasks data.")
            return self._tasks

        processed_path = self._get_cache_path(filename, processed=True)
        if not force_refresh and processed_path.exists():
            mtime = datetime.fromtimestamp(processed_path.stat().st_mtime, timezone.utc)
            if (datetime.now(timezone.utc) - mtime) <= self.live_cache_timeout:
                log.debug(f"Attempting to load tasks from processed cache: {processed_path}")
                # Load the *list* of task dicts from cache
                cached_task_dicts = load_json(processed_path)
                if isinstance(cached_task_dicts, list):
                    try:
                        # --- CHANGE HERE: Use from_processed_dicts ---
                        self._tasks = TaskList.from_processed_dicts(cached_task_dicts)

                        self._update_refresh_time(data_key)
                        log.success("Tasks loaded successfully from fresh processed cache.")
                        return self._tasks
                    except Exception as e:
                        log.exception(f"Error re-creating TaskList from cached data: {e}")
                else:
                    log.warning(f"Invalid data format in tasks cache file: {processed_path}. Expected list.")
            else:
                log.info("Tasks processed cache file is stale.")

        log.info(f"{'Forcing refresh for' if force_refresh else 'Fetching'} tasks data from API...")
        try:
            raw_data = await self.api.get_tasks()  # Returns list[dict]
            if not isinstance(raw_data, list):
                log.error(f"Received unexpected data type from tasks API: {type(raw_data)}")
                self._tasks = None
                return None
            self._tasks = TaskList.from_raw_api_list(raw_data)
            self._update_refresh_time(data_key)
            save_json(raw_data, filename, folder=self.raw_cache_dir)
            # Save the list of *processed* task dictionaries
            self._tasks.save_to_json(filename, folder=self.processed_cache_dir)
            log.success("Tasks data fetched and processed.")
            return self._tasks

        except Exception as e:
            log.exception("Failed to fetch or process tasks data from API.")
            self._tasks = None
            return None

    async def load_tags(self, force_refresh: bool = False) -> TagList | None:
        """Loads TagList from cache or API."""
        data_key = "tags"
        filename = "tags.json"
        model_class = TagList  # Simple TagList BaseModel

        if not force_refresh and self._tags and not self._is_live_cache_stale(data_key):
            log.debug("Using in-memory tags data.")
            return self._tags

        processed_path = self._get_cache_path(filename, processed=True)
        if not force_refresh and processed_path.exists():
            mtime = datetime.fromtimestamp(processed_path.stat().st_mtime, timezone.utc)
            if (datetime.now(timezone.utc) - mtime) <= self.live_cache_timeout:
                log.debug(f"Attempting to load tags from processed cache: {processed_path}")
                cached_model = load_pydantic_model(model_class, processed_path)
                if cached_model:
                    self._tags = cached_model
                    self._update_refresh_time(data_key)
                    log.info("Tags loaded from fresh processed cache.")
                    return self._tags
                else:
                    log.warning(f"Failed to load tags model from cache: {processed_path}")
            else:
                log.info("Tags processed cache file is stale.")

        log.info(f"{'Forcing refresh for' if force_refresh else 'Fetching'} tags data from API...")
        try:
            raw_data = await self.api.get_tags()  # Returns list[dict]
            if not isinstance(raw_data, list):
                log.error(f"Received unexpected data type from tags API: {type(raw_data)}")
                self._tags = None
                return None

            # Validate raw data using TagList's factory method
            self._tags = model_class.from_raw_data(raw_data)
            self._update_refresh_time(data_key)

            save_json(raw_data, filename, folder=self.raw_cache_dir)
            # Save the processed TagList model
            save_pydantic_model(self._tags, filename, folder=self.processed_cache_dir)
            log.success("Tags data fetched and processed.")
            return self._tags

        except Exception as e:
            log.exception("Failed to fetch or process tags data from API.")
            self._tags = None
            return None

    async def load_party(self, force_refresh: bool = False) -> Party | None:
        """Loads Party data from cache or API."""
        data_key = "party"
        filename = "party.json"
        model_class = Party

        # Party data includes chat, needs user context for validation
        # Get current USER_ID from config - assumes it's correctly set
        user_id_context = USER_ID
        if not user_id_context:
            log.error("USER_ID not configured. Cannot accurately process party chat messages.")
            # Decide whether to proceed without context or fail. Proceeding with warning.

        # Pass context to load_pydantic_model if needed
        validation_context = {"current_user_id": user_id_context}
        #        self._challenges = ChallengeList.from_raw_data(all_raw_challenges, context=validation_context)  # <<< Pass context

        if not force_refresh and self._party and not self._is_live_cache_stale(data_key):
            log.debug("Using in-memory party data.")
            return self._party

        processed_path = self._get_cache_path(filename, processed=True)
        if not force_refresh and processed_path.exists():
            mtime = datetime.fromtimestamp(processed_path.stat().st_mtime, timezone.utc)
            if (datetime.now(timezone.utc) - mtime) <= self.live_cache_timeout:
                log.debug(f"Attempting to load party from processed cache: {processed_path}")
                cached_model = load_pydantic_model(model_class, processed_path, context=validation_context)
                if cached_model:
                    self._party = cached_model
                    self._update_refresh_time(data_key)
                    log.info("Party loaded from fresh processed cache.")
                    return self._party
                else:
                    log.warning(f"Failed to load party model from cache: {processed_path}")
            else:
                log.info("Party processed cache file is stale.")

        log.info(f"{'Forcing refresh for' if force_refresh else 'Fetching'} party data from API...")
        try:
            raw_data = await self.api.get_party_data()  # Returns dict or None
            if not isinstance(raw_data, dict):
                # Handle API returning null/empty if not in party
                log.info("User is likely not in a party (API returned non-dict).")
                self._party = None  # Ensure party is None
                # Cache this None state? Maybe cache an empty dict raw/processed? Cache None for now.
                self._update_refresh_time(data_key)  # Update timestamp even for None result
                return None

            # Create Party model using factory method which handles context
            self._party = model_class.create_from_raw_data(raw_data, current_user_id=user_id_context)
            self._update_refresh_time(data_key)

            save_json(raw_data, filename, folder=self.raw_cache_dir)
            # Save the Party model (chat excluded by default based on field def)
            save_pydantic_model(self._party, filename, folder=self.processed_cache_dir)
            log.success("Party data fetched and processed.")
            return self._party

        except Exception as e:
            log.exception("Failed to fetch or process party data from API.")
            self._party = None  # Clear party on error
            return None

    # --- Modified load_challenges() ---
    async def load_challenges(self, force_refresh: bool = False) -> ChallengeList | None:
        """Loads LIVE user challenges list, saves raw/processed, handling pagination and caching."""
        data_key = "challenges"
        live_filename = "challenges.json"
        model_class = ChallengeList

        if not force_refresh and self._challenges and not self._is_live_cache_stale(data_key):
            log.debug("Using in-memory challenges data.")  # Added log
            return self._challenges

        live_processed_path = self._get_cache_path(live_filename, processed=True)
        # --- Determine User ID for context ---
        # Use loaded user if available, otherwise fallback to config
        user_id_context = self._user.id if self._user else USER_ID
        if not user_id_context:
            log.warning("Cannot determine current user ID for challenge context (ownership/joining). Proceeding without.")
        validation_context = {"current_user_id": user_id_context}
        # --- End User ID determination ---

        # Try loading from processed cache
        if not force_refresh and live_processed_path.exists():
            mtime = datetime.fromtimestamp(live_processed_path.stat().st_mtime, timezone.utc)
            timeout = self.challenge_cache_timeout  # Use specific timeout for challenges
            if (datetime.now(timezone.utc) - mtime) <= timeout:
                log.debug(f"Attempting to load challenges from processed cache: {live_processed_path}")
                # Pass context when loading from cache too, for re-validation consistency
                cached_model = load_pydantic_model(model_class, live_processed_path, context=validation_context)
                if cached_model:
                    self._challenges = cached_model
                    self._update_refresh_time(data_key)  # Update based on cache load time
                    log.info(f"Live challenges loaded from fresh processed cache ({len(self._challenges)} challenges).")
                    # --- Re-link tasks if loading from cache? ---
                    # If tasks are also loaded, linking might be needed again unless the cached
                    # challenges *already include* the tasks persistently. Assuming save_pydantic_model saves nested tasks.
                    # Let's rely on process_loaded_data to handle linking after all data is loaded.
                    return self._challenges
                else:
                    log.warning(f"Failed loading challenges model from cache file: {live_processed_path}")
            else:
                log.info("Live challenges processed cache stale.")

        # Fetch from API
        log.info(f"{'Forcing refresh for' if force_refresh else 'Fetching'} live challenges from API...")
        all_raw_challenges = []
        try:
            # Fetch all challenges (assuming pagination handled by the client method)
            fetched_challenges = await self.api.get_all_challenges_paginated()
            if not isinstance(fetched_challenges, list):
                log.error(f"Challenge API did not return a list. Received: {type(fetched_challenges)}")
                self._challenges = None  # Clear potentially stale data
                return None

            all_raw_challenges.extend(fetched_challenges)
            log.info(f"Fetched {len(all_raw_challenges)} raw challenges.")

            # --- >>> SAVE RAW DATA <<< ---
            raw_save_path = self._get_cache_path(live_filename, processed=False)
            log.debug(f"Saving raw challenges data to {raw_save_path}...")
            save_json(all_raw_challenges, live_filename, folder=self.raw_cache_dir)
            log.debug("Raw challenges data saved.")
            # --- >>> END SAVE RAW DATA <<< ---

            # Validate into ChallengeList model, passing context for ownership check etc.
            self._challenges = ChallengeList.from_raw_data(all_raw_challenges, context=validation_context)
            self._update_refresh_time(data_key)

            # Save the *initially* processed ChallengeList (without linked tasks yet)
            if self._challenges:
                log.debug(f"Saving initially processed challenges list to {live_processed_path}...")
                # Use save_pydantic_model which should handle the ChallengeList structure
                save_pydantic_model(self._challenges, live_filename, folder=self.processed_cache_dir)
                log.debug("Initially processed challenges saved.")
            else:
                log.warning("ChallengeList validation resulted in None or empty list, processed file not saved.")

            log.success(f"Live challenges fetched & processed ({len(self._challenges or [])} challenges).")
            return self._challenges

        except Exception as e:
            log.exception("Failed during challenge fetch, processing, or saving.")
            self._challenges = None  # Clear on error
            return None

    # --- Orchestration Methods ---

    async def load_all_data(self, force_refresh: bool = False) -> bool:
        """Loads all relevant data concurrently: User, Tasks, Tags, Party, and Static Content.
        Uses caching unless `force_refresh` is True for live data. Static content
        cache policy is managed by StaticContentManager (refreshed if needed).

        Args:
            force_refresh: If True, forces refresh for User, Tasks, Tags, Party.
                           StaticContentManager decides independently unless forced there too.
        """
        log.info(f"Initiating load_all_data (force_refresh={force_refresh})...")
        static_content_task: Coroutine = self.static_content_manager.load_content()

        # Define live data tasks *after* ensuring static is scheduled
        live_data_tasks: list[Coroutine] = [
            self.load_user(force_refresh=force_refresh),
            self.load_tasks(force_refresh=force_refresh),
            self.load_tags(force_refresh=force_refresh),
            self.load_party(force_refresh=force_refresh),
            self.load_challenges(force_refresh=force_refresh),
        ]

        all_tasks: list[Coroutine] = [static_content_task] + live_data_tasks

        # Run all loading tasks concurrently
        results = await asyncio.gather(*all_tasks, return_exceptions=True)

        # Log any errors encountered during loading
        success = True
        task_map = ["StaticContent", "User", "Tasks", "Tags", "Party", "Challenges"]  # Match new order
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                # Identify which task failed based on order (requires tasks list order to be consistent)
                failed_task_name = task_map[i] if i < len(task_map) else f"UnknownTask_{i}"
                log.error(f"Error loading {failed_task_name}: {result}")
                success = False

        # Models (_user, _tasks, etc.) are populated by the load methods.
        # Static content is loaded within static_content_manager.
        log.info("load_all_data finished.")
        return success

    async def process_loaded_data(self) -> bool:
        """Orchestrates post-loading processing of models. Requires data to be loaded first.

        Processes:
        1. User effective stats.
        2. Challenge joined/owned status.
        3. Task statuses, tag names, and Daily damage.
        4. Links Tasks to Challenges. <<< Added explicit save after this
        5. Party static quest details.

        Returns:
            True if processing was successful, False otherwise.
        """
        log.info("Initiating process_loaded_data...")
        # Add Challenges to required check
        required = {"User": self._user, "Tasks": self._tasks, "Tags": self._tags, "Challenges": self._challenges}
        missing = [k for k, v in required.items() if v is None]
        static_loaded = self.static_content_manager._content is not None

        # Handle optional Party dependency more gracefully
        optional = {"Party": self._party}

        if missing or not static_loaded:
            log.error(f"Cannot process - Missing Required: {', '.join(missing)}{', Static Content' if not static_loaded else ''}")
            return False

        log.debug(
            f"Data available for processing: User={self._user is not None}, Tasks={self._tasks is not None}, Tags={self._tags is not None}, Challenges={self._challenges is not None}, Party={self._party is not None}, Static={static_loaded}"
        )

        success = True
        try:
            # --- Processing Steps ---

            # 1. Process User Stats (Needs Static Gear)
            if self._user:
                gear_data = self.static_gear_data or {}
                self._user.calculate_effective_stats(gear_data=gear_data)
                log.debug("User stats processed.")

            # 2. Process Challenges Status (Needs User - already checked above)
            # Note: Ownership is now handled during validation via context.
            # We only need to explicitly set `joined` here.
            if self._user and self._challenges:
                # Ensure profile.challenges exists and is a list
                user_challenges_list = getattr(self._user, "challenges", [])
                joined_ids = set(user_challenges_list if isinstance(user_challenges_list, list) else [])
                # joined_ids = set(self._user.profile.challenges) if self._user.profile else set()
                count_joined = 0
                for c in self._challenges.challenges:
                    is_joined = c.id in joined_ids
                    if c.joined != is_joined:  # Only log if changed
                        pass
                        # log.debug(f"Challenge '{c.id[:8]}' joined status -> {is_joined}")
                    c.joined = is_joined
                    if is_joined:
                        count_joined += 1
                # log.debug(f"Challenge joined status processed ({count_joined} marked as joined).")

            # 3. Process Tasks (Needs User, Tags, Static Content Manager)
            if self._tasks and self._user and self._tags:
                self._tasks.process_tasks(user=self._user, tags_provider=self._tags, content_manager=self.static_content_manager)
                log.debug("Tasks processed.")
            else:
                log.error("Skipping task processing: User, Tasks or Tags missing.")  # Should fail dep check

            # 4. Link Tasks to LIVE Challenges (Needs Challenges, Tasks)
            linked_count = 0
            if self._challenges and self._tasks:
                log.debug(f"Linking {len(self._tasks)} tasks to {len(self._challenges)} challenges...")
                linked_count = self._challenges.link_tasks(self._tasks)
                log.debug(f"Tasks linked to challenges ({linked_count} links made).")

                # --- >>> SAVE CHALLENGES AGAIN (WITH LINKED TASKS) <<< ---
                log.info("Saving fully processed challenges state (with linked tasks)...")
                try:
                    chal_filename = "challenges.json"
                    processed_chal_path = self._get_cache_path(chal_filename, processed=True)
                    # Overwrite the previously saved processed file
                    challenges_to_save = self._challenges.model_dump(
                        mode="json",  # Use 'json' mode for JSON-compatible types
                        exclude={
                            # Exclude from items within the 'challenges' list:
                            "challenges": {
                                "__all__": {  # Apply to all Challenge items in the list
                                    # Exclude from items within the 'tasks' list of each Challenge:
                                    "tasks": {"__all__": {"styled_text", "styled_notes"}}  # Adjust these field names based on your Task model
                                }
                            }
                        },
                    )
                    save_successful = save_json(challenges_to_save, chal_filename, folder=self.processed_cache_dir)
                    log.success(f"Saved processed challenges state to {processed_chal_path}")
                except Exception as e:
                    log.error(f"Failed saving processed challenges state: {e}")
                    # Don't mark overall processing as failed just for this save failure
                # --- >>> END SAVE CHALLENGES AGAIN <<< ---
            else:
                log.warning("Skipping task linking to challenges due to missing Challenges or Tasks.")

            # 5. Process Party (Needs Party, Static Content Manager)
            if self._party and self._party.quest and self._party.quest.key:
                if not self._party.static_quest_details:
                    log.debug("Fetching Party quest details...")
                    # This modifies the self._party object in place
                    await self._party.fetch_and_set_static_quest_details(self.static_content_manager)
                    log.debug("Party quest details processed.")
            elif self._party:
                log.debug("Party exists but no active quest to process.")
            # No else needed if self._party is None

        except Exception as e:
            log.exception("Error during main processing steps of process_loaded_data.")
            success = False

        # --- Optionally save processed TASKS again AFTER processing ---
        if success and self._tasks:
            log.info("Saving fully processed tasks state post-processing...")
            try:
                tasks_filename = "tasks.json"
                processed_tasks_path = self._get_cache_path(tasks_filename, processed=True)
                # TaskList has its own save method
                self._tasks.save_to_json(tasks_filename, folder=self.processed_cache_dir)
                log.success(f"Saved processed tasks state to {processed_tasks_path}")
            except Exception as e:
                log.error(f"Failed saving processed tasks state: {e}")

        log.info(f"process_loaded_data finished {'successfully' if success else 'with errors'}.")
        return success

    # Helper for sync gear access during processing
    def _get_static_gear_data_sync(self) -> dict[str, Gear]:
        content = self.static_content_manager._content
        if content and content.gear:
            return content.gear
        log.warning("Static gear data accessed sync but not pre-loaded.")
        return {}

    # No close method needed as no DB connection managed here


# ──────────────────────────────────────────────────────────────────────────────
-e 
-------- END OF FILE services/data_manager.py --------

-------- START OF FILE services/tag_service.py --------
# pixabit/services/tag_service.py

from __future__ import annotations

import asyncio
from typing import TYPE_CHECKING

from pixabit.helpers._logger import log
from pixabit.models.tag import Tag  # Asumiendo el Tag simple por ahora

if TYPE_CHECKING:
    from pixabit.api.client import HabiticaClient
    from pixabit.models.tag import TagList  # O tag_factory.TagList si usas el avanzado

    from .data_manager import DataManager


class TagService:
    """Service layer for interacting with Habitica Tags.
    Coordinates API calls and updates the local data managed by DataManager.
    """

    def __init__(self, api_client: HabiticaClient, data_manager: DataManager):
        """Initializes the TagService.

        Args:
            api_client: The Habitica API client instance.
            data_manager: The data manager instance holding live data models.
        """
        self.api = api_client
        self.dm = data_manager
        log.debug("TagService initialized.")

    # --- Read Operations (Synchronous - access cached data) ---

    def get_tags(self) -> TagList | None:
        """Returns the cached TagList instance from the DataManager."""
        # Ensure data is loaded before accessing? The TUI layer should ensure this.
        if not self.dm.tags:
            log.warning("Attempted to get tags, but TagList is not loaded in DataManager.")
        return self.dm.tags

    def get_tag_by_id(self, tag_id: str) -> Tag | None:
        """Gets a specific tag by its ID from the cached TagList."""
        tag_list = self.get_tags()
        if tag_list:
            return tag_list.get_by_id(tag_id)
        return None

    # --- Write Operations (Asynchronous - interact with API and cache) ---

    async def create_tag(self, name: str) -> Tag | None:
        """Creates a new tag via the API and adds it to the local TagList.

        Args:
            name: The name for the new tag.

        Returns:
            The created Tag object, or None if creation failed.

        Raises:
            ValueError: If the name is empty.
            HabiticaAPIError: If the API call fails.
        """
        name_stripped = name.strip()
        if not name_stripped:
            log.error("Tag creation failed: Name cannot be empty.")
            raise ValueError("Tag name cannot be empty.")

        log.info(f"Attempting to create tag with name: '{name_stripped}'")
        try:
            # 1. Call API
            # create_tag in mixin returns dict or None
            tag_data: dict[str, any] | None = await self.api.create_tag(name=name_stripped)

            if not tag_data:
                # This case might happen if API returns success but no data, or if the method returns None on failure
                log.error("API call to create tag did not return data.")
                # Consider raising an error or returning None based on expected API behavior
                return None  # Or raise HabiticaAPIError("Failed to create tag, no data returned")

            # 2. Validate API response into Tag model
            # Use TagList's factory if available and configured, otherwise simple Tag
            tag_list = self.get_tags()
            if tag_list and hasattr(tag_list, "factory") and tag_list.factory:
                new_tag = tag_list.factory.create_tag(tag_data)
            else:
                new_tag = Tag.model_validate(tag_data)  # Use simple Tag validation

            # 3. Add to local cache (DataManager's TagList)
            if tag_list:
                tag_list.add_tag(new_tag)  # Assumes TagList has add_tag method
                log.info(f"Successfully created and cached tag: {new_tag}")
                # Optionally trigger saving the updated TagList cache file via DataManager
                # self.dm.save_tags() # If such a method exists
                return new_tag
            else:
                log.error("Cannot add created tag: TagList not loaded in DataManager.")
                # Return the tag instance even if caching fails? Or None? Return instance for now.
                return new_tag

        except ValueError as ve:  # Catch specific input errors
            log.error(f"Input validation error creating tag: {ve}")
            raise  # Re-raise input validation errors
        except Exception as e:  # Catch API errors or other issues
            log.exception(f"Failed to create tag '{name_stripped}': {e}")
            # Re-raise the exception so the caller (TUI) knows it failed
            raise

    async def update_tag(self, tag_id: str, new_name: str) -> Tag | None:
        """Updates an existing tag's name via the API and updates the local cache.

        Args:
            tag_id: The ID of the tag to update.
            new_name: The new name for the tag.

        Returns:
            The updated Tag object, or None if update failed.

        Raises:
            ValueError: If the name is empty or tag not found/is challenge tag.
            HabiticaAPIError: If the API call fails.
        """
        name_stripped = new_name.strip()
        if not name_stripped:
            log.error("Tag update failed: New name cannot be empty.")
            raise ValueError("New tag name cannot be empty.")

        tag_list = self.get_tags()
        if not tag_list:
            log.error(f"Cannot update tag '{tag_id}': TagList not loaded.")
            raise ValueError("Tag list not available.")  # Or return None

        existing_tag = tag_list.get_by_id(tag_id)
        if not existing_tag:
            log.error(f"Tag update failed: Tag ID '{tag_id}' not found.")
            raise ValueError(f"Tag with ID '{tag_id}' not found.")
        if existing_tag.challenge:
            log.error(f"Tag update failed: Cannot update challenge tag '{tag_id}'.")
            raise ValueError("Cannot update challenge tags.")

        log.info(f"Attempting to update tag '{tag_id}' name to: '{name_stripped}'")
        try:
            # 1. Call API
            updated_data = await self.api.update_tag(tag_id=tag_id, name=name_stripped)

            if not updated_data:
                log.error(f"API call to update tag '{tag_id}' did not return data.")
                return None  # Or raise

            # 2. Update local cache
            # Use model_validate with update=True or manually set attributes
            existing_tag.model_validate(updated_data, update=True)
            log.info(f"Successfully updated and cached tag: {existing_tag}")
            # self.dm.save_tags() # Optional: Save updated cache
            return existing_tag

        except ValueError as ve:
            log.error(f"Input validation error updating tag: {ve}")
            raise
        except Exception as e:
            log.exception(f"Failed to update tag '{tag_id}': {e}")
            raise

    async def delete_tag(self, tag_id: str) -> bool:
        """Deletes a tag via the API and removes it from the local cache.

        Args:
            tag_id: The ID of the tag to delete.

        Returns:
            True if deletion was successful (both API and local), False otherwise.

        Raises:
            ValueError: If tag not found or is a challenge tag.
            HabiticaAPIError: If the API call fails.
        """
        tag_list = self.get_tags()
        if not tag_list:
            log.error(f"Cannot delete tag '{tag_id}': TagList not loaded.")
            raise ValueError("Tag list not available.")

        existing_tag = tag_list.get_by_id(tag_id)
        if not existing_tag:
            log.error(f"Tag deletion failed: Tag ID '{tag_id}' not found.")
            raise ValueError(f"Tag with ID '{tag_id}' not found.")
        if existing_tag.challenge:
            log.error(f"Tag deletion failed: Cannot delete challenge tag '{tag_id}'.")
            raise ValueError("Cannot delete challenge tags.")

        log.info(f"Attempting to delete tag '{tag_id}' ({existing_tag.name})...")
        try:
            # 1. Call API
            api_success = await self.api.delete_tag(tag_id=tag_id)

            if not api_success:
                # API call failed or returned False
                log.error(f"API call to delete tag '{tag_id}' failed.")
                return False  # Indicate failure

            # 2. Remove from local cache
            removed_local = tag_list.remove_tag(tag_id)  # Assumes TagList has remove_tag
            if removed_local:
                log.info(f"Successfully deleted tag '{tag_id}' from API and cache.")
                # self.dm.save_tags() # Optional: Save updated cache
                return True
            else:
                # This shouldn't happen if get_by_id worked, but log just in case
                log.warning(f"API deletion successful, but failed to remove tag '{tag_id}' from local cache.")
                return False  # Indicate partial success/failure

        except ValueError as ve:
            log.error(f"Input validation error deleting tag: {ve}")
            raise
        except Exception as e:
            log.exception(f"Failed to delete tag '{tag_id}': {e}")
            raise

    async def reorder_tag(self, tag_id: str, position: int) -> bool:
        """Reorders a tag via the API and updates the local cache order.

        Args:
            tag_id: The ID of the tag to move.
            position: The desired 0-based index.

        Returns:
            True if reordering was successful, False otherwise.

        Raises:
            ValueError: If tag not found or position is invalid.
            HabiticaAPIError: If the API call fails.
        """
        tag_list = self.get_tags()
        if not tag_list:
            log.error(f"Cannot reorder tag '{tag_id}': TagList not loaded.")
            raise ValueError("Tag list not available.")

        if not isinstance(position, int) or position < 0:
            log.error("Tag reorder failed: Position must be a non-negative integer.")
            raise ValueError("Position must be a non-negative integer index.")

        # Check if tag exists locally before calling API
        if tag_id not in tag_list:
            log.error(f"Tag reorder failed: Tag ID '{tag_id}' not found locally.")
            raise ValueError(f"Tag with ID '{tag_id}' not found.")

        log.info(f"Attempting to reorder tag '{tag_id}' to position {position}...")
        try:
            # 1. Call API first
            api_success = await self.api.reorder_tag(tag_id=tag_id, position=position)

            if not api_success:
                log.error(f"API call to reorder tag '{tag_id}' failed.")
                return False

            # 2. Reorder locally
            # Assumes TagList has a reorder_tags method that handles updating internal positions
            local_reorder_success = tag_list.reorder_tags(tag_id, position)

            if local_reorder_success:
                log.info(f"Successfully reordered tag '{tag_id}' in API and cache.")
                # self.dm.save_tags() # Optional: Save updated cache
                return True
            else:
                log.warning(f"API reorder successful, but failed to reorder tag '{tag_id}' locally.")
                # Maybe force a full refresh of tags here?
                # await self.dm.load_tags(force_refresh=True)
                return False  # Indicate local state might be inconsistent

        except ValueError as ve:
            log.error(f"Input validation error reordering tag: {ve}")
            raise
        except Exception as e:
            log.exception(f"Failed to reorder tag '{tag_id}': {e}")
            raise
-e 
-------- END OF FILE services/tag_service.py --------

-------- START OF FILE services/task_service.py --------
# pixabit/services/task_service.py

from __future__ import annotations

import asyncio
from typing import TYPE_CHECKING

from pixabit.helpers._logger import log
from pixabit.models.task import AnyTask, TaskData  # Import specific model if needed

if TYPE_CHECKING:
    from pixabit.api.client import HabiticaClient
    from pixabit.api.mixin.task_mixin import ScoreDirection  # Import Enum
    from pixabit.models.task import TaskList  # The container/manager model

    from .data_manager import DataManager


class TaskService:
    """Service layer for interacting with Habitica Tasks.
    Coordinates API calls and updates the local data managed by DataManager.
    """

    def __init__(self, api_client: HabiticaClient, data_manager: DataManager):
        """Initializes the TaskService.

        Args:
            api_client: The Habitica API client instance.
            data_manager: The data manager instance holding live data models.
        """
        self.api = api_client
        self.dm = data_manager
        log.debug("TaskService initialized.")

    # --- Read Operations (Synchronous - access cached data) ---

    def get_tasks(self) -> TaskList | None:
        """Returns the cached TaskList instance from the DataManager."""
        if not self.dm.tasks:
            log.warning("Attempted to get tasks, but TaskList is not loaded in DataManager.")
        return self.dm.tasks

    def get_task_by_id(self, task_id: str) -> AnyTask | None:
        """Gets a specific task by its ID from the cached TaskList."""
        task_list = self.get_tasks()
        if task_list:
            return task_list.get_by_id(task_id)
        return None

    def get_tasks_by_type(self, task_type: Literal[habit, daily, todo, reward]) -> list[AnyTask]:
        """Gets tasks of a specific type from the cached TaskList."""
        task_list = self.get_tasks()
        if task_list:
            # TaskList should provide direct access by type
            return task_list.get_tasks_by_type(task_type)
        return []

    # --- Write Operations (Asynchronous) ---

    async def create_task(self, task_input: TaskData | dict[str, Any]) -> AnyTask | None:
        """Creates a new task via the API and adds it to the local TaskList.

        Args:
            task_input: Task data (Pydantic model TaskData or dictionary).

        Returns:
            The created Task object (Habit, Daily, Todo, Reward), or None if failed.

        Raises:
            ValueError: If task_input is invalid.
            HabiticaAPIError: If the API call fails.
        """
        log.info("Attempting to create task...")
        try:
            # 1. Call API
            # create_task expects dict or TaskData model, handles validation
            task_data: dict[str, Any] | None = await self.api.create_task(task=task_input)

            if not task_data:
                log.error("API call to create task did not return data.")
                return None

            # 2. Add to local cache using TaskList's method
            task_list = self.get_tasks()
            if task_list:
                # add_task should validate the raw dict and create the correct subclass
                new_task_instance = task_list.add_task(task_data)
                if new_task_instance:
                    log.info(f"Successfully created and cached task: {new_task_instance}")
                    # self.dm.save_tasks() # Optional: Save updated cache
                    return new_task_instance
                else:
                    log.error("Failed to add created task to local TaskList.")
                    # Task created in API but not locally? Inconsistent state.
                    # Maybe return the raw dict? Or None? Let's return None for now.
                    return None
            else:
                log.error("Cannot add created task: TaskList not loaded in DataManager.")
                return None  # Local cache not available

        except ValueError as ve:
            log.error(f"Input validation error creating task: {ve}")
            raise
        except Exception as e:
            log.exception(f"Failed to create task: {e}")
            raise

    async def update_task(self, task_id: str, update_data: dict[str, Any]) -> AnyTask | None:
        """Updates an existing task via the API and updates the local cache.

        Args:
            task_id: The ID of the task to update.
            update_data: Dictionary containing fields to update.

        Returns:
            The updated Task object, or None if update failed.

        Raises:
            ValueError: If update_data is empty or task not found.
            HabiticaAPIError: If the API call fails.
        """
        if not update_data:
            log.error("Task update failed: update_data cannot be empty.")
            raise ValueError("Update data cannot be empty.")

        task_list = self.get_tasks()
        if not task_list or task_id not in task_list:
            log.error(f"Cannot update task '{task_id}': Task not found or TaskList not loaded.")
            raise ValueError(f"Task with ID '{task_id}' not found locally.")

        log.info(f"Attempting to update task '{task_id}'...")
        try:
            # 1. Call API
            updated_data_from_api = await self.api.update_task(task_id=task_id, data=update_data)

            if not updated_data_from_api:
                log.error(f"API call to update task '{task_id}' did not return data.")
                return None

            # 2. Update local cache using TaskList's method
            # edit_task should handle validation and updating the existing instance
            updated_task_instance = task_list.edit_task(task_id, updated_data_from_api)

            if updated_task_instance:
                log.info(f"Successfully updated and cached task: {updated_task_instance}")
                # self.dm.save_tasks() # Optional
                return updated_task_instance
            else:
                log.error(f"Failed to update task '{task_id}' in local TaskList.")
                return None

        except ValueError as ve:
            log.error(f"Input validation error updating task: {ve}")
            raise
        except Exception as e:
            log.exception(f"Failed to update task '{task_id}': {e}")
            raise

    async def delete_task(self, task_id: str) -> bool:
        """Deletes a task via the API and removes it from the local cache.

        Args:
            task_id: The ID of the task to delete.

        Returns:
            True if deletion was successful (both API and local), False otherwise.

        Raises:
            ValueError: If task not found.
            HabiticaAPIError: If the API call fails.
        """
        task_list = self.get_tasks()
        if not task_list or task_id not in task_list:
            log.error(f"Cannot delete task '{task_id}': Task not found or TaskList not loaded.")
            raise ValueError(f"Task with ID '{task_id}' not found locally.")

        log.info(f"Attempting to delete task '{task_id}'...")
        try:
            # 1. Call API
            api_success = await self.api.delete_task(task_id=task_id)

            if not api_success:
                log.error(f"API call to delete task '{task_id}' failed.")
                return False

            # 2. Remove from local cache using TaskList's method
            deleted_task = task_list.delete_task(task_id)

            if deleted_task:
                log.info(f"Successfully deleted task '{task_id}' from API and cache.")
                # self.dm.save_tasks() # Optional
                return True
            else:
                log.warning(f"API deletion successful, but failed to remove task '{task_id}' from local cache.")
                return False

        except ValueError as ve:
            log.error(f"Input validation error deleting task: {ve}")
            raise
        except Exception as e:
            log.exception(f"Failed to delete task '{task_id}': {e}")
            raise

    async def score_task(self, task_id: str, direction: ScoreDirection | Literal[up, down]) -> dict[str, Any] | None:
        """Scores a task (up or down) via the API, updates local user stats,
        and updates the local task state.

        Args:
            task_id: The ID of the task to score.
            direction: "up" or "down" (or ScoreDirection enum).

        Returns:
            The score result dictionary from the API (containing deltas), or None on failure.

        Raises:
            ValueError: If task or user not found.
            HabiticaAPIError: If the API call fails.
        """
        task_list = self.get_tasks()
        user = self.dm.user
        if not task_list or task_id not in task_list:
            log.error(f"Cannot score task '{task_id}': Task not found or TaskList not loaded.")
            raise ValueError(f"Task with ID '{task_id}' not found locally.")
        if not user:
            log.error(f"Cannot score task '{task_id}': User data not loaded.")
            raise ValueError("User data not loaded.")

        task = task_list.get_by_id(task_id)  # Get the specific task instance

        log.info(f"Attempting to score task '{task_id}' direction '{direction}'...")
        try:
            # 1. Call API
            score_result = await self.api.score_task(task_id=task_id, direction=direction)

            if not score_result:
                log.error(f"API call to score task '{task_id}' failed or returned no data.")
                return None

            # 2. Update local User stats based on score_result deltas
            # This is complex: need to parse score_result keys (+hp, -mp, etc.)
            # and apply them carefully to user.stats and user.party.quest.progress if applicable
            delta = score_result.get("delta", 0)  # Habitica's main score delta

            # --- Apply Deltas to User (Simplified Example) ---
            # A more robust implementation would parse all possible delta keys
            user.stats.hp = max(0, user.stats.hp + score_result.get("hp", 0))  # Ensure HP doesn't go below 0
            user.stats.mp += score_result.get("mp", 0)
            user.stats.exp += score_result.get("exp", 0)
            user.stats.gp += score_result.get("gp", 0)
            new_lvl = score_result.get("lvl")
            if new_lvl and new_lvl > user.stats.lvl:
                user.stats.lvl = new_lvl
                # Need to update maxHP/MP, expToNextLevel etc. based on level up
                # Maybe trigger a full user re-processing?
                log.info(f"User leveled up to {new_lvl}!")
                # Re-fetch user data might be simplest after level up
                # await self.dm.load_user(force_refresh=True) ? Or recalculate locally?

            log.debug(f"Applied score deltas to user: {score_result}")

            # 3. Update local Task state
            update_payload = {}
            if task.type == "habit":
                if direction == "up":
                    update_payload["counterUp"] = task.counter_up + 1
                else:  # direction == "down"
                    update_payload["counterDown"] = task.counter_down + 1
            elif task.type in ["daily", "todo"]:
                # Scoring 'up' completes it, scoring 'down' uncompletes it? Check API behavior.
                # Assume 'up' means complete, 'down' means uncomplete for score_task endpoint
                update_payload["completed"] = direction == "up"
                if direction == "up":
                    # TODO: Add completedDate? API might set this implicitly.
                    update_payload["dateCompleted"] = datetime.now(timezone.utc).isoformat()
                else:
                    update_payload["dateCompleted"] = None

            # Edit the task locally with the derived state changes
            if update_payload:
                updated_task = task_list.edit_task(task_id, update_payload)
                if updated_task:
                    log.debug(f"Updated local task state after scoring: {updated_task}")
                else:
                    log.warning(f"Failed to update local task state for {task_id} after scoring.")
            else:
                # For rewards, scoring consumes GP, no task state change usually needed
                log.debug(f"Scored reward '{task_id}'. No local task state change applied.")

            # 4. Return API score result
            return score_result

        except ValueError as ve:
            log.error(f"Input validation error scoring task: {ve}")
            raise
        except Exception as e:
            log.exception(f"Failed to score task '{task_id}': {e}")
            raise

    async def clear_completed_todos(self) -> bool:
        """Clears completed Todos via API and removes them locally.

        Returns:
            True if successful, False otherwise.
        """
        log.info("Attempting to clear completed Todos...")
        task_list = self.get_tasks()
        if not task_list:
            log.error("Cannot clear Todos: TaskList not loaded.")
            return False

        try:
            # 1. Call API
            api_success = await self.api.clear_completed_todos()
            if not api_success:
                log.error("API call to clear completed Todos failed.")
                return False

            # 2. Remove completed Todos locally
            ids_to_remove = [task.id for task in task_list.get_tasks_by_type("todo") if task.completed]
            log.debug(f"Found {len(ids_to_remove)} completed Todos locally to remove.")
            removed_count = 0
            for task_id in ids_to_remove:
                if task_list.delete_task(task_id):
                    removed_count += 1

            log.info(f"Cleared completed Todos. API success: True. Local removed: {removed_count}/{len(ids_to_remove)}.")
            # self.dm.save_tasks() # Optional
            return True

        except Exception as e:
            log.exception("Failed to clear completed Todos: {e}")
            raise

    # --- Tagging/Checklist Methods (Example: Add Tag) ---

    async def add_tag_to_task(self, task_id: str, tag_id: str) -> AnyTask | None:
        """Adds a tag to a task via API and updates local cache."""
        task_list = self.get_tasks()
        tag_list = self.dm.tags
        if not task_list or task_id not in task_list:
            raise ValueError(f"Task '{task_id}' not found locally.")
        if not tag_list or tag_id not in tag_list:
            raise ValueError(f"Tag '{tag_id}' not found locally.")

        log.info(f"Attempting to add tag '{tag_id}' to task '{task_id}'...")
        try:
            # 1. Call API
            updated_task_data = await self.api.add_tag_to_task(task_id=task_id, tag_id=tag_id)
            if not updated_task_data:
                log.error("API call to add tag to task returned no data.")
                return None

            # 2. Update local task
            # The API response should contain the updated task data including the new tag list
            updated_task = task_list.edit_task(task_id, updated_task_data)
            if updated_task:
                log.info(f"Successfully added tag '{tag_id}' to task '{task_id}'.")
                # self.dm.save_tasks() # Optional
                return updated_task
            else:
                log.error(f"Failed to update local task '{task_id}' after adding tag.")
                return None

        except Exception as e:
            log.exception(f"Failed to add tag '{tag_id}' to task '{task_id}': {e}")
            raise

    async def delete_tag_from_task(self, task_id: str, tag_id: str) -> AnyTask | None:
        """Removes a tag from a task via API and updates local cache."""
        task_list = self.get_tasks()
        tag_list = self.dm.tags  # Assuming TagList holds tag info
        if not task_list or task_id not in task_list:
            raise ValueError(f"Task '{task_id}' not found locally.")
        # No need to check if tag exists, API handles non-existent tag removal gracefully?
        # existing_task = task_list.get_by_id(task_id)
        # if tag_id not in existing_task.tags_id:
        #     log.warning(f"Tag '{tag_id}' not found on task '{task_id}' locally, attempting API removal anyway.")
        #     # return existing_task # Or proceed? Proceed for now.

        log.info(f"Attempting to remove tag '{tag_id}' from task '{task_id}'...")
        try:
            # 1. Call API
            updated_task_data = await self.api.delete_tag_from_task(task_id=task_id, tag_id=tag_id)
            if not updated_task_data:
                log.error("API call to remove tag from task returned no data.")
                return None

            # 2. Update local task
            updated_task = task_list.edit_task(task_id, updated_task_data)
            if updated_task:
                log.info(f"Successfully removed tag '{tag_id}' from task '{task_id}'.")
                # self.dm.save_tasks() # Optional
                return updated_task
            else:
                log.error(f"Failed to update local task '{task_id}' after removing tag.")
                return None

        except Exception as e:
            log.exception(f"Failed to remove tag '{tag_id}' from task '{task_id}': {e}")
            raise

    # --- Checklist Methods (Example: Add Item) ---
    async def add_checklist_item(self, task_id: str, text: str) -> AnyTask | None:
        """Adds a checklist item via API and updates local cache."""
        if not text.strip():
            raise ValueError("Checklist item text cannot be empty.")

        task_list = self.get_tasks()
        if not task_list or task_id not in task_list:
            raise ValueError(f"Task '{task_id}' not found locally.")

        existing_task = task_list.get_by_id(task_id)
        if existing_task.type not in ["daily", "todo"]:
            raise ValueError("Checklists are only supported for Dailies and Todos.")

        log.info(f"Attempting to add checklist item to task '{task_id}'...")
        try:
            # 1. Call API
            updated_task_data = await self.api.add_checklist_item(task_id=task_id, text=text)
            if not updated_task_data:
                log.error("API call to add checklist item returned no data.")
                return None

            # 2. Update local task
            updated_task = task_list.edit_task(task_id, updated_task_data)
            if updated_task:
                log.info(f"Successfully added checklist item to task '{task_id}'.")
                # self.dm.save_tasks() # Optional
                return updated_task
            else:
                log.error(f"Failed to update local task '{task_id}' after adding checklist item.")
                return None

        except Exception as e:
            log.exception(f"Failed to add checklist item to task '{task_id}': {e}")
            raise

    # Add other checklist/tagging methods following the same pattern...
    # score_checklist_item, delete_checklist_item, update_checklist_item
-e 
-------- END OF FILE services/task_service.py --------

-------- START OF FILE services/test.py --------
# pixabit/models/test.py

# ─── Title ────────────────────────────────────────────────────────────────────
#          DataManager Test/Example Script
# ──────────────────────────────────────────────────────────────────────────────

# SECTION: IMPORTS
import asyncio
import logging  # Configure logging for the test
from pathlib import Path

# Project Imports (adjust paths if needed)
try:
    from pixabit.api.client import HabiticaClient
    from pixabit.config import HABITICA_DATA_PATH
    from pixabit.helpers._logger import log

    # Models needed for type checks if accessing data directly
    from pixabit.models.challenge import Challenge, ChallengeList
    from pixabit.models.game_content import StaticContentManager
    from pixabit.models.task import Daily, Task, TaskList

    # Data Manager and Dependencies
    from pixabit.services.data_manager import DataManager

except ImportError as e:
    print(f"Error importing modules for test script: {e}")
    print("Ensure pixabit is installed correctly or PYTHONPATH is set.")
    exit(1)


# SECTION: MAIN TEST FUNCTION
async def main():
    """Main test function for DataManager."""
    log.info("--- Starting DataManager Test Script ---")

    try:
        # 1. Setup Dependencies
        # --- API Client ---
        # Assumes HabiticaClient can be instantiated and picks up credentials
        api_client = HabiticaClient()
        static_cache_dir = HABITICA_DATA_PATH / "static_content"
        content_manager = StaticContentManager(cache_dir=static_cache_dir)
        test_cache_dir = HABITICA_DATA_PATH  # Use main cache for testing interaction
        data_manager = DataManager(api_client=api_client, static_content_manager=content_manager, cache_dir=test_cache_dir)
        log.info(f"DataManager instantiated (Cache: {test_cache_dir}).")

        # 2. Load All Data
        log.info("Loading all data using DataManager...")
        await data_manager.load_all_data(force_refresh=False)  # Set force_refresh=True to bypass caches
        log.success("Data loading phase complete.")

        # 3. Process Loaded Data
        log.info("Processing loaded data...")
        processing_successful = await data_manager.process_loaded_data()
        if not processing_successful:
            log.error("Data processing phase failed. Results may be incomplete.")
            # Decide if script should exit or continue with potentially unprocessed data
            # exit(1)

        # 4. Access Data via Properties
        log.info("Accessing data through DataManager properties...")

        user = data_manager.user
        tasks = data_manager.tasks
        tags = data_manager.tags
        party = data_manager.party
        static_gear = data_manager.static_gear_data  # Access loaded static data

        if user:
            print("\n--- User Info ---")
            print(f"Username: {user.username}")
            print(f"Display Name: {user.display_name}")
            print(f"Level: {user.level}")
            # Access calculated stats
            print(f"Effective STR: {user.effective_stats.get('str', 'N/A'):.1f}")
            print(f"Effective CON: {user.effective_stats.get('con', 'N/A'):.1f}")
            print(f"Max HP: {user.max_hp:.1f}")
            print(f"Gems: {user.gems}")
        else:
            print("\nUser data failed to load.")

        if tasks:
            print("\n--- Tasks Info ---")
            print(f"Total Tasks: {len(tasks)}")
            dailies = tasks.get_dailies()
            print(f"Dailies Count: {len(dailies)}")
            if dailies:
                first_daily = dailies[0]
                print(f"First Daily Text: {first_daily.text}")
                # Access processed info (tag names, status, damage)
                print(f"  -> Tags: {first_daily.tag_names}")
                print(f"  -> Status: {first_daily.calculated_status}")
                print(f"  -> User Damage: {first_daily.user_damage}")  # Uses computed_field
        else:
            print("\nTasks data failed to load.")

        # --- Access Tags ---
        tags = data_manager.tags
        if tags:
            print("\n--- Tags Info ---")
            print(f"Total Tags: {len(tags.tags)}")
        else:
            print("\nTags data unavailable.")

        # --- Access Party ---
        party = data_manager.party
        if party:
            print("\n--- Party Info ---")
            print(f"Party Name: {party.name}")  # ... (other party details)
        else:
            print("\nParty data unavailable or not in party.")

        # --- >>> NEW: Access and Display Challenges <<< ---
        challenges = data_manager.challenges  # Access the ChallengeList property
        if challenges:
            print("\n--- Challenges Info (Joined Sample) ---")
            print(f"Total challenges loaded: {len(challenges)}")
            joined_challenges = challenges.filter_joined(True)  # Filter example
            print(f"Joined challenges found: {len(joined_challenges)}")

            # Display first few joined challenges and their linked tasks
            for i, chal in enumerate(joined_challenges.challenges[:5]):  # Show first 5 joined
                print(f"\n[{i+1}] {repr(chal)}")  # Uses Challenge repr
                # --- Check the linked tasks ---
                if chal.tasks:
                    print(f"    Tasks Linked ({len(chal.tasks)}):")
                    for t_idx, task in enumerate(chal.tasks[:3]):  # Show first 3 linked tasks
                        print(f"      - Task {t_idx+1}: '{task.text[:40]}...' (ID: {task.id[:8]})")
                    if len(chal.tasks) > 3:
                        print("      ...")
                else:
                    print("    (No tasks linked from user's task list)")
        else:
            print("\nChallenges data unavailable.")
        # --- >>> END Challenge Display <<< ---

        log.info("\nData access demonstration complete.")
        log.info(f"Check cache folders '{test_cache_dir}/raw' and '{test_cache_dir}/processed' for saved files.")

    except Exception as e:
        log.exception("An unexpected error occurred in the test script.")


# --- Script Execution ---
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)-8s] %(message)s", datefmt="%H:%M:%S")
    logging.getLogger("Pixabit").setLevel(logging.DEBUG)  # Set level for your app's logger
    asyncio.run(main())
    log.info("--- DataManager Test Script Finished ---")


# ──────────────────────────────────────────────────────────────────────────────
-e 
-------- END OF FILE services/test.py --------

-------- START OF FILE services/__init__.py --------
-e 
-------- END OF FILE services/__init__.py --------

-------- START OF FILE test/Md_Rich/de chatgppti.py --------
"""Provides utilities for converting Markdown to Rich Text and Textual widgets."""

# SECTION: IMPORTS
from typing import Any, Dict, List, Optional

import markdown_it
from markdown_it.token import Token
from rich.console import Console
from rich.markdown import Markdown
from rich.panel import Panel
from rich.style import Style
from rich.text import Text

# Textual integration (optional)
try:
    from textual.widgets import Static

    TEXTUAL_AVAILABLE = True
except ImportError:
    Static = object
    TEXTUAL_AVAILABLE = False

# Use themed console if available
try:
    from .display import console
except ImportError:
    console = Console()

# SECTION: UTILITY FUNCTIONS


def escape_rich(text: str) -> str:
    """Escape Rich markup control characters."""
    return text.replace("[", r"\[").replace("]", r"\]") if text else ""


# SECTION: MARKDOWN RENDERER


class MarkdownRenderer:
    """Converts Markdown to Rich Text and optionally to Textual widgets."""

    DEFAULT_STYLES = {
        "h1": Style(bold=True, color="cyan"),
        "h2": Style(bold=True, color="bright_cyan"),
        "h3": Style(bold=True, color="blue"),
        "h4": Style(underline=True, color="bright_blue"),
        "h5": Style(italic=True, color="blue"),
        "h6": Style(italic=True, dim=True),
        "strong": Style(bold=True),
        "em": Style(italic=True),
        "code_inline": Style(reverse=True),
        "code_block": Style(bgcolor="black", color="green"),
        "strike": Style(strike=True),
        "link": Style(underline=True, color="bright_blue"),
        "blockquote": Style(italic=True, color="green"),
        "hr": Style(dim=True),
    }

    def __init__(self, custom_styles: dict[str, Style] | None = None):
        self.md_parser = markdown_it.MarkdownIt(
            "commonmark", {"breaks": True, "html": True}
        )
        self.styles = self.DEFAULT_STYLES.copy()
        if custom_styles:
            self.styles.update(custom_styles)

    def markdown_to_rich_text(self, markdown_str: str) -> Text:
        if not markdown_str:
            return Text()
        tokens = self.md_parser.parse(markdown_str)
        result = Text()
        self._process_tokens(tokens, result)
        return result

    def _process_tokens(
        self,
        tokens: list[Token],
        text_obj: Text,
        current_styles: dict[str, Any] | None = None,
    ) -> None:
        if current_styles is None:
            current_styles = {}

        i = 0
        while i < len(tokens):
            token = tokens[i]

            if token.type == "inline" and token.children:
                self._process_tokens(
                    token.children, text_obj, current_styles.copy()
                )
                i += 1
                continue

            elif token.type.startswith("heading_open"):
                level = int(token.tag[1])
                heading_style = self.styles.get(f"h{level}", Style())
                temp_styles = current_styles.copy()
                temp_styles.update(heading_style.serialize())

                if i + 1 < len(tokens) and tokens[i + 1].type == "inline":
                    heading_text = Text()
                    self._process_tokens(
                        [tokens[i + 1]], heading_text, temp_styles
                    )
                    text_obj.append(heading_text)
                    text_obj.append("\n")
                    i += 3
                    continue

            elif token.type == "text":
                text_obj.append(token.content, Style(**current_styles))

            elif token.type == "strong_open":
                current_styles.update(self.styles["strong"].serialize())
            elif token.type == "strong_close":
                for k in self.styles["strong"].serialize():
                    current_styles.pop(k, None)

            elif token.type == "em_open":
                current_styles.update(self.styles["em"].serialize())
            elif token.type == "em_close":
                for k in self.styles["em"].serialize():
                    current_styles.pop(k, None)

            elif token.type == "code_inline":
                text_obj.append(token.content, self.styles["code_inline"])

            elif token.type == "code_block":
                text_obj.append("\n")
                text_obj.append(token.content, self.styles["code_block"])
                text_obj.append("\n")

            elif token.type == "fence":
                text_obj.append("\n")
                text_obj.append(token.content, self.styles["code_block"])
                text_obj.append("\n")

            elif token.type == "s_open":
                current_styles.update(self.styles["strike"].serialize())
            elif token.type == "s_close":
                for k in self.styles["strike"].serialize():
                    current_styles.pop(k, None)

            elif token.type == "link_open":
                current_styles.update(self.styles["link"].serialize())
                if "href" in token.attrs:
                    current_styles["url"] = token.attrs["href"]
            elif token.type == "link_close":
                for k in self.styles["link"].serialize():
                    current_styles.pop(k, None)
                current_styles.pop("url", None)

            elif (
                token.type == "paragraph_close"
                or token.type == "blockquote_close"
            ):
                text_obj.append("\n\n")

            elif token.type == "hardbreak":
                text_obj.append("\n")

            elif token.type == "hr":
                text_obj.append("-" * 40 + "\n", self.styles["hr"])

            elif token.type == "blockquote_open":
                text_obj.append("> ", self.styles["blockquote"])

            i += 1

    def markdown_to_rich_markdown(self, markdown_str: str) -> Markdown:
        return Markdown(markdown_str)

    def render_to_console(
        self, markdown_str: str, console: Console | None = None
    ) -> None:
        (console or globals().get("console", Console())).print(
            self.markdown_to_rich_text(markdown_str)
        )

    def render_to_panel(
        self, markdown_str: str, title: str | None = None, **panel_kwargs
    ) -> Panel:
        return Panel(
            self.markdown_to_rich_text(markdown_str),
            title=title,
            **panel_kwargs,
        )


# SECTION: TEXTUAL WIDGET

if TEXTUAL_AVAILABLE:

    class MarkdownStatic(Static):
        def __init__(
            self,
            markdown: str = "",
            renderer: MarkdownRenderer | None = None,
            *args,
            **kwargs,
        ):
            super().__init__("", *args, **kwargs)
            self.markdown_content = markdown
            self._renderer = renderer or MarkdownRenderer()

        def set_markdown(self, markdown: str) -> None:
            self.markdown_content = markdown
            self.update(self._renderer.markdown_to_rich_text(markdown))

        def on_mount(self) -> None:
            if self.markdown_content:
                self.update(
                    self._renderer.markdown_to_rich_text(self.markdown_content)
                )


# SECTION: EXPORTS

__all__ = ["MarkdownRenderer", "escape_rich"]
if TEXTUAL_AVAILABLE:
    __all__.append("MarkdownStatic")
-e 
-------- END OF FILE test/Md_Rich/de chatgppti.py --------

-------- START OF FILE test/Md_Rich/markdown_to_rich.py --------
"""Helpers to convert Markdown to Rich."""

from __future__ import annotations

import logging
from typing import Any, ClassVar, Dict, Optional, Sequence

logger = logging.getLogger(__name__)

try:
    import markdown_it
    from markdown_it.renderer import RendererProtocol
    from markdown_it.token import Token
    from markdown_it.utils import OptionsDict

    MarkdownToken = Token
    MarkdownOptionsDict = OptionsDict
    MarkdownRendererProtocol = RendererProtocol

    MARKDOWN_IT_AVAILABLE = True
except ImportError:
    from typing import Any, Dict, List

    MarkdownToken = Any
    MarkdownOptionsDict = dict[str, Any]
    MarkdownRendererProtocol = Any  # Fallback, not used for actual rendering

    Sequence = List  # fallback
    MARKDOWN_IT_AVAILABLE = False


def escape_rich(text: str) -> str:
    """Escape Rich control characters in text, e.g., unescaped brackets."""
    return text.replace("[", r"\[") if text else ""


class RichRenderer:
    """Renderer that translates Markdown tokens into Rich-friendly strings."""

    name = "rich"
    options: ClassVar[MarkdownOptionsDict] = {}

    def __init__(self) -> None:
        self.output: list[str] = []

    def render(
        self,
        tokens: Sequence[MarkdownToken],
        options: MarkdownOptionsDict | None,
        env: dict | None = None,
    ) -> str:
        """Render Markdown tokens into a Rich-friendly string."""
        self.output.clear()
        for i, token in enumerate(tokens):
            method_name = f"{token.type}_render"
            method = getattr(self, method_name, self.fallback)
            self.output.append(method(tokens, i, options, env))
        return "".join(self.output)

    def fallback(
        self,
        tokens: Sequence[MarkdownToken],
        idx: int,
        options: MarkdownOptionsDict | None,
        env: dict | None,
    ) -> str:
        """Fallback renderer when no specific method is found."""
        return tokens[idx].content or ""

    def inline(
        self,
        tokens: Sequence[MarkdownToken],
        idx: int,
        options: MarkdownOptionsDict | None,
        env: dict | None,
    ) -> str:
        """Render inline tokens recursively."""
        return self.render(tokens[idx].children or [], options, env)

    def paragraph_open(
        self,
        tokens: Sequence[MarkdownToken],
        idx: int,
        options: MarkdownOptionsDict | None,
        env: dict | None,
    ) -> str:
        return ""

    def paragraph_close(
        self,
        tokens: Sequence[MarkdownToken],
        idx: int,
        options: MarkdownOptionsDict | None,
        env: dict | None,
    ) -> str:
        return "\n"

    def bullet_list_open(
        self,
        tokens: Sequence[MarkdownToken],
        idx: int,
        options: MarkdownOptionsDict | None,
        env: dict | None,
    ) -> str:
        return ""

    def bullet_list_close(
        self,
        tokens: Sequence[MarkdownToken],
        idx: int,
        options: MarkdownOptionsDict | None,
        env: dict | None,
    ) -> str:
        return ""

    def list_item_open(
        self,
        tokens: Sequence[MarkdownToken],
        idx: int,
        options: MarkdownOptionsDict | None,
        env: dict | None,
    ) -> str:
        return "• "

    def list_item_close(
        self,
        tokens: Sequence[MarkdownToken],
        idx: int,
        options: MarkdownOptionsDict | None,
        env: dict | None,
    ) -> str:
        return "\n"

    def strong_open(
        self,
        tokens: Sequence[MarkdownToken],
        idx: int,
        options: MarkdownOptionsDict | None,
        env: dict | None,
    ) -> str:
        return "[bold]"

    def strong_close(
        self,
        tokens: Sequence[MarkdownToken],
        idx: int,
        options: MarkdownOptionsDict | None,
        env: dict | None,
    ) -> str:
        return "[/bold]"

    def em_open(
        self,
        tokens: Sequence[MarkdownToken],
        idx: int,
        options: MarkdownOptionsDict | None,
        env: dict | None,
    ) -> str:
        return "[italic]"

    def em_close(
        self,
        tokens: Sequence[MarkdownToken],
        idx: int,
        options: MarkdownOptionsDict | None,
        env: dict | None,
    ) -> str:
        return "[/italic]"

    def link_open(
        self,
        tokens: Sequence[MarkdownToken],
        idx: int,
        options: MarkdownOptionsDict | None,
        env: dict | None,
    ) -> str:
        href = tokens[idx].attrs.get("href") if tokens[idx].attrs else ""
        return f"[link={href}]"

    def link_close(
        self,
        tokens: Sequence[MarkdownToken],
        idx: int,
        options: MarkdownOptionsDict | None,
        env: dict | None,
    ) -> str:
        return "[/link]"

    def heading_open(
        self,
        tokens: Sequence[MarkdownToken],
        idx: int,
        options: MarkdownOptionsDict | None,
        env: dict | None,
    ) -> str:
        level = tokens[idx].tag[1]
        return "[bold magenta]"

    def heading_close(
        self,
        tokens: Sequence[MarkdownToken],
        idx: int,
        options: MarkdownOptionsDict | None,
        env: dict | None,
    ) -> str:
        return "[/bold magenta]\n"


class RichMarkdown:
    """Markdown processor that renders to Rich-friendly output using markdown-it-py."""

    md_parser: ClassVar[Any | None] = None

    @classmethod
    def to_rich(cls, text: str) -> str:
        """Convert Markdown text into a Rich-compatible string."""
        if not text:
            return ""

        if not MARKDOWN_IT_AVAILABLE:
            return escape_rich(text)

        if cls.md_parser is None:
            try:
                cls.md_parser = markdown_it.MarkdownIt(
                    renderer_cls=RichRenderer
                )
            except Exception as e:
                logger.exception(
                    "Failed to initialize MarkdownIt with RichRenderer"
                )
                return escape_rich(text)

        try:
            return cls.md_parser.render(text)
        except Exception as e:
            logger.exception("Failed to render Markdown with RichRenderer")
            return escape_rich(text)
-e 
-------- END OF FILE test/Md_Rich/markdown_to_rich.py --------

-------- START OF FILE test/Md_Rich/prev_mdrichconverter.py --------
# Importamos markdown-it-py
import markdown_it
from markdown_it.token import Token
from rich.markdown import Markdown
from rich.style import Style
from rich.text import Text
from textual.app import App, ComposeResult
from textual.binding import Binding
from textual.containers import Horizontal, Vertical
from textual.widgets import DataTable, Footer, Header, Static


class MarkdownTableApp(App):
    """Una aplicación que muestra datos en una tabla con títulos y contenido formateado usando markdown-it."""

    CSS = """
    .table-container {
        width: 40%;
        height: 100%;
        border: solid green;
    }

    .details-container {
        width: 60%;
        height: 100%;
        border: solid blue;
        overflow: auto;
    }

    #markdown-content {
        padding: 1 2;
    }

    .details-title {
        background: $boost;
        padding: 1 2;
        text-align: center;
        color: $text;
    }
    """

    BINDINGS = [
        Binding(key="q", action="quit", description="Salir"),
        Binding(key="escape", action="quit", description="Salir"),
    ]

    def __init__(self):
        super().__init__()
        # Inicializar el parser de markdown-it
        self.md_parser = markdown_it.MarkdownIt()

        # Almacenar datos con su contenido Markdown
        self.items_data = [
            {
                "id": "1",
                "title": "**Introducción** a Python",
                "description": "Lenguaje de *programación* `versátil`",
                "markdown_content": """
# Introducción a Python

Python es un lenguaje de programación **interpretado** de alto nivel y propósito general.

## Características principales

* Sintaxis clara y legible
* Tipado dinámico
* Orientado a objetos
* Gran biblioteca estándar

```python
def hello_world():
    print("¡Hola, mundo!")
```

Para más información, visita [python.org](https://www.python.org/).
                """,
            },
            {
                "id": "2",
                "title": "Aprendiendo `Textual` para **TUI**",
                "description": "*Framework* **TUI** para Python",
                "markdown_content": """
# Framework Textual

Textual es un *framework* de **TUI** (Text User Interface) para Python.

## Componentes principales

1. Widgets
2. Contenedores
3. Eventos
4. CSS para TUI

### Ejemplo de código

```python
from textual.app import App

class MiApp(App):
    def compose(self):
        yield Header()
        yield Footer()
```

Visita [textual.textualize.io](https://textual.textualize.io/) para documentación.
                """,
            },
            {
                "id": "3",
                "title": "Markdown en *Rich* con `código`",
                "description": "~~Básico~~ -> **Avanzado** y *completo*",
                "markdown_content": """
# Markdown en Rich

Rich permite renderizar Markdown en la terminal con estilos.

## Ejemplo de uso

```python
from rich.markdown import Markdown
from rich.console import Console

console = Console()
md = Markdown("# Título\n\nContenido **importante**.")
console.print(md)
```

## Elementos soportados

| Elemento | Sintaxis |
|----------|----------|
| Negrita | `**texto**` |
| Cursiva | `*texto*` |
| Código | `` `código` `` |
| Enlaces | `[texto](url)` |

> Rich hace que el texto en terminal sea más expresivo y agradable.
                """,
            },
        ]

    def markdown_to_rich_text(self, markdown_str: str) -> Text:
        """Convierte Markdown a Rich Text usando markdown-it-py para un parsing más completo."""
        # Parsear el markdown
        tokens = self.md_parser.parse(markdown_str)

        # Crear un objeto Text para el resultado
        result = Text()

        # Procesar los tokens recursivamente
        self._process_tokens(tokens, result)

        return result

    def _process_tokens(self, tokens, text_obj):
        """Procesa los tokens de markdown-it y aplica los estilos correspondientes."""
        current_styles = {}

        for token in tokens:
            # Manejar tokens inline
            if token.type == "inline" and token.children:
                self._process_tokens(token.children, text_obj)
                continue

            # Procesar formato de texto
            if token.type == "text":
                text_obj.append(token.content, Style(**current_styles))

            elif token.type == "strong_open":
                current_styles["bold"] = True

            elif token.type == "strong_close":
                current_styles.pop("bold", None)

            elif token.type == "em_open":
                current_styles["italic"] = True

            elif token.type == "em_close":
                current_styles.pop("italic", None)

            elif token.type == "code_inline":
                # Para código inline, añadimos directamente con estilo
                text_obj.append(token.content, Style(reverse=True))

            elif token.type == "s_open":
                current_styles["strike"] = True

            elif token.type == "s_close":
                current_styles.pop("strike", None)

            elif token.type == "link_open":
                current_styles["underline"] = True

            elif token.type == "link_close":
                current_styles.pop("underline", None)

    def compose(self) -> ComposeResult:
        yield Header()
        with Horizontal():
            with Vertical(classes="table-container"):
                yield Static(
                    "Selecciona un elemento para ver detalles",
                    classes="details-title",
                )
                yield DataTable(id="items-table")
            with Vertical(classes="details-container"):
                yield Static(
                    "Contenido detallado",
                    id="details-title",
                    classes="details-title",
                )
                yield Static(id="markdown-content")
        yield Footer()

    def on_mount(self) -> None:
        # Configurar la tabla
        table = self.query_one("#items-table", DataTable)
        table.cursor_type = "row"

        # Añadir columnas con títulos formateados
        table.add_columns(
            "ID",
            self.markdown_to_rich_text("**Título**"),
            self.markdown_to_rich_text("*Descripción*"),
        )

        # Añadir filas con contenido formateado
        for item in self.items_data:
            table.add_row(
                item["id"],
                self.markdown_to_rich_text(item["title"]),
                self.markdown_to_rich_text(item["description"]),
            )

    def on_data_table_row_selected(self, event) -> None:
        """Manejar el evento de selección de fila."""
        table = event.data_table
        selected_row_index = table.cursor_row

        if selected_row_index is not None:
            # Obtener el ID de la fila seleccionada
            selected_id = table.get_row_at(selected_row_index)[0]

            # Buscar el elemento correspondiente
            selected_item = next(
                (item for item in self.items_data if item["id"] == selected_id),
                None,
            )

            if selected_item:
                # Actualizar el título del panel de detalles
                details_title = self.query_one("#details-title", Static)
                details_title.update(
                    f"Detalles: {selected_item['title'].replace('*', '').replace('`', '').replace('**', '')}"
                )

                # Actualizar el contenido Markdown
                markdown_content = self.query_one("#markdown-content", Static)
                markdown_content.update(
                    Markdown(selected_item["markdown_content"])
                )


if __name__ == "__main__":
    app = MarkdownTableApp()
    app.run()
-e 
-------- END OF FILE test/Md_Rich/prev_mdrichconverter.py --------

-------- START OF FILE ui/app.py --------
# pixabit/ui/app.py
import asyncio
from datetime import datetime
from typing import Any, Callable, Dict, Optional

from textual import events, on
from textual.app import App, ComposeResult
from textual.binding import Binding
from textual.containers import Container, Horizontal, Vertical
from textual.reactive import reactive
from textual.screen import ModalScreen
from textual.widgets import Button, Label, Static, Switch, Tab, TabbedContent, TabPane

from pixabit.api.client import HabiticaClient
from pixabit.config import HABITICA_DATA_PATH
from pixabit.helpers._logger import log
from pixabit.models.game_content import StaticContentManager
from pixabit.models.user import User
from pixabit.services.challenge_service import ChallengeService
from pixabit.services.data_manager import DataManager
from pixabit.ui.widgets.challenge_view import ChallengeScreen, ChallengeView
from pixabit.ui.widgets.help_modal import HelpModal
from pixabit.ui.widgets.main_panel import TaskView
from pixabit.ui.widgets.sidebar_stats import SidebarStats
from pixabit.ui.widgets.sleep_toggle import SleepToggle

# Importar widgets modulares
from pixabit.ui.widgets.stats_count import StatsCount
from pixabit.ui.widgets.table_detail_panel import TaskDetailPanel
from pixabit.ui.widgets.table_list_panel import TaskListWidget


class HabiticaApp(App):
    """Textual App for Habitica with sidebar and improved UI."""

    CSS_PATH = "style.tcss"
    BINDINGS = [
        Binding(key="q", action="quit", description="Salir"),
        Binding(key="escape", action="quit", description="Salir"),
        Binding(key="r", action="refresh_data", description="Actualizar datos"),
        # Add task-specific bindings
        Binding(key="c", action="complete_task", description="Complete task", show=False),
        Binding(key="e", action="edit_task", description="Edit task", show=False),
        Binding(key="d", action="delete_task", description="Delete task", show=False),
        Binding(key="?", action="help", description="Show Keyboard Shortcuts"),
    ]

    def __init__(self):
        super().__init__()
        # Inicializar componentes principales
        self._initialize_dependencies()
        # Estado de la aplicación
        self.quest_data: Dict[str, Any] = {}
        self.sidebar_stats = None
        self.widgets_initialized = False
        self.task_container = None

    def get_bindings_info(self):
        """Extract (key, action, description) from BINDINGS."""
        return [(key, action, desc) for key, action, desc in self.BINDINGS]

    def _initialize_dependencies(self) -> None:
        """Inicializa las dependencias principales de la aplicación."""
        log.info("Initializing core dependencies")
        # Setup API client
        self.api_client = HabiticaClient()
        # Setup Static Content Manager
        static_cache_dir = HABITICA_DATA_PATH / "static_content"
        content_manager = StaticContentManager(cache_dir=static_cache_dir)
        # Setup Data Manager
        cache_dir = HABITICA_DATA_PATH
        self.data_manager = DataManager(api_client=self.api_client, static_content_manager=content_manager, cache_dir=cache_dir)
        self.challenge_service = ChallengeService(api_client=self.api_client, data_manager=self.data_manager)

    async def on_mount(self) -> None:
        """Runs when the app starts: setup and load initial data."""
        # Get references to important widgets
        self.sidebar_stats = self.query_one(SidebarStats)
        # self.task_container = self.query_one("#task-tab-container", TaskView)
        self.widgets_initialized = True

        # Initial data load
        await self.load_and_refresh_data(show_status=True)

    async def load_and_refresh_data(self, force_refresh: bool = False, show_status: bool = True) -> bool:
        """Carga todos los datos necesarios y actualiza la UI.

        Args:
            force_refresh: Si es True, forzar la recarga desde la API
            show_status: Si es True, mostrar mensajes de estado

        Returns:
            bool: True si los datos se cargaron y procesaron correctamente
        """
        if show_status:
            self.update_status("Loading Habitica data...", "loading")

        success = False
        try:
            # Paso 1: Cargar datos
            data_loaded = await self.data_manager.load_all_data(force_refresh=force_refresh)

            # Paso 2: Procesar datos
            if data_loaded:
                processing_successful = await self.data_manager.process_loaded_data()

                # Paso 3: Obtener datos de quest si el usuario está en una
                if processing_successful and self.data_manager.user:
                    if getattr(self.data_manager.user, "is_on_quest", False):
                        self.quest_data = await self._get_quest_data()

                # Paso 4: Actualizar UI con los datos cargados
                await self.update_ui_with_data(data_loaded, processing_successful)

                # Establecer éxito general
                success = processing_successful

                if show_status:
                    if success:
                        self.update_status("Data loaded successfully", "success")
                    else:
                        self.update_status("Error processing data", "error")
            else:
                if show_status:
                    self.update_status("Failed to load data", "error")

        except Exception as e:
            log.error(f"Data loading error: {e}")
            if show_status:
                self.update_status(f"Error: {str(e)}", "error")

        return success

    async def _get_quest_data(self) -> Dict[str, Any]:
        """Gets quest data from the API or data manager."""
        if not self.data_manager or not self.data_manager.user:
            return {}

        # En una implementación real, obtendrías estos datos de party.quest
        # Este es solo un placeholder
        is_boss = getattr(self.data_manager.user, "is_on_boss_quest", False)

        if is_boss:
            return {
                "type": "boss",
                "title": "The Mighty Dragon",
                "progress": 150,  # Current damage
                "progressNeeded": 500,  # Boss HP
                "boss": {"hp": 500, "name": "Dragon"},
            }
        else:
            # Collection quest
            return {
                "type": "collect",
                "title": "Gather Supplies",
                "progress": 7,  # Items collected
                "progressNeeded": 15,  # Items needed
                "collect": {"items": [{"name": "Wood", "count": 3}, {"name": "Stone", "count": 4}]},
            }

    def action_help(self) -> None:
        """Show the keyboard help modal."""
        self.app.push_screen(HelpModal())

    def update_status(self, message: str, status_class: str = "") -> None:
        """Actualiza el mensaje de estado en la barra lateral.

        Args:
            message: Mensaje a mostrar
            status_class: Clase CSS para el estilo del mensaje
        """
        if self.sidebar_stats and self.widgets_initialized:
            self.sidebar_stats.update_status(message, status_class)
        else:
            log.info(f"Status update: {message}")

    async def update_ui_with_data(self, data_loaded: bool, processing_successful: bool) -> None:
        """Update UI widgets with data from DataManager."""
        if not self.data_manager or not self.widgets_initialized:
            log.error("DataManager is not initialized or widgets not ready when trying to update UI.")
            return

        try:
            # Get widget references
            user_info_widget = self.query_one("#user-info-static", Static)
            stats_widget = self.query_one(StatsCount)
            sidebar_stats = self.query_one(SidebarStats)
            sleep_toggle = self.query_one(SleepToggle)

            # Update widgets if data is available
            if self.data_manager.user and data_loaded and processing_successful:
                # Update user info
                username = self.data_manager.user.username
                user_class = self.data_manager.user.klass
                is_sleeping = getattr(self.data_manager.user, "is_sleeping", False)

                # Emojis
                class_emoji = {"warrior": "🗡️", "wizard": "🧙", "healer": "💚", "rogue": "⚔️", "": "👤"}.get(user_class.lower(), "👤")

                # Update widgets
                user_info_widget.update(f"{class_emoji} [b]{username}[/b]")
                stats_widget.update_display(self.data_manager.user)
                sidebar_stats.update_sidebar_stats(self.data_manager.user, self.quest_data)
                sleep_toggle.update_sleep_state(self.data_manager.user)

                # Refresh task data if task container is initialized
                if self.task_container:
                    await self.task_container.refresh_data()
            else:
                # Reset widgets if no data
                user_info_widget.update("Failed to load user data")
                stats_widget.update_display(None)
                sidebar_stats.update_sidebar_stats(None)
                sleep_toggle.update_sleep_state(None)
        except Exception as e:
            log.error(f"Error updating UI with data: {e}")

    async def on_data_changed(self, event_data: Dict[str, Any] = None) -> None:
        """Manejador de eventos para cuando cambian los datos.
        Esto puede ser invocado por cualquier widget que modifique los datos.

        Args:
            event_data: Datos opcionales sobre el cambio
        """
        log.info(f"Data changed event received: {event_data}")
        await self.load_and_refresh_data(force_refresh=True)

    async def action_refresh_data(self) -> None:
        """Action handler for manual data refresh (key binding 'r')."""
        self.update_status("Manually refreshing data...", "loading")
        await self.load_and_refresh_data(force_refresh=True)

        # Task-related action handlers

        async def action_complete_task(self) -> None:
            """Complete the currently selected task."""
            if not self.task_container:
                return

            # Use direct widget references for simplicity
            detail_panel = self.task_container.query_one(TaskDetailPanel)
            if detail_panel and detail_panel.current_task:
                task_id = detail_panel.current_task.get("id", "")
                detail_panel.post_message(TaskDetailPanel.CompleteTask(task_id))

    async def action_edit_task(self) -> None:
        """Edit the currently selected task."""
        if not self.task_container:
            return

        detail_panel = self.task_container.query_one(TaskDetailPanel)
        if detail_panel and detail_panel.current_task:
            task_id = detail_panel.current_task.get("id", "")
            detail_panel.post_message(TaskDetailPanel.EditTask(task_id))

    async def action_delete_task(self) -> None:
        """Delete the currently selected task."""
        if not self.task_container:
            return

        detail_panel = self.task_container.query_one(TaskDetailPanel)
        if detail_panel and detail_panel.current_task:
            task_id = detail_panel.current_task.get("id", "")
            detail_panel.post_message(TaskDetailPanel.DeleteTask(task_id))

    # Event handlers for task-related messages

    async def handle_score_task_request(self, message: TaskView.ScoreTaskRequest) -> None:
        """Handle scoring a task."""
        if not self.data_manager:
            self.update_status("No data manager available to score tasks", "error")
            return

        try:
            if await self.data_manager.score_task(message.task_id, message.direction):
                self.update_status(f"Task scored {message.direction}", "success")
                # Refresh data after scoring
                await self.load_and_refresh_data(force_refresh=True, show_status=False)
            else:
                self.update_status("Failed to score task", "error")
        except Exception as e:
            log.error(f"Error scoring task: {e}")
            self.update_status(f"Error: {str(e)}", "error")

    def compose(self) -> ComposeResult:
        """Create the UI layout with sidebar and main content."""
        with Container(id="app-container"):
            # Sidebar
            with Horizontal(id="sidebar"):
                yield Static("Loading user...", id="user-info-static")
                # Sidebar stats
                yield SidebarStats()
            # Main content area
            with Vertical(id="main-content"):
                with Vertical(id="content"):
                    with TabbedContent(id="main-tabs"):
                        # Paneles de pestañas
                        with TabPane("User", id="user"):
                            yield Static("Main content area - Tasks will display here")
                            yield StatsCount()
                            # Pasamos una referencia del método on_data_changed para que el widget
                            # pueda notificar cambios en los datos
                            yield SleepToggle(api_client=self.api_client, status_update_callback=self.update_status, on_data_changed=self.on_data_changed)
                        with TabPane("Tasks", id="tasks"):
                            # Integramos el contenedor de tareas aquí
                            yield TaskView(id="task-tab-container", data_manager=self.data_manager, api_client=self.api_client, on_data_changed=self.on_data_changed)

                        with TabPane("Tags", id="tags"):
                            yield Static("Tags content goes here")
                        with TabPane("Challenges", id="challenges"):
                            yield ChallengeView(challenge_service=self.challenge_service)
                        with TabPane("Party", id="party"):
                            yield Static("Party content goes here")
                        with TabPane("Messages", id="messages"):
                            yield Static("Messages content goes here")
                        with TabPane("Settings", id="settings"):
                            yield Static("Settings content goes here")


if __name__ == "__main__":
    app = HabiticaApp()
    app.run()
-e 
-------- END OF FILE ui/app.py --------

-------- START OF FILE ui/widgets/challenge_detail_panel.py --------
from pixabit.helpers._logger import log
from pixabit.helpers._md_to_rich import MarkdownRenderer
from pixabit.helpers._textual import Button, ComposeResult, DataTable, Horizontal, Markdown, Message, ScrollableContainer, Select, Static, Vertical, on, reactive
from pixabit.models.challenge import Challenge


def md_render(str):
    return MarkdownRenderer.markdown_to_rich_text(str)


class ChallengeDetailPanel(ScrollableContainer):
    """Panel that displays challenge details and actions."""

    current_challenge = reactive(None)
    loading_tasks = reactive(False)

    # Define messages
    class ScoreChallengeDetail(Message):
        """Message for viewing challenge tasks."""

        def __init__(self, challenge_id: str, get_all: bool = False) -> None:
            self.challenge_id = challenge_id
            self.get_all = get_all
            super().__init__()

    class JoinChallenge(Message):
        """Message for joining a challenge."""

        def __init__(self, challenge_id: str) -> None:
            self.challenge_id = challenge_id
            super().__init__()

    class LeaveChallenge(Message):
        """Message for leaving a challenge."""

        def __init__(self, challenge_id: str, keep: str) -> None:
            self.challenge_id = challenge_id
            self.keep = keep
            super().__init__()

    class CompleteChallenge(Message):
        """Message for completing a challenge."""

        def __init__(self, challenge_id: str) -> None:
            self.challenge_id = challenge_id
            super().__init__()

    class EditChallenge(Message):
        """Message for editing a challenge."""

        def __init__(self, challenge_id: str) -> None:
            self.challenge_id = challenge_id
            super().__init__()

    class ViewChallengeTasks(Message):
        """Message for viewing challenge tasks."""

        def __init__(self, challenge_id: str) -> None:
            self.challenge_id = challenge_id
            super().__init__()

    def __init__(self, id: str = None, challenge_service=None) -> None:
        """Initialize the challenge detail panel.

        Args:
            id: Widget ID.
            challenge_service: The challenge service for API interactions.
        """
        super().__init__(id=id)
        self.challenge_service = challenge_service
        self._detail_content = None
        self._task_list = None
        self._keep_option = "keep-all"  # Default option for leaving challenges

    def compose(self) -> ComposeResult:
        """Compose the challenge detail panel."""
        yield Static("Select a challenge to view details", id="challenge-detail-header")

        with Vertical(id="challenge-detail-content"):

            yield Markdown("", id="challenge-details")

        with Horizontal(id="challenge-action-buttons", classes="hidden"):
            yield Button("Join", id="join-challenge-btn", variant="success")
            yield Button("Leave ", id="leave-challenge-btn", variant="error")
            yield Select([("Keep", "keep-all"), ("Remove Tasks", "remove-all")], id="leave-option-select", value="keep-all")
            yield Button("Tasks", id="view-tasks-btn", variant="primary")
            # yield Button("Edit", id="edit-challenge-btn", variant="default")

        with Vertical(id="challenge-tasks-container", classes="hidden"):
            yield Static("Challenge Tasks", id="tasks-header")
            yield DataTable(id="challenge-tasks-table")
            yield Button("+", id="load-more-tasks-btn", variant="primary")

    def watch_current_challenge(self, challenge: Challenge) -> None:
        """Watch for changes to the current_challenge property."""
        self._update_detail_view()

    def _update_detail_view(self) -> None:
        """Update the detail view with current challenge data."""
        detail_content = self.query_one("#challenge-details")
        action_buttons = self.query_one("#challenge-action-buttons")
        tasks_container = self.query_one("#challenge-tasks-container")

        if not self.current_challenge:
            detail_content.update("Select a challenge to view details")
            action_buttons.add_class("hidden")
            tasks_container.add_class("hidden")
            return

        # Format challenge details as markdown
        details = f"""# {self.current_challenge.name}
**Description:** {self.current_challenge.description if hasattr(self.current_challenge, 'description') else 'No description'}
**Owner:** {self.current_challenge.leader.name if hasattr(self.current_challenge, 'leader') else 'Unknown'}
**Member Count:** {self.current_challenge.memberCount if hasattr(self.current_challenge, 'memberCount') else 'Unknown'}
**Prize:** {self.current_challenge.prize if hasattr(self.current_challenge, 'prize') else '0'} gems
**Joined:** {"Yes" if self.current_challenge.joined else "No"}
**Created:** {self.current_challenge.createdAt if hasattr(self.current_challenge, 'createdAt') else 'Unknown'}
"""
        detail_content.update(details)

        # Update action buttons based on whether user has joined the challenge
        action_buttons.remove_class("hidden")
        join_btn = self.query_one("#join-challenge-btn")
        leave_btn = self.query_one("#leave-challenge-btn")
        leave_options = self.query_one("#leave-option-select")

        if self.current_challenge.joined is False:
            join_btn(disabled=False)
            leave_btn(disabled=True)
            leave_options(disabled=True)
        else:
            join_btn(disabled=True)
            leave_btn(disabled=False)
            leave_options(disabled=False)

        # Initially hide tasks
        tasks_container.add_class("hidden")

    async def _load_challenge_tasks(self) -> None:
        """Load tasks for the current challenge."""
        if not self.current_challenge or not self.challenge_service:
            return

        self.loading_tasks = True
        tasks_container = self.query_one("#challenge-tasks-container")
        tasks_table = self.query_one("#challenge-tasks-table")

        try:
            # Fetch tasks for the current challenge
            tasks = await self.challenge_service.fetch_challenge_tasks(self.current_challenge.id)

            if tasks:
                # Set up table if not already done
                if not tasks_table.columns:
                    tasks_table.add_columns("Text", "Type", "Difficulty", "Notes")

                # Clear existing rows
                tasks_table.clear()

                # Add task rows
                for task in tasks:
                    text = task.text if hasattr(task, "text") else "Unknown"
                    task_type = task.type if hasattr(task, "type") else "Unknown"
                    difficulty = task.priority if hasattr(task, "priority") else "1"
                    notes = task.notes if hasattr(task, "notes") else ""

                    tasks_table.add_row(text, task_type, difficulty, notes)

                # Show the tasks container
                tasks_container.remove_class("hidden")
            else:
                tasks_table.clear()
                if not tasks_table.columns:
                    tasks_table.add_columns("Text", "Type", "Difficulty", "Notes")
                tasks_table.add_row("No tasks found for this challenge", "", "", "")
                tasks_container.remove_class("hidden")

        except Exception as e:
            log.exception(f"Error loading challenge tasks: {e}")
            tasks_table.clear()
            if not tasks_table.columns:
                tasks_table.add_columns("Error")
            tasks_table.add_row(f"Error loading tasks: {str(e)}")
            tasks_container.remove_class("hidden")
        finally:
            self.loading_tasks = False

    # Event handlers

    @on(Button.Pressed, "#join-challenge-btn")
    async def handle_join_challenge(self) -> None:
        """Handle join challenge button press."""
        if not self.current_challenge:
            return

        self.post_message(self.JoinChallenge(self.current_challenge.id))

    @on(Button.Pressed, "#leave-challenge-btn")
    async def handle_leave_challenge(self) -> None:
        """Handle leave challenge button press."""
        if not self.current_challenge:
            return

        keep_option = self.query_one("#leave-option-select").value
        self.post_message(self.LeaveChallenge(self.current_challenge.id, keep_option))

    @on(Select.Changed, "#leave-option-select")
    def handle_leave_option_change(self, event: Select.Changed) -> None:
        """Handle leave option change."""
        self._keep_option = event.value

    @on(Button.Pressed, "#view-tasks-btn")
    async def handle_view_tasks(self) -> None:
        """Handle view tasks button press."""
        if not self.current_challenge:
            return

        self.post_message(self.ViewChallengeTasks(self.current_challenge.id))
        await self._load_challenge_tasks()

    @on(Button.Pressed, "#edit-challenge-btn")
    def handle_edit_challenge(self) -> None:
        """Handle edit challenge button press."""
        if not self.current_challenge:
            return

        self.post_message(self.EditChallenge(self.current_challenge.id))

    @on(Button.Pressed, "#load-more-tasks-btn")
    async def handle_load_more_tasks(self) -> None:
        """Handle load more tasks button press."""
        # This would implement pagination for tasks if the API supports it
        await self._load_challenge_tasks()


from pixabit.helpers._textual import (
    Button,
    ComposeResult,
    Container,
    Label,
    Message,
    ScrollableContainer,
    Select,
    TabbedContent,
    TabPane,
    on,
    reactive,
)
from pixabit.models.challenge import Challenge, ChallengeList

# Import our custom Pagination widget
from .pagination import Pagination


class ChallengeListWidget(Container):
    """Widget that displays a list of challenges with filtering and pagination."""

    # Define reactive properties
    challenges_data = reactive([])
    current_page = reactive(0)
    total_pages = reactive(50)
    member_only = reactive(False)
    loading = reactive(False)

    # Define constants
    ITEMS_PER_PAGE = 10

    class ViewChallengeDetailsRequest(Message):
        """Message requesting to view challenge details."""

        def __init__(self, challenge_id: str) -> None:
            self.challenge_id = challenge_id
            super().__init__()

    def __init__(self, id: str = None, challenge_service=None) -> None:
        """Initialize the challenge list widget.

        Args:
            id: Widget ID.
            challenge_service: The challenge service for API interactions.
        """
        super().__init__(id=id)
        self.challenge_service = challenge_service
        # Initialize widget references to None; they will be assigned in compose
        self._data_table = None
        self._pagination = None
        self._tabbed_content = None
        self.cursor_type = "row"  # Still applies to the DataTable

    def compose(self) -> ComposeResult:
        """Compose the challenge list widget with tabs, data table, and pagination."""
        with Vertical(id="challenge-list-container"):
            with Horizontal(id="challenge-filter-controls"):
                # Create and yield the TabbedContent with tab titles
                with TabbedContent(id="challenges-tabs") as tabbed_content:
                    self._tabbed_content = tabbed_content
                    yield TabPane("All Challenges", id="tab-all")
                    yield TabPane("Member Challenges", id="tab-member")

            # Yield the DataTable and Pagination widgets

            self._data_table = DataTable(id="challenge-data-table")
            yield self._data_table
            self._pagination = Pagination(
                page_count=50,
                current_page=self.current_page + 1,
                page_size=self.ITEMS_PER_PAGE,
                id="challenge-pagination",
            )
            yield self._pagination

    async def on_mount(self) -> None:
        """Configure the widget when it's mounted."""
        # Set cursor type on the data table
        self._data_table.cursor_type = "row"

        # Set up the data table columns
        self._data_table.add_columns("Name", "Member Count", "Prize", "Joined")

        # Initial data load will happen when the TabbedContent.TabActivated event fires

    async def load_or_refresh_data(self) -> None:
        """Load or refresh challenge data from the service."""
        if not self.challenge_service:
            log.warning("No challenge service available to load challenges")
            return

        if self.loading:
            log.debug("Load already in progress, skipping.")
            return  # Prevent duplicate loads

        # Ensure widgets are initialized
        if self._data_table is None or self._pagination is None:
            log.error("DataTable or Pagination widget is not initialized.")
            return

        self.loading = True
        self._data_table.loading = True  # Show loading indicator

        try:
            log.debug(f"Fetching challenges: member_only={self.member_only}, page={self.current_page}")

            # Fetch challenges using the challenge service
            challenge_list_response = await self.challenge_service.fetch_challenges(member_only=self.member_only, page=self.current_page)

            # Process the response
            self._process_challenge_response(challenge_list_response)

        except Exception as e:
            log.exception(f"Error loading challenges: {e}")
            self._handle_load_error("Error loading challenges.")

        finally:
            self.loading = False
            if self._data_table is not None:
                self._data_table.loading = False  # Hide loading indicator

    def _process_challenge_response(self, challenge_list_response):
        """Process the challenge list response and update the UI."""
        if challenge_list_response and hasattr(challenge_list_response, "challenges"):
            self.challenges_data = challenge_list_response.challenges

            # Update total_pages based on API response
            if hasattr(challenge_list_response, "totalPages"):
                self.total_pages = challenge_list_response.totalPages
            else:
                log.warning("API response missing totalPages. Using default calculation.")
                self.total_pages = max(1, (len(self.challenges_data) + self.ITEMS_PER_PAGE - 1) // self.ITEMS_PER_PAGE)

            # Update the data table
            self._data_table.clear()

            if not self.challenges_data:
                self._data_table.add_row("No challenges found for this filter.", "", "", "", "")
            else:
                for challenge in self.challenges_data:
                    self._data_table.add_row(
                        getattr(challenge, "name", "N/A"),
                        # getattr(challenge.leader, "name", "Unknown") if hasattr(challenge, "leader") else "Unknown",
                        str(getattr(challenge, "memberCount", "0")),
                        str(getattr(challenge, "prize", "0")),
                        "✓" if getattr(challenge, "joined", False) else "",
                    )

            # Update pagination
            self._pagination.page_count = self.total_pages
            self._pagination.current_page = self.current_page + 1  # Convert 0-indexed to 1-indexed
        else:
            log.warning("Failed to load challenges or empty result/unexpected format")
            self._handle_load_error("Failed to load challenges.")

    def _handle_load_error(self, error_message):
        """Handle loading errors by resetting state and showing an error message."""
        self.challenges_data = []
        self._data_table.clear()
        self.total_pages = 100  # Reset total pages
        self._pagination.page_count = self.total_pages
        self._pagination.current_page = 1  # Reset to first page
        self._data_table.add_row(error_message, "", "", "", "")  # Display error message

    # Event handlers

    @on(DataTable.RowHighlighted)
    def handle_row_selected(self, event: DataTable.RowHighlighted) -> None:
        """Handle row selection in the data table."""
        # Check if the selected row index is within the bounds of the loaded data
        if 0 <= event.cursor_row < len(self.challenges_data):
            challenge = self.challenges_data[event.cursor_row]
            challenge_id = getattr(challenge, "id", None)

            if challenge_id:
                log.debug(f"Selected challenge ID: {challenge_id}")
                self.post_message(self.ViewChallengeDetailsRequest(challenge_id))
            else:
                log.warning(f"Challenge at row {event.cursor_row} is missing 'id' attribute.")
        else:
            log.warning(f"Row index {event.cursor_row} is out of bounds. challenges_data length: {len(self.challenges_data)}")

    @on(TabbedContent.TabActivated)
    async def on_tabbed_content_tab_activated(self, event: TabbedContent.TabActivated) -> None:
        """Handle tab activation change."""
        log.debug(f"Tab activated: {event.tab.id}")

        # Determine the filter based on the activated tab's ID
        self.member_only = event.tab.id == "tab-member"
        log.debug(f"Switched to {'Member' if self.member_only else 'All'} tab.")

        # Reset to the first page when changing tabs
        self.current_page = 0

        # Load data for the newly selected filter
        await self.load_or_refresh_data()

    @on(Pagination.PageChanged)
    async def handle_page_changed(self, event: Pagination.PageChanged) -> None:
        """Handle pagination page change."""
        log.debug(f"Page changed to: {event.page}")

        # Convert from 1-indexed (UI) to 0-indexed (API)
        self.current_page = event.page - 1
        await self.load_or_refresh_data()

    # If you want to add a refresh button:
    # @on(Button.Pressed, "#refresh-challenges-btn")
    # async def handle_refresh_button(self) -> None:
    #     """Handle refresh button press."""
    #     log.debug("Refresh button pressed.")
    #     await self.load_or_refresh_data()
-e 
-------- END OF FILE ui/widgets/challenge_detail_panel.py --------

-------- START OF FILE ui/widgets/challenge_list_widget.py --------
from pixabit.helpers._logger import log
from pixabit.helpers._textual import (
    Button,
    ComposeResult,
    Container,
    DataTable,
    Horizontal,
    Label,
    Message,
    ScrollableContainer,
    Select,
    Static,
    TabbedContent,
    TabPane,
    Vertical,
    on,
    reactive,
)
from pixabit.models.challenge import Challenge, ChallengeList

# Import our custom Pagination widget
from .pagination import Pagination


class ChallengeListWidget(Container):
    """Widget that displays a list of challenges with filtering and pagination."""

    # Define reactive properties
    challenges_data = reactive([])
    current_page = reactive(0)
    total_pages = reactive(50)
    member_only = reactive(False)
    loading = reactive(False)

    # Define constants
    ITEMS_PER_PAGE = 10

    class ViewChallengeDetailsRequest(Message):
        """Message requesting to view challenge details."""

        def __init__(self, challenge_id: str) -> None:
            self.challenge_id = challenge_id
            super().__init__()

    def __init__(self, id: str = None, challenge_service=None) -> None:
        """Initialize the challenge list widget.

        Args:
            id: Widget ID.
            challenge_service: The challenge service for API interactions.
        """
        super().__init__(id=id)
        self.challenge_service = challenge_service
        # Initialize widget references to None; they will be assigned in compose
        self._data_table = None
        self._pagination = None
        self._tabbed_content = None
        self.cursor_type = "row"  # Still applies to the DataTable

    def compose(self) -> ComposeResult:
        """Compose the challenge list widget with tabs, data table, and pagination."""
        with Vertical(id="challenge-list-container"):
            with Horizontal(id="challenge-filter-controls"):
                # Create and yield the TabbedContent with tab titles
                with TabbedContent(id="challenges-tabs") as tabbed_content:
                    self._tabbed_content = tabbed_content
                    yield TabPane("All Challenges", id="tab-all")
                    yield TabPane("Member Challenges", id="tab-member")

            # Yield the DataTable and Pagination widgets

            self._data_table = DataTable(id="challenge-data-table")
            yield self._data_table
            self._pagination = Pagination(
                page_count=50,
                current_page=self.current_page + 1,
                page_size=self.ITEMS_PER_PAGE,
                id="challenge-pagination",
            )
            yield self._pagination

    async def on_mount(self) -> None:
        """Configure the widget when it's mounted."""
        # Set cursor type on the data table
        self._data_table.cursor_type = "row"

        # Set up the data table columns
        self._data_table.add_columns("Name", "Member Count", "Prize", "Joined")

        # Initial data load will happen when the TabbedContent.TabActivated event fires

    async def load_or_refresh_data(self) -> None:
        """Load or refresh challenge data from the service."""
        if not self.challenge_service:
            log.warning("No challenge service available to load challenges")
            return

        if self.loading:
            log.debug("Load already in progress, skipping.")
            return  # Prevent duplicate loads

        # Ensure widgets are initialized
        if self._data_table is None or self._pagination is None:
            log.error("DataTable or Pagination widget is not initialized.")
            return

        self.loading = True
        self._data_table.loading = True  # Show loading indicator

        try:
            log.debug(f"Fetching challenges: member_only={self.member_only}, page={self.current_page}")

            # Fetch challenges using the challenge service
            challenge_list_response = await self.challenge_service.fetch_challenges(member_only=self.member_only, page=self.current_page)

            # Process the response
            self._process_challenge_response(challenge_list_response)

        except Exception as e:
            log.exception(f"Error loading challenges: {e}")
            self._handle_load_error("Error loading challenges.")

        finally:
            self.loading = False
            if self._data_table is not None:
                self._data_table.loading = False  # Hide loading indicator

    def _process_challenge_response(self, challenge_list_response):
        """Process the challenge list response and update the UI."""
        if challenge_list_response and hasattr(challenge_list_response, "challenges"):
            self.challenges_data = challenge_list_response.challenges

            # Update total_pages based on API response
            if hasattr(challenge_list_response, "totalPages"):
                self.total_pages = challenge_list_response.totalPages
            else:
                log.warning("API response missing totalPages. Using default calculation.")
                self.total_pages = max(1, (len(self.challenges_data) + self.ITEMS_PER_PAGE - 1) // self.ITEMS_PER_PAGE)

            # Update the data table
            self._data_table.clear()

            if not self.challenges_data:
                self._data_table.add_row("No challenges found for this filter.", "", "", "", "")
            else:
                for challenge in self.challenges_data:
                    self._data_table.add_row(
                        getattr(challenge, "name", "N/A"),
                        # getattr(challenge.leader, "name", "Unknown") if hasattr(challenge, "leader") else "Unknown",
                        str(getattr(challenge, "memberCount", "0")),
                        str(getattr(challenge, "prize", "0")),
                        "✓" if getattr(challenge, "joined", False) else "",
                    )

            # Update pagination
            self._pagination.page_count = self.total_pages
            self._pagination.current_page = self.current_page + 1  # Convert 0-indexed to 1-indexed
        else:
            log.warning("Failed to load challenges or empty result/unexpected format")
            self._handle_load_error("Failed to load challenges.")

    def _handle_load_error(self, error_message):
        """Handle loading errors by resetting state and showing an error message."""
        self.challenges_data = []
        self._data_table.clear()
        self.total_pages = 100  # Reset total pages
        self._pagination.page_count = self.total_pages
        self._pagination.current_page = 1  # Reset to first page
        self._data_table.add_row(error_message, "", "", "", "")  # Display error message

    # Event handlers

    @on(DataTable.RowHighlighted)
    def handle_row_selected(self, event: DataTable.RowHighlighted) -> None:
        """Handle row selection in the data table."""
        # Check if the selected row index is within the bounds of the loaded data
        if 0 <= event.cursor_row < len(self.challenges_data):
            challenge = self.challenges_data[event.cursor_row]
            challenge_id = getattr(challenge, "id", None)

            if challenge_id:
                log.debug(f"Selected challenge ID: {challenge_id}")
                self.post_message(self.ViewChallengeDetailsRequest(challenge_id))
            else:
                log.warning(f"Challenge at row {event.cursor_row} is missing 'id' attribute.")
        else:
            log.warning(f"Row index {event.cursor_row} is out of bounds. challenges_data length: {len(self.challenges_data)}")

    @on(TabbedContent.TabActivated)
    async def on_tabbed_content_tab_activated(self, event: TabbedContent.TabActivated) -> None:
        """Handle tab activation change."""
        log.debug(f"Tab activated: {event.tab.id}")

        # Determine the filter based on the activated tab's ID
        self.member_only = event.tab.id == "tab-member"
        log.debug(f"Switched to {'Member' if self.member_only else 'All'} tab.")

        # Reset to the first page when changing tabs
        self.current_page = 0

        # Load data for the newly selected filter
        await self.load_or_refresh_data()

    @on(Pagination.PageChanged)
    async def handle_page_changed(self, event: Pagination.PageChanged) -> None:
        """Handle pagination page change."""
        log.debug(f"Page changed to: {event.page}")

        # Convert from 1-indexed (UI) to 0-indexed (API)
        self.current_page = event.page - 1
        await self.load_or_refresh_data()

    # If you want to add a refresh button:
    # @on(Button.Pressed, "#refresh-challenges-btn")
    # async def handle_refresh_button(self) -> None:
    #     """Handle refresh button press."""
    #     log.debug("Refresh button pressed.")
    #     await self.load_or_refresh_data()
-e 
-------- END OF FILE ui/widgets/challenge_list_widget.py --------

-------- START OF FILE ui/widgets/challenge_view.py --------
from rich.text import Text

from pixabit.helpers._logger import log
from pixabit.helpers._md_to_rich import MarkdownRenderer
from pixabit.helpers._rich import Text
from pixabit.helpers._textual import (
    Button,
    ComposeResult,
    Container,
    DataTable,
    Horizontal,
    Label,
    Markdown,
    Message,
    Screen,
    ScrollableContainer,
    Select,
    Static,
    TabbedContent,
    TabPane,
    Tree,
    Vertical,
    on,
    reactive,
)
from pixabit.models.challenge import Challenge, ChallengeList

MarkdownRenderer = MarkdownRenderer()


# Tree navigation implementation
class ChallengeTree(Tree):
    """Tree widget for navigating challenges with pagination support."""

    def __init__(self, label="Challenges", *children, id=None):
        super().__init__(label, *children, id=id)
        # Track the current filter and pagination state
        self.member_only = False
        self.current_page = 0
        self.total_pages = 10
        self._challenge_map = {}  # Maps node IDs to challenge objects

    def populate(self, challenges, member_only=False, current_page=0, total_pages=1):
        """Populate the tree with challenge nodes."""
        self.clear()
        self._challenge_map = {}

        # Add filter nodes
        filter_node = self.root.add("Filters", expand=True)

        # Create filter nodes with visual indication of current selection
        all_prefix = "► " if not member_only else "  "
        member_prefix = "► " if member_only else "  "

        all_node = filter_node.add(f"{all_prefix}All Challenges", data={"filter": "all"})
        member_node = filter_node.add(f"{member_prefix}My Challenges", data={"filter": "member"})

        # Add pagination info if there are multiple pages
        if total_pages > 1:
            page_info = f"Page {current_page + 1} of {total_pages}"
            pagination_node = self.root.add(page_info, expand=True)

            # Add navigation options if needed
            if current_page > 0:
                pagination_node.add("◀ Previous Page", data={"action": "prev_page"})

            if current_page < total_pages - 1:
                pagination_node.add("▶ Next Page", data={"action": "next_page"})

        # Add challenges
        challenges_node = self.root.add(f"Challenges ({len(challenges)})", expand=True)

        for challenge in challenges:
            # Format the node label with joined status indicator
            joined_indicator = "✓ " if getattr(challenge, "joined", False) else "  "
            challenge_name = getattr(challenge, "name", "Unknown Challenge")
            member_count = getattr(challenge, "memberCount", 0)
            prize = getattr(challenge, "prize", 0)

            # Create a formatted node label
            label = f"{joined_indicator}{challenge_name} ({member_count} members, {prize} gems)"

            # Add node and store the challenge object mapping
            node = challenges_node.add(label)
            node_id = id(node)
            self._challenge_map[node_id] = challenge

        # Expand the root to show filters and challenges
        self.root.expand()

    def get_challenge_from_node(self, node):
        """Get the challenge object associated with a node."""
        return self._challenge_map.get(id(node))


class ChallengeTasksPanel(Vertical):
    """Panel that displays challenge tasks as a markdown list with filtering options."""

    def __init__(self, id=None, challenge_service=None):
        super().__init__(id=id)
        self.challenge_service = challenge_service
        self.challenge_id = None
        self.loading = False
        self.task_type_filter = "all"
        self.sort_by = "priority"

    def compose(self) -> ComposeResult:
        """Compose the tasks panel."""
        with Horizontal(id="tasks-header"):
            yield Static("Challenge Tasks", id="tasks-title")

        # Using Markdown widget instead of DataTable
        yield ScrollableContainer(Markdown("*Select a challenge to view its tasks*", id="tasks-markdown"), id="tasks-container")

    async def load_tasks(self, challenge_id):
        """Load tasks for a challenge and display as markdown."""
        if not challenge_id or not self.challenge_service:
            return

        self.challenge_id = challenge_id
        self.loading = True
        tasks_markdown = self.query_one("#tasks-markdown")

        try:
            # Indicate loading
            tasks_markdown.update("*Loading tasks...*")

            # Fetch tasks from service
            tasks = await self.challenge_service.fetch_challenge_tasks(challenge_id)

            if tasks and len(tasks) > 0:
                # Filter tasks if needed
                if self.task_type_filter != "all":
                    tasks = [t for t in tasks if hasattr(t, "type") and t.type.lower() == self.task_type_filter.lower()]

                # Sort tasks
                if self.sort_by == "priority":
                    tasks = sorted(tasks, key=lambda t: getattr(t, "priority", 999), reverse=False)
                elif self.sort_by == "created":
                    tasks = sorted(tasks, key=lambda t: getattr(t, "createdAt", ""), reverse=True)

                # Generate markdown content
                md_content = "# Task List\n\n"

                for i, task in enumerate(tasks):
                    text = task.text if hasattr(task, "text") else "Unknown Task"
                    task_type = task.type if hasattr(task, "type") else "Unknown"
                    priority = task.priority if hasattr(task, "priority") else "1"
                    notes = task.notes if hasattr(task, "notes") else ""

                    # Format each task as a markdown list item
                    md_content += f"- **[{task_type}]** {text}"

                    # Add priority indicator with stars
                    if priority and str(priority).isdigit():
                        stars = "⭐" * int(priority)
                        md_content += f" {stars}"

                    # Add notes if they exist
                    if notes:
                        md_content += f"\n  - *{notes}*"

                    md_content += "\n"

                tasks_markdown.update(md_content)
            else:
                tasks_markdown.update("*No tasks found for this challenge*")

        except Exception as e:
            log.exception(f"Error loading challenge tasks: {e}")
            tasks_markdown.update(f"**Error loading tasks:** {str(e)}")

        finally:
            self.loading = False

    @on(Button.Pressed, "#refresh-tasks-btn")
    async def refresh_tasks(self):
        """Refresh the tasks list."""
        if self.challenge_id:
            await self.load_tasks(self.challenge_id)

    @on(Select.Changed, "#task-type-filter")
    def handle_type_filter_change(self, event: Select.Changed):
        """Handle task type filter change."""
        self.task_type_filter = event.value
        if self.challenge_id:
            self.refresh_tasks_async()

    @on(Select.Changed, "#task-sort")
    def handle_sort_change(self, event: Select.Changed):
        """Handle sort order change."""
        self.sort_by = event.value
        if self.challenge_id:
            self.refresh_tasks_async()

    def refresh_tasks_async(self):
        """Helper to refresh tasks asynchronously."""

        async def _refresh():
            await self.load_tasks(self.challenge_id)

        self.app.call_later(_refresh)


class ChallengeDetailPanel(ScrollableContainer):
    """Panel that displays challenge details with actions."""

    current_challenge = reactive(None)

    class JoinChallenge(Message):
        """Message for joining a challenge."""

        def __init__(self, challenge_id: str) -> None:
            self.challenge_id = challenge_id
            super().__init__()

    class LeaveChallenge(Message):
        """Message for leaving a challenge."""

        def __init__(self, challenge_id: str, keep: str) -> None:
            self.challenge_id = challenge_id
            self.keep = keep
            super().__init__()

    class LoadChallengeTasks(Message):
        """Message for loading challenge tasks."""

        def __init__(self, challenge_id: str) -> None:
            self.challenge_id = challenge_id
            super().__init__()

    def __init__(self, id=None, challenge_service=None):
        super().__init__(id=id)
        self.challenge_service = challenge_service

    def compose(self) -> ComposeResult:
        """Compose the challenge detail panel."""
        with Horizontal(id="challenge-detail-content"):
            yield ScrollableContainer(Markdown("", id="challenge-details"))

        with Horizontal(id="challenge-action-buttons", classes="action-row"):

            yield Button("Tasks", id="view-tasks-btn", variant="primary")
            yield Button("Join", id="join-challenge-btn", variant="success")

            with Horizontal(id="leave-challenge-container"):
                yield Button("Leave", id="leave-challenge-btn", variant="error")
                yield Select([("Keep", "keep-all"), ("Remove", "remove-all")], id="leave-option-select", value="keep-all")

    def watch_current_challenge(self, challenge: Challenge) -> None:
        """Watch for changes to the current_challenge property."""
        self._update_detail_view()

    def _update_detail_view(self) -> None:
        """Update the detail view with the current challenge data."""
        detail_content = self.query_one("#challenge-details")
        action_buttons = self.query_one("#challenge-action-buttons")

        if not self.current_challenge:
            detail_content.update("Select a challenge from the sidebar")
            action_buttons.add_class("hidden")
            return
        md_content = f"# {self.current_challenge.name}\n\n"
        # Format the challenge details with rich styling
        md_content += f"## Description\n\n {self.current_challenge.description}\n\n"
        md_content += f"**Owner:** {self.current_challenge.leader.name}\n"
        md_content += f" **Prize:** {self.current_challenge.prize}\n"
        md_content += f" **Members:** {self.current_challenge.member_count}"

        # Convert markdown to rich text
        detail_content.update(md_content)

        # Update action buttons based on joined status
        action_buttons.remove_class("hidden")
        join_btn = self.query_one("#join-challenge-btn")
        leave_container = self.query_one("#leave-challenge-container")

        if self.current_challenge.joined is False:
            join_btn.disabled = False
            leave_container.add_class("hidden")
        else:
            join_btn.disabled = True
            leave_container.remove_class("hidden")

    @on(Button.Pressed, "#join-challenge-btn")
    def handle_join_challenge(self):
        """Handle join challenge button press."""
        if self.current_challenge:
            self.post_message(self.JoinChallenge(self.current_challenge.id))

    @on(Button.Pressed, "#leave-challenge-btn")
    def handle_leave_challenge(self):
        """Handle leave challenge button press."""
        if self.current_challenge:
            keep_option = self.query_one("#leave-option-select").value
            self.post_message(self.LeaveChallenge(self.current_challenge.id, keep_option))

    @on(Button.Pressed, "#view-tasks-btn")
    def handle_view_tasks(self):
        """Handle view tasks button press."""
        if self.current_challenge:
            self.post_message(self.LoadChallengeTasks(self.current_challenge.id))


class ChallengeView(Container):
    """Main container that combines sidebar navigation and content panels."""

    current_challenge_id = reactive(None)
    loading = reactive(False)

    def __init__(self, id=None, challenge_service=None):
        super().__init__(id=id)
        self.challenge_service = challenge_service
        self.member_only = False
        self.current_page = 0
        self.total_pages = 1

    def compose(self) -> ComposeResult:
        """Compose the main challenge view."""
        with Horizontal(id="challenge-layout"):
            # Sidebar with tree navigation
            with Vertical(id="sidebar-container"):
                with Horizontal(id="sidebar-header"):
                    yield Static("Challenge Explorer", id="sidebar-title")
                #                    yield Button("↻", id="refresh-challenges-btn", variant="primary")

                yield ChallengeTree(id="challenge-tree")

            # Content panels
            with Vertical(id="content-container"):
                # Challenge details panel
                yield ChallengeDetailPanel(id="challenge-detail-panel", challenge_service=self.challenge_service)

                # Tasks panel (initially hidden)
                with Vertical(id="tasks-panel", classes="hidden"):
                    yield ChallengeTasksPanel(id="challenge-tasks-panel", challenge_service=self.challenge_service)

    async def on_mount(self) -> None:
        """Initialize the view on mount."""
        await self.load_challenges()

    async def load_challenges(self):
        """Load challenges from the service."""
        if not self.challenge_service:
            log.warning("No challenge service available")
            return

        if self.loading:
            return  # Prevent duplicate loads

        self.loading = True
        tree = self.query_one("#challenge-tree")

        try:
            # Fetch challenges using the service
            challenge_list = await self.challenge_service.fetch_challenges(member_only=self.member_only, page=self.current_page)

            # Extract and store pagination info
            self.total_pages = 50

            # Update the tree with challenges
            challenges = getattr(challenge_list, "challenges", [])
            tree.populate(challenges=challenges, member_only=self.member_only, current_page=self.current_page, total_pages=self.total_pages)

        except Exception as e:
            log.exception(f"Error loading challenges: {e}")
            # Show error in the tree
            tree.clear()
            tree.root.add("Error loading challenges")

        finally:
            self.loading = False

    @on(Tree.NodeSelected)
    async def handle_tree_node_selected(self, event: Tree.NodeSelected):
        """Handle tree node selection."""
        # Check for filter selection
        if event.node.data and "filter" in event.node.data:
            filter_type = event.node.data["filter"]

            # Switch the filter and reload
            if filter_type == "all" and self.member_only:
                self.member_only = False
                self.current_page = 0
                await self.load_challenges()
            elif filter_type == "member" and not self.member_only:
                self.member_only = True
                self.current_page = 0
                await self.load_challenges()

        # Check for pagination actions
        elif event.node.data and "action" in event.node.data:
            action = event.node.data["action"]

            if action == "prev_page" and self.current_page > 0:
                self.current_page -= 1
                await self.load_challenges()
            elif action == "next_page" and self.current_page < self.total_pages - 1:
                self.current_page += 1
                await self.load_challenges()

        # Check for challenge selection
        else:
            # Get the challenge from the tree
            tree = self.query_one("#challenge-tree")
            challenge = tree.get_challenge_from_node(event.node)

            if challenge:
                # Update the detail panel
                detail_panel = self.query_one("#challenge-detail-panel")
                detail_panel.current_challenge = challenge

                # Hide the tasks panel
                tasks_panel = self.query_one("#tasks-panel")
                tasks_panel.add_class("hidden")

    @on(Button.Pressed, "#refresh-challenges-btn")
    async def handle_refresh(self):
        """Handle refresh button press."""
        await self.load_challenges()

    @on(ChallengeDetailPanel.JoinChallenge)
    async def handle_join_challenge(self, event: ChallengeDetailPanel.JoinChallenge):
        """Handle join challenge message."""
        if not self.challenge_service:
            return

        try:
            # Call the service to join the challenge
            result = await self.challenge_service.join_challenge(event.challenge_id)

            if result:
                # Refresh the challenges to update the UI
                await self.load_challenges()

                # Update the detail panel - need to get the updated challenge
                challenge = await self.challenge_service.fetch_challenge(event.challenge_id)
                if challenge:
                    detail_panel = self.query_one("#challenge-detail-panel")
                    detail_panel.current_challenge = challenge

        except Exception as e:
            log.exception(f"Error joining challenge: {e}")

    @on(ChallengeDetailPanel.LeaveChallenge)
    async def handle_leave_challenge(self, event: ChallengeDetailPanel.LeaveChallenge):
        """Handle leave challenge message."""
        if not self.challenge_service:
            return

        try:
            # Call the service to leave the challenge
            result = await self.challenge_service.leave_challenge(event.challenge_id, keep=event.keep == "keep-all")

            if result:
                # Refresh the challenges to update the UI
                await self.load_challenges()

                # Update the detail panel
                challenge = await self.challenge_service.fetch_challenge(event.challenge_id)
                if challenge:
                    detail_panel = self.query_one("#challenge-detail-panel")
                    detail_panel.current_challenge = challenge

        except Exception as e:
            log.exception(f"Error leaving challenge: {e}")

    @on(ChallengeDetailPanel.LoadChallengeTasks)
    async def handle_load_tasks(self, event: ChallengeDetailPanel.LoadChallengeTasks):
        """Handle load tasks message."""
        tasks_panel = self.query_one("#tasks-panel")
        tasks_panel.remove_class("hidden")

        tasks_panel_widget = self.query_one("#challenge-tasks-panel")
        await tasks_panel_widget.load_tasks(event.challenge_id)


# Custom CSS for the widgets


# Example usage in a screen
class ChallengeScreen(Screen):
    """Example screen using the ChallengeView widget."""

    def __init__(self, challenge_service=None):
        super().__init__()
        self.challenge_service = challenge_service

    def compose(self) -> ComposeResult:
        """Compose the screen."""
        yield ChallengeView(challenge_service=self.challenge_service)
-e 
-------- END OF FILE ui/widgets/challenge_view.py --------

-------- START OF FILE ui/widgets/challenge_widget.py --------
"""Challenge widgets for the Habitica client UI."""

from typing import Any, Callable, Dict, List, Optional, Union

from pixabit.helpers._logger import log
from pixabit.helpers._textual import (
    Button,
    ComposeResult,
    Container,
    DataTable,
    Horizontal,
    Input,
    Label,
    Message,
    OptionList,
    ScrollableContainer,
    Select,
    Static,
    Vertical,
    on,
    reactive,
)
from pixabit.models.challenge import Challenge
from pixabit.ui.widgets.challenge_detail_panel import ChallengeDetailPanel
from pixabit.ui.widgets.challenge_list_widget import ChallengeListWidget


class ChallengeContainer(Container):
    """Container widget that holds both challenge list and detail panel."""

    class ChallengeInteraction(Message):
        """Message for challenge interactions."""

        def __init__(self, challenge_id: str, action: str, data: Optional[Dict[str, Any]] = None) -> None:
            self.challenge_id = challenge_id
            self.action = action
            self.data = data or {}
            super().__init__()

    def __init__(self, id: str = None, challenge_service=None, on_data_changed: Optional[Callable] = None) -> None:
        """Initialize the challenge tab container.

        Args:
            id: Widget ID.
            challenge_service: The challenge service for API interactions.
            on_data_changed: Callback when challenge data changes.
        """
        super().__init__(id=id)
        self.challenge_service = challenge_service
        self.on_data_changed = on_data_changed
        self._challenge_list = None
        self._challenge_detail = None

    def compose(self) -> ComposeResult:
        """Compose the layout with challenge list and detail panel."""
        yield Static("Challenges", classes="tab-header")

        with Horizontal(id="challenges-content-container"):
            yield ChallengeListWidget(id="challenge-list-widget", challenge_service=self.challenge_service)
            yield ChallengeDetailPanel(id="challenge-detail-panel", challenge_service=self.challenge_service)

    async def on_mount(self) -> None:
        """Configure the widget when it's mounted."""
        # Get references to child widgets
        self._challenge_list = self.query_one(ChallengeListWidget)
        self._challenge_detail = self.query_one(ChallengeDetailPanel)

        # Initial data load
        await self._challenge_list.load_or_refresh_data()

    async def refresh_data(self) -> None:
        """Refresh the challenge data."""
        await self._challenge_list.load_or_refresh_data()

        # If there's a challenge selected in the detail panel, refresh it too
        if self._challenge_detail.current_challenge:
            challenge_id = self._challenge_detail.current_challenge.id
            if challenge_id and self.challenge_service:
                updated_challenge = await self.challenge_service.fetch_challenge_details(challenge_id)
                self._challenge_detail.current_challenge = updated_challenge

    # Message handlers

    @on(ChallengeListWidget.ViewChallengeDetailsRequest)
    async def handle_view_challenge_details(self, message: ChallengeListWidget.ViewChallengeDetailsRequest) -> None:
        # In ChallengeContainer.handle_view_challenge_details
        log.debug(f"Received ViewChallengeDetailsRequest for ID: {message.challenge_id}")

        # ... rest of the fallback logic
        """Handle request to view challenge details."""
        if not self.challenge_service:
            log.warning("No challenge service available to fetch challenge details")
            # Fallback to searching in list data
            for challenge in self._challenge_list.challenges_data:
                if challenge.id == message.challenge_id:
                    self._challenge_detail.current_challenge = challenge
                    return
            return

        try:
            # First, check if we have it in the cached list
            challenge = None
            for c in self._challenge_list.challenges_data:
                if c.id == message.challenge_id:
                    challenge = c
                    break

            # If not found or we want full details, fetch from API
            if not challenge or not hasattr(challenge, "description"):
                challenge = await self.challenge_service.fetch_challenge_details(message.challenge_id)

            self._challenge_detail.current_challenge = challenge
        except Exception as e:
            log.error(f"Error fetching challenge details: {e}")
            # Try to get from challenge list if API call fails
            for challenge in self._challenge_list.challenges_data:
                if challenge.id == message.challenge_id:
                    self._challenge_detail.current_challenge = challenge
                    return
            self._challenge_detail.current_challenge = None

    @on(ChallengeDetailPanel.ViewChallengeTasks)
    async def handle_view_challenge_tasks(self, message: ChallengeDetailPanel.ViewChallengeTasks) -> None:
        """Handle viewing challenge tasks."""
        # No need to do anything here - the detail panel handles loading tasks itself
        self.post_message(self.ChallengeInteraction(message.challenge_id, "view_tasks"))

    @on(ChallengeDetailPanel.JoinChallenge)
    async def handle_join_challenge(self, message: ChallengeDetailPanel.JoinChallenge) -> None:
        """Handle joining a challenge."""
        if not self.challenge_service:
            log.warning("No challenge service available to join challenge")
            return

        try:
            # Join the challenge via the service
            success = await self.challenge_service.join_challenge(message.challenge_id)

            if success:
                # Notify that data has changed
                if self.on_data_changed:
                    await self.on_data_changed({"action": "join", "challenge_id": message.challenge_id})

                # Fetch updated challenge details
                updated_challenge = await self.challenge_service.fetch_challenge_details(message.challenge_id)
                self._challenge_detail.current_challenge = updated_challenge

                # Refresh challenge list
                await self.refresh_data()
        except Exception as e:
            log.error(f"Error joining challenge: {e}")

    @on(ChallengeDetailPanel.LeaveChallenge)
    async def handle_leave_challenge(self, message: ChallengeDetailPanel.LeaveChallenge) -> None:
        """Handle leaving a challenge."""
        if not self.challenge_service:
            log.warning("No challenge service available to leave challenge")
            return

        try:
            # Leave the challenge via the service
            success = await self.challenge_service.leave_challenge(message.challenge_id, message.keep)

            if success:
                # Notify that data has changed
                if self.on_data_changed:
                    await self.on_data_changed({"action": "leave", "challenge_id": message.challenge_id, "keep": message.keep})

                # Fetch updated challenge details
                updated_challenge = await self.challenge_service.fetch_challenge_details(message.challenge_id)
                self._challenge_detail.current_challenge = updated_challenge

                # Refresh challenge list
                await self.refresh_data()
        except Exception as e:
            log.error(f"Error leaving challenge: {e}")

    @on(ChallengeDetailPanel.CompleteChallenge)
    async def handle_complete_challenge(self, message: ChallengeDetailPanel.CompleteChallenge) -> None:
        """Handle completing a challenge."""
        # This appears to be a placeholder in your original code
        # We'll keep it as such, but with better error handling
        if not self.challenge_service:
            log.warning("No challenge service available to complete challenge")
            return

        try:
            # This method doesn't exist in our service yet, so we'll log a warning
            log.warning(f"Complete challenge functionality not implemented for challenge {message.challenge_id}")

            # Notify that data has changed (if you implement this later)
            if self.on_data_changed:
                await self.on_data_changed({"action": "complete", "challenge_id": message.challenge_id})
        except Exception as e:
            log.error(f"Error completing challenge: {e}")

    @on(ChallengeDetailPanel.EditChallenge)
    async def handle_edit_challenge(self, message: ChallengeDetailPanel.EditChallenge) -> None:
        """Handle editing a challenge."""
        if not self.challenge_service:
            log.warning("No challenge service available to edit challenge")
            return

        # In a real implementation, you would show a form/modal to edit the challenge
        log.info(f"Edit challenge request for challenge {message.challenge_id} - not implemented yet")

        # For now, just notify that we would edit the challenge
        if self.on_data_changed:
            await self.on_data_changed({"action": "edit", "challenge_id": message.challenge_id})
-e 
-------- END OF FILE ui/widgets/challenge_widget.py --------

-------- START OF FILE ui/widgets/help_modal.py --------
from textual import events
from textual.containers import Vertical
from textual.screen import ModalScreen
from textual.widgets import Static


class HelpModal(ModalScreen):
    """Modal screen to show keyboard help."""

    def compose(self):
        yield Vertical(
            Static("**Keyboard Shortcuts**", id="help-title"),
            *(Static(f"[b]{key}[/b]: {desc}", id="help-item") for key, _, desc in self.app.get_bindings_info()),
            id="help-list",
        )

    def on_key(self, event: events.Key) -> None:
        """Close help on Escape."""
        if event.key == "escape":
            self.dismiss()
-e 
-------- END OF FILE ui/widgets/help_modal.py --------

-------- START OF FILE ui/widgets/main_panel.py --------
from typing import Any, Dict, List, Optional

from textual import on
from textual.app import ComposeResult
from textual.binding import Binding
from textual.containers import Container, Horizontal
from textual.message import Message
from textual.widget import Widget
from textual.widgets import Static, TabbedContent, TabPane

from pixabit.helpers._logger import log
from pixabit.models.task import Task
from pixabit.ui.widgets.table_detail_panel import (
    CompleteTask,
    DeleteTask,
    EditTask,
    ScoreTaskDetail,
    TaskDetailPanel,
)
from pixabit.ui.widgets.table_list_panel import (
    ScoreTaskRequest,
    TaskListWidget,
    ViewTaskDetailsRequest,
)


class TaskView(Widget):
    """A container widget that holds a tabbed task list and a detail panel side by side.
    This widget handles the communication between the TaskListWidget and TaskDetailPanel.
    """

    BINDINGS = [
        Binding(key="c", action="complete_task", description="Complete task"),
        Binding(key="e", action="edit_task", description="Edit task"),
        Binding(key="d", action="delete_task", description="Delete task"),
    ]

    def __init__(
        self,
        task_service=None,
        id: str = "task-view",
        **kwargs,
    ):
        super().__init__(id=id, **kwargs)
        self.task_service = task_service
        self.tag_colors = {}
        self._last_selected_task_id = None

    def compose(self) -> ComposeResult:
        """Compose the TaskView with a task list and detail panel."""
        with Horizontal(id="task-view-container"):
            with Container(id="task-list-container"):
                with TabbedContent(id="task-tabs"):
                    with TabPane("Todos", id="todo-tab"):
                        yield TaskListWidget(task_type="todo", id="todo-list")
                    with TabPane("Dailies", id="daily-tab"):
                        yield TaskListWidget(task_type="daily", id="daily-list")
                    with TabPane("Habits", id="habit-tab"):
                        yield TaskListWidget(task_type="habit", id="habit-list")
                    with TabPane("Rewards", id="reward-tab"):
                        yield TaskListWidget(task_type="reward", id="reward-list")
                    with TabPane("All", id="all-tab"):
                        yield TaskListWidget(task_type="all", id="all-list")

            # Task detail panel on the right
            yield TaskDetailPanel(id="task-detail-panel")

    async def on_mount(self) -> None:
        """Initialize after widget is mounted."""
        # Get all task list widgets and detail panel
        task_lists = self.query(TaskListWidget)
        detail_panel = self.query_one(TaskDetailPanel)

        # Initialize tag colors in all widgets
        self.tag_colors = await self._get_tag_colors()
        for task_list in task_lists:
            task_list.tag_colors = self.tag_colors
        detail_panel.tag_colors = self.tag_colors

        # Initialize task lists with data
        for task_list in task_lists:
            await task_list.load_or_refresh_data()

    async def _get_tag_colors(self) -> Dict[str, str]:
        """Get tag colors from the data store."""
        if hasattr(self.app, "datastore") and self.app.datastore:
            try:
                return await self.app.run_in_thread(self.app.datastore.get_tag_colors)
            except Exception as e:
                log.error(f"Error fetching tag colors: {e}")
        return {}

    async def refresh_all_task_lists(self) -> None:
        """Refresh all task list widgets."""
        task_lists = self.query(TaskListWidget)
        for task_list in task_lists:
            await task_list.load_or_refresh_data()

    # Event handlers for communication between widgets

    async def handle_view_task_details(self, event: ViewTaskDetailsRequest) -> None:
        """Handle request to view task details."""
        log.info(f"TaskView: Request to view details for task {event.task_id}")
        detail_panel = self.query_one(TaskDetailPanel)

        try:
            # Store the last selected task ID
            self._last_selected_task_id = event.task_id

            # Get task from data store and update detail panel
            task = await self.app.run_in_thread(self.app.datastore.get_task_by_id, event.task_id)

            if task:
                detail_panel.current_task = task
            else:
                detail_panel.current_task = None
                log.warning(f"Task with ID {event.task_id} not found.")
        except Exception as e:
            log.error(f"Error fetching task details: {e}")
            detail_panel.current_task = None

    async def handle_score_task(self, event: Message) -> None:
        """Handle request to score a task."""
        task_id = event.task_id
        direction = event.direction
        log.info(f"TaskView: Scoring task {task_id} {direction}")

        try:
            # Score the task
            result = await self.app.run_in_thread(self.app.datastore.score_task, task_id, direction)

            if result:
                # Refresh all task lists and update detail panel
                await self.refresh_all_task_lists()

                # Update detail panel if it's showing the scored task
                detail_panel = self.query_one(TaskDetailPanel)
                if detail_panel.current_task and getattr(detail_panel.current_task, "id", None) == task_id:
                    task = await self.app.run_in_thread(self.app.datastore.get_task_by_id, task_id)
                    detail_panel.current_task = task
            else:
                log.warning(f"Failed to score task {task_id}")
        except Exception as e:
            log.error(f"Error scoring task: {e}")

    async def handle_complete_task(self, event: CompleteTask) -> None:
        """Handle request to complete a task."""
        task_id = event.task_id
        log.info(f"TaskView: Completing task {task_id}")

        try:
            # Complete the task
            result = await self.app.run_in_thread(self.app.datastore.complete_task, task_id)

            if result:
                # Refresh all task lists and clear detail panel
                await self.refresh_all_task_lists()

                # Clear detail panel
                detail_panel = self.query_one(TaskDetailPanel)
                detail_panel.current_task = None
            else:
                log.warning(f"Failed to complete task {task_id}")
        except Exception as e:
            log.error(f"Error completing task: {e}")

    async def handle_delete_task(self, event: DeleteTask) -> None:
        """Handle request to delete a task."""
        task_id = event.task_id
        log.info(f"TaskView: Deleting task {task_id}")

        try:
            # Delete the task
            result = await self.app.run_in_thread(self.app.datastore.delete_task, task_id)

            if result:
                # Refresh all task lists and clear detail panel
                await self.refresh_all_task_lists()

                # Clear detail panel
                detail_panel = self.query_one(TaskDetailPanel)
                detail_panel.current_task = None
            else:
                log.warning(f"Failed to delete task {task_id}")
        except Exception as e:
            log.error(f"Error deleting task: {e}")

    async def handle_edit_task(self, event: EditTask) -> None:
        """Handle request to edit a task."""
        task_id = event.task_id
        log.info(f"TaskView: Edit request for task {task_id}")

        # This would be implemented when you have an edit task dialog
        log.info("Edit task feature not yet implemented")

    # Actions for key bindings

    async def action_complete_task(self) -> None:
        """Action to complete the currently selected task."""
        detail_panel = self.query_one(TaskDetailPanel)
        if detail_panel.current_task:
            task_id = getattr(detail_panel.current_task, "id", None)
            if task_id:
                await self.handle_complete_task(CompleteTask(task_id))

    async def action_edit_task(self) -> None:
        """Action to edit the currently selected task."""
        detail_panel = self.query_one(TaskDetailPanel)
        if detail_panel.current_task:
            task_id = getattr(detail_panel.current_task, "id", None)
            if task_id:
                await self.handle_edit_task(EditTask(task_id))

    async def action_delete_task(self) -> None:
        """Action to delete the currently selected task."""
        detail_panel = self.query_one(TaskDetailPanel)
        if detail_panel.current_task:
            task_id = getattr(detail_panel.current_task, "id", None)
            if task_id:
                await self.handle_delete_task(DeleteTask(task_id))
-e 
-------- END OF FILE ui/widgets/main_panel.py --------

-------- START OF FILE ui/widgets/pagination.py --------
from rich.text import Text
from textual import events
from textual.containers import Horizontal
from textual.message import Message
from textual.widget import Widget
from textual.widgets import Button, Static


class Pagination(Horizontal):
    """A custom pagination widget for Textual applications."""

    class PageChanged(Message):
        """Message sent when the page is changed."""

        def __init__(self, page: int) -> None:
            """Initialize with the new page number.

            Args:
                page: The new page number (1-indexed)
            """
            self.page = page
            super().__init__()

    def __init__(
        self,
        page_count: int = 1,
        current_page: int = 1,
        page_size: int = 10,
        id: str = None,
        classes: str = None,
    ) -> None:
        """Initialize the pagination widget.

        Args:
            page_count: Total number of pages
            current_page: Currently active page (1-indexed)
            page_size: Number of items per page
            id: The optional ID of the widget
            classes: The optional CSS classes of the widget
        """
        super().__init__(id=id, classes=classes)
        self.page_count = max(1, page_count)
        self._current_page = min(max(1, current_page), self.page_count)
        self.page_size = page_size

    @property
    def current_page(self) -> int:
        """Get the current page (1-indexed)."""
        return self._current_page

    @current_page.setter
    def current_page(self, value: int) -> None:
        """Set the current page, ensuring it's within valid range.

        Args:
            value: The new page number (1-indexed)
        """
        old_value = self._current_page
        self._current_page = min(max(1, value), self.page_count)

        if self._current_page != old_value:
            self.post_message(self.PageChanged(self._current_page))
            self.refresh()

    @property
    def page_count(self) -> int:
        """Get the total number of pages."""
        return self._page_count

    @page_count.setter
    def page_count(self, value: int) -> None:
        """Set the total number of pages.

        Args:
            value: The new page count
        """
        self._page_count = max(1, value)
        # Ensure current page is still valid
        if hasattr(self, "_current_page"):
            self._current_page = min(self._current_page, self._page_count)
        self.refresh()

    def compose(self):
        """Compose the pagination widget with navigation buttons."""
        yield Button("«", id="page-first", disabled=self.current_page == 1)
        yield Button("‹", id="page-prev", disabled=self.current_page == 1)

        yield Static(f"Page {self.current_page} of {self.page_count}", id="page-info")

        yield Button("›", id="page-next", disabled=self.current_page == self.page_count)
        yield Button("»", id="page-last", disabled=self.current_page == self.page_count)

    def on_button_pressed(self, event: Button.Pressed) -> None:
        """Handle button press events for pagination navigation.

        Args:
            event: The button pressed event
        """
        button_id = event.button.id

        if button_id == "page-first":
            self.current_page = 1
        elif button_id == "page-prev":
            self.current_page = self.current_page - 1
        elif button_id == "page-next":
            self.current_page = self.current_page + 1
        elif button_id == "page-last":
            self.current_page = self.page_count
-e 
-------- END OF FILE ui/widgets/pagination.py --------

-------- START OF FILE ui/widgets/sidebar_stats.py --------
# pixabit/ui/widgets/sidebar_stats.py

from typing import Any, Dict

from textual.app import ComposeResult
from textual.containers import Horizontal, Vertical
from textual.reactive import reactive
from textual.widgets import Label, ProgressBar, Static

from pixabit.helpers._logger import log
from pixabit.models.user import User


class SidebarStats(Vertical):
    """Widget that displays detailed user stats in the sidebar."""

    def __init__(self, name: str | None = None, id: str | None = None, classes: str | None = None):
        """Inicializa el widget de estadísticas de la barra lateral."""
        super().__init__(name=name, id=id, classes=classes)
        self._status_label = None

    # Reactive variables
    user_class: reactive[str] = reactive("Unknown")
    is_sleeping: reactive[bool] = reactive(False)
    quest_progress: reactive[float] = reactive(0.0)
    quest_name: reactive[str] = reactive("")
    total_damage: reactive[float] = reactive(0.0)
    day_start: reactive[str] = reactive("00:00")
    needs_cron: reactive[bool] = reactive(False)
    last_log: reactive[str] = reactive("")

    def on_mount(self):
        """Se ejecuta cuando el widget es montado."""
        self._status_label = self.query_one("#api-status-label", Label)
        self._status_label.update("Esperando datos...")

    def update_sidebar_stats(self, user_data: User | None, quest_data: Dict[str, Any] | None = None) -> None:
        """Updates sidebar stats with user and quest data."""
        if not user_data:
            self.user_class = "Unknown"
            self.is_sleeping = False
            self.quest_progress = 0.0
            self.quest_name = "No Quest"
            self.total_damage = 0.0
            self.day_start = "00:00"
            self.needs_cron = False
            self.last_log = "No activity logs available"
            self.username = ""
            self.composed_user = ""
            return

        # Sleep status with emoji
        try:
            self.is_sleeping = getattr(user_data, "is_sleeping", False)
        except Exception:
            self.is_sleeping = False

        # Quest data
        if quest_data:
            self.quest_name = quest_data.get("title", "Unknown Quest")
            # Calculate quest progress as a percentage
            progress = quest_data.get("progress", 0)
            total = quest_data.get("progressNeeded")
            self.quest_progress = (progress / total) if total > 0 else 0
        else:
            self.quest_name = "No quest"
            self.quest_progress = 0.0

        # Total damage - assume we have a method or attribute for this
        self.total_damage = getattr(user_data, "total_damage", 0.0)

        # Day start time - format: "HH:MM"
        preferences = user_data.preferences
        if preferences:
            day_start = preferences.day_start
            self.day_start = f"{day_start:02d}:00"
        else:
            self.day_start = "00:00"

        # Needs cron check
        self.needs_cron = getattr(user_data, "needs_cron", False)
        self.username = getattr.user_data.username
        self.user_class = getattr.user_data.klass

        # Emojis
        class_emoji = {"warrior": "🗡️", "wizard": "🧙", "healer": "💚", "rogue": "⚔️", "": "👤"}.get(self.user_class.lower(), "👤")
        self.composed_user = f"{class_emoji} [b]{self.username}[/b]"

        # Last activity log (simplified - in real app, get from actual logs)
        # In a real implementation, you'd fetch this from user history or activity logs

        # Update widgets

    def watch_is_sleeping(self, sleeping: bool) -> None:
        """Updates the sleep status label when sleep state changes."""
        try:
            emoji = "💤" if sleeping else "👁️‍🗨️"
            status = "Sleeping" if sleeping else "Awake"
            self.query_one("#sleep-status", Label).update(f"{self.composed_user} {emoji} {status} ")
        except Exception as e:
            log.error(f"Error updating sleep status: {e}")

    def watch_quest_progress(self, progress: float) -> None:
        """Updates the quest progress bar when progress changes."""
        try:
            progress_bar = self.query_one("#quest-progress", ProgressBar)
            progress_bar.update(progress)
        except Exception as e:
            log.error(f"Error updating quest progress: {e}")

    def watch_quest_name(self, name: str) -> None:
        """Updates the quest name when it changes."""
        try:
            self.query_one("#quest-name", Label).update(f"🐣 {name} ")
        except Exception as e:
            log.error(f"Error updating quest name: {e}")

    def watch_user_class(self, class_name: str) -> None:
        """Updates the user class label when it changes."""
        try:
            self.query_one("#user-class", Label).update(class_name)
        except Exception as e:
            log.error(f"Error updating user class: {e}")

    def watch_total_damage(self, damage: float) -> None:
        """Updates the total damage when it changes."""
        try:
            self.query_one("#total-damage", Label).update(f"💥 {damage:.1f} ")
        except Exception as e:
            log.error(f"Error updating total damage: {e}")

    def watch_day_start(self, time: str) -> None:
        """Updates the day start time when it changes."""
        try:
            self.query_one("#day-start-time", Label).update(f"🌙 {time} ")
        except Exception as e:
            log.error(f"Error updating day start time: {e}")

    def watch_needs_cron(self, needs_cron: bool) -> None:
        """Updates the cron status when it changes."""
        try:
            label = self.query_one("#needs-cron", Label)
            if needs_cron:
                label.update("⚠️ Cron needed! ")
                label.add_class("needs-action")
            else:
                label.update("🆗 Cron ")
                label.remove_class("needs-action")
        except Exception as e:
            log.error(f"Error updating cron status: {e}")

    # def watch_last_log(self, log_text: str) -> None:
    #     """Updates the last activity log when it changes."""
    #     try:
    #         self.query_one("#last-log", Static).update(last_action)
    #     except Exception as e:
    #         self.update_status(f"Error updating last log: {e}")

    def update_status(self, message: str, status_class: str = "") -> None:
        """Updates the status label with a message and optional CSS class."""
        try:
            if self._status_label is None:
                self._status_label = self.query_one("#api-status-label", Label)

            # Limpiar clases anteriores
            self._status_label.remove_class("loading", "success", "error", "warning")

            # Añadir nueva clase si se proporciona
            if status_class:
                self._status_label.add_class(status_class)
            self._status_label.update(message)
        except Exception as e:
            log.error(f"Failed to update status: {e}")

    def compose(self) -> ComposeResult:
        """Create sidebar components."""
        with Horizontal(id="sidebar"):
            yield Label("", id="sleep-status", classes="stat-label")
            yield Label("", id="quest-name", classes="stat-label")
            yield Label("", id="total-damage", classes="stat-label")
            yield Label("", id="day-start-time", classes="stat-label")
            yield Label("", id="needs-cron", classes="stat-label")
            yield Label("", id="api-status-label")
-e 
-------- END OF FILE ui/widgets/sidebar_stats.py --------

-------- START OF FILE ui/widgets/sleep_toggle.py --------
# pixabit/ui/widgets/sleep_toggle.py
import asyncio
from typing import Any, Callable, Dict, Optional

from textual.containers import Horizontal
from textual.widgets import Label, Static, Switch

from pixabit.api.client import HabiticaClient
from pixabit.helpers._logger import log
from pixabit.models.user import User


class SleepToggle(Horizontal):
    """Widget for toggling the user's sleep mode in Habitica."""

    DEFAULT_CSS = """
    SleepToggle {
        height: auto;
        padding: 1;
        margin-top: 1;
        background: $panel;
        border: round $accent;
        align: center middle;
    }

    SleepToggle #sleep-label {
        margin-right: 0;
    }
    """

    def __init__(
        self,
        api_client: Optional[HabiticaClient] = None,
        status_update_callback: Optional[Callable[[str, str], None]] = None,
        on_data_changed: Optional[Callable[[Dict[str, Any]], Any]] = None,
        name: Optional[str] = None,
        id: Optional[str] = None,
        classes: Optional[str] = None,
    ):
        """Initialize the SleepToggle widget.

        Args:
            api_client: HabiticaClient for API calls
            status_update_callback: Callback function to update status messages
            on_data_changed: Callback called when data changes to notify parent components
            name: Widget name
            id: Widget ID
            classes: CSS classes
        """
        super().__init__(name=name, id=id, classes=classes)
        self.api_client = api_client
        self.status_update_callback = status_update_callback
        self.on_data_changed = on_data_changed

        # Flag to prevent toggle loop
        self._updating_ui = False
        # Flag to track the last known sleep state
        self._last_sleep_state = False

    def compose(self):
        """Create child widgets."""
        yield Label("Sleep Mode:", id="sleep-label")
        yield Switch(value=False, id="sleep-toggle")

    def update_sleep_state(self, user: Optional[User]) -> None:
        """Update the switch state based on the user's sleep state.

        Args:
            user: User object containing sleep state
        """
        self._updating_ui = True
        try:
            sleep_toggle = self.query_one("#sleep-toggle", Switch)

            if user is None:
                sleep_toggle.disabled = True
                return

            try:
                is_sleeping = getattr(user, "is_sleeping", False)
                sleep_toggle.value = is_sleeping
                sleep_toggle.disabled = False
                self._last_sleep_state = is_sleeping
            except Exception as e:
                log.error(f"Error updating sleep toggle: {e}")
                sleep_toggle.disabled = True
        finally:
            self._updating_ui = False

    async def on_switch_changed(self, event: Switch.Changed) -> None:
        """Handle switch toggle events."""
        if event.switch.id == "sleep-toggle" and not self._updating_ui:
            # Only proceed if it's a genuine user action and state has changed
            if event.value != self._last_sleep_state:
                await self._toggle_sleep(event.value)

    async def _toggle_sleep(self, sleep_value: bool) -> None:
        """Toggle user sleep state."""
        if not self.api_client:
            self._update_status("Error: API client not initialized", "error")
            return

        self._update_status("Toggling sleep state...", "loading")

        try:
            # Call API to toggle sleep
            # The API call returns the CURRENT sleep state after toggle, not a success flag
            new_sleep_state = await self.api_client.toggle_user_sleep()
            log.info(f"API returned sleep state: {new_sleep_state}")

            # Check if the toggle was successful by comparing with desired state
            if new_sleep_state == sleep_value:
                self._update_status("Sleep state changed successfully", "success")

                # Update our cached sleep state to match the API response
                self._last_sleep_state = new_sleep_state

                # Call the callback to notify the parent component using new data_changed pattern
                if self.on_data_changed:
                    await self.on_data_changed({"action": "sleep_toggled", "new_value": new_sleep_state})
            else:
                self._update_status("Failed to toggle sleep state", "error")
                # Reset switch to match the actual state returned by API
                self._reset_switch_value(new_sleep_state)

        except Exception as e:
            self._update_status(f"Error: {e}", "error")
            # Reset switch to previous state since we don't know the current state
            self._reset_switch_value(not sleep_value)

    def _reset_switch_value(self, value: bool) -> None:
        """Reset the switch value without triggering events."""
        self._updating_ui = True
        try:
            switch = self.query_one("#sleep-toggle", Switch)
            switch.value = value
        finally:
            self._updating_ui = False

    def _update_status(self, message: str, status_class: str = "") -> None:
        """Updates the status via callback."""
        if self.status_update_callback:
            self.status_update_callback(message, status_class)
-e 
-------- END OF FILE ui/widgets/sleep_toggle.py --------

-------- START OF FILE ui/widgets/stats_count.py --------
# pixabit/ui/widgets/stats_count.py

from textual.app import ComposeResult
from textual.containers import Horizontal, Vertical
from textual.reactive import reactive
from textual.widgets import Digits, Label

from pixabit.helpers._logger import log
from pixabit.models.user import User


class StatsCount(Horizontal):
    """Widget that displays user stats in a horizontal layout."""

    DEFAULT_CSS = """
    StatsCount {
        /* Layout */
        height: auto;
        padding: 1 1;
        background: $panel;
        align: center middle;
        width: 100%;

        /* Child Alignment */
        &> Vertical {
            width: auto;
            height: auto;
            margin: 0 1;
            align: center top;
        }

        Label { width: auto; text-style: bold; color: $text; margin-bottom: 1; }
        Digits { width: auto; }

        /* Stat-specific Colors */
        #lvl-container Digits { color: $warning; }
        #mp-container Digits { color: $primary; }
        #hp-container Digits { color: $error; }
        #gp-container Digits { color: $warning-darken-2; }
        #exp-container Digits { color: $success; }
    }
    """

    # Reactive variables
    hp: reactive[int] = reactive(0)
    mp: reactive[int] = reactive(0)
    gp: reactive[int] = reactive(0)
    exp: reactive[int] = reactive(0)
    lvl: reactive[int] = reactive(0)
    max_exp: reactive[int] = reactive(0)
    max_hp: reactive[int] = reactive(50)  # Default values
    max_mp: reactive[int] = reactive(30)  # Default values

    def update_display(self, user_data: User | None) -> None:
        """Updates the widget's display based on data from the User model."""
        if not user_data or not hasattr(user_data, "stats") or user_data.stats is None:
            log.warning("StatsCount received no user data or user stats to display. Resetting.")
            # Reset to default values
            self.hp = 0
            self.mp = 0
            self.gp = 0
            self.exp = 0
            self.lvl = 0
            self.max_exp = 0
            self.max_hp = 50
            self.max_mp = 30
        else:
            # Update reactive variables with safe attribute handling
            self.hp = int(getattr(user_data.stats, "hp", 0))
            self.mp = int(getattr(user_data.stats, "mp", 0))
            self.gp = int(getattr(user_data.stats, "gp", 0))
            self.exp = int(getattr(user_data.stats, "exp", 0))
            self.lvl = int(getattr(user_data, "level", 0))

            # Use getattr with defaults for potentially missing attributes
            self.max_exp = int(getattr(user_data.stats, "max_exp", getattr(user_data.stats, "toNextLevel", 0)))
            self.max_hp = int(getattr(user_data.stats, "max_hp", getattr(user_data.stats, "maxHealth", 50)))
            self.max_mp = int(getattr(user_data.stats, "max_mp", getattr(user_data.stats, "maxMana", 30)))

        # Update tooltips
        self._update_tooltips()

    def _update_tooltips(self) -> None:
        """Updates tooltips for stat widgets."""
        try:
            self.query_one("#lvl-digit", Digits).tooltip = f"Level: {self.lvl}"
            self.query_one("#mp-digit", Digits).tooltip = f"Mana: {self.mp} / {self.max_mp}"
            self.query_one("#hp-digit", Digits).tooltip = f"Health: {self.hp} / {self.max_hp}"
            self.query_one("#exp-digit", Digits).tooltip = f"Experience: {self.exp} / {self.max_exp}"
            self.query_one("#gp-digit", Digits).tooltip = f"Gold: {self.gp}"
        except Exception as e:
            log.error(f"Error updating tooltips: {e}")

    # Watch methods to update UI when reactive variables change
    def watch_lvl(self, value: int) -> None:
        try:
            self.query_one("#lvl-digit", Digits).update(f"{value}LVL")
        except Exception as e:
            log.error(f"Error in watch_lvl: {e}")

    def watch_mp(self, value: int) -> None:
        try:
            self.query_one("#mp-digit", Digits).update(f"{value}MP")
        except Exception as e:
            log.error(f"Error in watch_mp: {e}")

    def watch_hp(self, value: int) -> None:
        try:
            self.query_one("#hp-digit", Digits).update(f"{value}HP")
        except Exception as e:
            log.error(f"Error in watch_hp: {e}")

    def watch_exp(self, value: int) -> None:
        try:
            self.query_one("#exp-digit", Digits).update(f"{value}XP")
        except Exception as e:
            log.error(f"Error in watch_exp: {e}")

    def watch_gp(self, value: int) -> None:
        try:
            self.query_one("#gp-digit", Digits).update(f"{value}GP")
        except Exception as e:
            log.error(f"Error in watch_gp: {e}")

    def compose(self) -> ComposeResult:
        """Create UI components."""
        with Vertical(id="lvl-container"):
            yield Digits(f"{self.lvl}LVL", id="lvl-digit")

        with Vertical(id="mp-container"):
            yield Digits(f"{self.mp}MP", id="mp-digit")

        with Vertical(id="hp-container"):
            yield Digits(f"{self.hp}HP", id="hp-digit")

        with Vertical(id="gp-container"):
            yield Digits(f"{self.gp}GP", id="gp-digit")

        with Vertical(id="exp-container"):
            yield Digits(f"{self.exp}XP", id="exp-digit")
-e 
-------- END OF FILE ui/widgets/stats_count.py --------

-------- START OF FILE ui/widgets/table_detail_panel.py --------
from typing import ClassVar, Dict, List, Optional

from rich.text import Text
from textual import log, on
from textual.app import ComposeResult
from textual.containers import Container, Horizontal, Vertical
from textual.css.query import NoMatches
from textual.message import Message
from textual.reactive import reactive
from textual.widget import Widget
from textual.widgets import Button, Static

from pixabit.models.task import Daily, Task, Todo


class CompleteTask(Message):
    """Message to request task completion."""

    def __init__(self, task_id: str) -> None:
        super().__init__()
        self.task_id = task_id


class DeleteTask(Message):
    """Message to request task deletion."""

    def __init__(self, task_id: str) -> None:
        super().__init__()
        self.task_id = task_id


class EditTask(Message):
    """Message to request task editing."""

    def __init__(self, task_id: str) -> None:
        super().__init__()
        self.task_id = task_id


class ScoreTaskDetail(Message):
    """Message to request task scoring."""

    def __init__(self, task_id: str, direction: str) -> None:
        super().__init__()
        self.task_id = task_id
        self.direction = direction


class TaskDetailPanel(Widget):
    """Panel for displaying and interacting with task details."""

    # Reactive attributes
    current_task = reactive(None)
    tag_colors = reactive({})
    _visible = reactive(False)

    # Fields to display in detail panel
    DETAIL_FIELDS: ClassVar[List[str]] = ["text", "status", "value", "priority", "due", "tags", "notes"]
    BINDINGS = [
        ("d", "complete_task", "Complete Task"),
        ("+", "score_up", "Score Up"),
        ("-", "score_down", "Score Down"),
        ("e", "edit_task", "Edit Task"),
        ("x", "delete_task", "Delete Task"),
    ]

    def compose(self) -> ComposeResult:
        """Compose the detail panel layout."""
        with Vertical(id="details-container"):
            # Placeholder when no task is selected
            yield Static("Select a task to view details.", id="task-detail-placeholder")

            # Container for task details
            with Container(id="task-details-content", classes="hidden"):
                # Create Static widgets for each detail field
                for field in self.DETAIL_FIELDS:
                    yield Static("", id=f"task-detail-{field}")

            # Container for action buttons
            with Container(id="task-detail-actions", classes="hidden"):
                with Horizontal():
                    yield Button("Done", id="btn-complete", variant="success")
                    yield Button("+", id="btn-score-up", classes="score-button")
                    yield Button("-", id="btn-score-down", classes="score-button")
                with Horizontal():
                    yield Button("Edit", id="btn-edit", variant="primary")
                    yield Button("Delete", id="btn-delete", variant="error")

    def on_mount(self) -> None:
        """Handle widget mount."""
        self._update_visibility()

    def watch_current_task(self, task: Task | None) -> None:
        """React to changes in the current task."""
        self._visible = task is not None
        if task:
            self._populate_detail_fields(task)
        self._update_visibility()

    def watch_tag_colors(self, tag_colors: dict[str, str]) -> None:
        """React to changes in tag colors."""
        log.debug("Watch: Tag colors changed in detail panel.")
        if self.current_task is not None:
            self._update_tags_display(self.current_task)

    def _update_visibility(self) -> None:
        """Update visibility of container elements based on whether a task is selected."""
        placeholder = self.query_one("#task-detail-placeholder", Static)
        content = self.query_one("#task-details-content", Container)
        actions = self.query_one("#task-detail-actions", Container)

        if self._visible:
            placeholder.add_class("hidden")
            content.remove_class("hidden")
            actions.remove_class("hidden")
        else:
            placeholder.remove_class("hidden")
            content.add_class("hidden")
            actions.add_class("hidden")

    def _populate_detail_fields(self, task: Task) -> None:
        """Llena todos los campos de detalle con los datos de la tarea."""
        # Actualiza el campo de texto
        self._get_detail_field("text").update(f"[b]Task:[/b] {Text.from_markup(getattr(task, 'text', ''))}")

        # Actualiza el campo de estado
        self._get_detail_field("status").update(f"[b]Status:[/b] {getattr(task, '_status', 'unknown')}")

        # Actualiza los campos de valor y prioridad con formato
        self._get_detail_field("value").update(f"[b]Value:[/b] {getattr(task, 'value', 0.0):.1f}")
        self._get_detail_field("priority").update(f"[b]Priority:[/b] {getattr(task, 'priority', 1.0):.1f}")

        # Actualiza la fecha de vencimiento con manejo de errores mejorado
        self._update_due_date_display(task)

        # Actualiza las etiquetas
        self._update_tags_display(task)

        # Actualiza las notas
        self._get_detail_field("notes").update(f"[b]Notes:[/b]\n{getattr(task, 'notes', '')}")

    def _update_due_date_display(self, task: Task) -> None:
        """Actualiza la visualización de la fecha de vencimiento con manejo de errores mejorado."""
        due_str = "None"
        try:
            import datetime

            if isinstance(task, Todo) and hasattr(task, "due_date") and task.due_date:
                due_str = task.due_date.strftime("%Y-%m-%d")
            elif isinstance(task, Daily) and hasattr(task, "next_due") and task.next_due:
                if isinstance(task.next_due, list) and task.next_due:
                    due_str = task.next_due[0].strftime("%Y-%m-%d")
                elif hasattr(task.next_due, "strftime"):
                    due_str = task.next_due.strftime("%Y-%m-%d")
        except Exception as e:
            log.error(f"Error formatting due date: {e}")
            due_str = "Invalid Date"

        self._get_detail_field("due").update(f"[b]Due:[/b] {due_str}")

    def _update_tags_display(self, task: Task) -> None:
        """Actualiza la visualización de las etiquetas usando los colores de etiquetas actuales."""
        tag_names = getattr(task, "tag_names", [])
        tag_text = Text()

        if tag_names:
            for i, tag in enumerate(tag_names):
                color = self.tag_colors.get(tag, "$accent")
                tag_text.append(f"[{color}]{tag}[/{color}]")
                if i < len(tag_names) - 1:
                    tag_text.append(", ")
        else:
            tag_text.append("None")

        self._get_detail_field("tags").update(Text.assemble("[b]Tags:[/b] ", tag_text))

    def _get_detail_field(self, field_name: str) -> Static:
        """Obtiene un campo de detalle por nombre con manejo de errores."""
        try:
            return self.query_one(f"#task-detail-{field_name}", Static)
        except NoMatches:
            log.error(f"Detail field '{field_name}' not found")
            # Crear un campo de respaldo para evitar errores
            static = Static(f"Error: Missing '{field_name}' field")
            return static

    # --- Manejadores de Eventos de Botones ---
    def action_complete_task(self) -> None:
        """Complete the current task."""
        if self.current_task:
            log.debug(f"Binding: Complete task ID {self.current_task.id}")
            self.post_message(CompleteTask(str(self.current_task.id)))

    def action_score_up(self) -> None:
        """Score up the current task."""
        if self.current_task:
            log.debug(f"Binding: Score up task ID {self.current_task.id}")
            self.post_message(ScoreTaskDetail(str(self.current_task.id), "up"))

    def action_score_down(self) -> None:
        """Score down the current task."""
        if self.current_task:
            log.debug(f"Binding: Score down task ID {self.current_task.id}")
            self.post_message(ScoreTaskDetail(str(self.current_task.id), "down"))

    def action_edit_task(self) -> None:
        """Edit the current task."""
        if self.current_task:
            log.debug(f"Binding: Edit task ID {self.current_task.id}")
            self.post_message(EditTask(str(self.current_task.id)))

    def action_delete_task(self) -> None:
        """Delete the current task."""
        if self.current_task:
            log.debug(f"Binding: Delete task ID {self.current_task.id}")
            self.post_message(DeleteTask(str(self.current_task.id)))
-e 
-------- END OF FILE ui/widgets/table_detail_panel.py --------

-------- START OF FILE ui/widgets/table_list_panel.py --------
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional

import pytz
from rich.text import Text
from textual import events, on
from textual.app import ComposeResult
from textual.containers import Vertical
from textual.coordinate import Coordinate
from textual.message import Message
from textual.reactive import reactive
from textual.widget import Widget
from textual.widgets import DataTable, Input, Select
from textual.widgets._data_table import CellKey, ColumnKey, RowKey

from pixabit.helpers._logger import log
from pixabit.models.task import Daily, Task, Todo


class ScoreTaskRequest(Message):
    """Message to request scoring a task."""

    def __init__(self, task_id: str, direction: str) -> None:
        super().__init__()
        self.task_id = task_id
        self.direction = direction


class ViewTaskDetailsRequest(Message):
    """Message to request viewing task details."""

    def __init__(self, task_id: str) -> None:
        super().__init__()
        self.task_id = task_id


class TaskListWidget(Widget):
    """Widget for displaying and interacting with a list of tasks."""

    # Reactive attributes for filtering and sorting
    _text_filter = reactive("", layout=True)
    tag_colors = reactive({})
    sort_key = reactive(None)
    sort_ascending = reactive(True)

    def __init__(
        self,
        task_type: str | None = None,
        id: str | None = None,
        **kwargs,
    ):
        """Initialize the task list widget.

        Args:
            task_type: Type of tasks to display ('todo', 'daily', etc.)
            id: Widget ID
        """
        self.task_type_filter = task_type
        widget_id = id or f"task-list-{task_type or 'all'}"
        super().__init__(id=widget_id, **kwargs)
        self._datatable = None
        self._tasks = []
        self._sort_value_cache = {}

    def compose(self) -> ComposeResult:
        """Compose the widget layout."""
        with Vertical():
            yield Input(placeholder="Filter tasks...", id="task-filter-input")
            self._datatable = DataTable(id="tasks-data-table", cursor_type="row", zebra_stripes=True)
            self._datatable.add_column("S", key="status", width=3)
            self._datatable.add_column("Task", key="text", width=40)
            self._datatable.add_column("Value", key="value", width=8)
            self._datatable.add_column("Pri", key="priority", width=5)
            self._datatable.add_column("Due", key="due", width=12)
            self._datatable.add_column("Tags", key="tags")
            yield self._datatable

    async def on_mount(self) -> None:
        """Handle widget mount."""
        log.info(f"Mounting TaskListWidget {self.id}...")
        if not self._datatable:
            log.error("DataTable instance not found in on_mount!")
            return

        # Initial data load
        self.run_worker(
            self.load_or_refresh_data,
            exclusive=True,
            name=f"load_{self.id}",
            group="load_tasks",
        )

    def watch__text_filter(self, new_filter: str) -> None:
        """React to changes in text filter."""
        log.info(f"Watch: _text_filter changed to '{new_filter}'")
        self.run_worker(
            self.load_or_refresh_data,
            name=f"filter_text_{self.id}",
            group="load_tasks",
            exclusive=True,
        )

    def watch_tag_colors(self, new_colors: dict[str, str]) -> None:
        """React to changes in tag colors."""
        log.info("Watch: Tag colors changed. Re-sorting/displaying tasks.")
        self._invalidate_sort_cache()
        self.sort_and_display_tasks()

    def watch_sort_key(self, new_key: str) -> None:
        """React to changes in sort key."""
        if new_key:
            log.info(f"Watch: sort_key changed to {new_key}")
            self._invalidate_sort_cache()
            self.sort_and_display_tasks()

    def watch_sort_ascending(self, ascending: bool) -> None:
        """React to changes in sort direction."""
        log.info(f"Watch: sort_ascending changed to {ascending}")
        self.sort_and_display_tasks()

    @on(Input.Changed, "#task-filter-input")
    def handle_filter_change(self, event: Input.Changed) -> None:
        """Handle changes to the text filter input."""
        self._text_filter = event.value

    @on(DataTable.HeaderSelected)
    def on_header_selected(self, event: DataTable.HeaderSelected) -> None:
        """Handle clicking on table headers for sorting."""
        table = self._datatable
        if not table:
            return

        column_key = event.column_key
        try:
            column_string_key = str(column_key.value)
            sortable_keys = ["status", "text", "value", "priority", "due", "tags"]

            if column_string_key not in sortable_keys:
                return

            # Toggle sort direction if same column, change column otherwise
            if column_string_key == self.sort_key:
                self.sort_ascending = not self.sort_ascending
            else:
                self.sort_key = column_string_key
                self.sort_ascending = True

            log.info(f"Sorting by {self.sort_key}, ascending={self.sort_ascending}")
        except Exception as e:
            log.error(f"Error during header selection: {e}")

    def on_key(self, event: events.Key) -> None:
        """Handle keyboard input for task actions."""
        table = self._datatable
        if not table or not table.row_count:
            return

        cursor_coordinate = table.cursor_coordinate
        if cursor_coordinate and table.is_valid_coordinate(cursor_coordinate):
            try:
                cell_key: CellKey = table.coordinate_to_cell_key(cursor_coordinate)
                if cell_key.row_key is None:
                    return

                task_id = str(cell_key.row_key.value)
                if task_id:
                    if event.key == "+":
                        self.post_message(ScoreTaskRequest(task_id, "up"))
                        event.stop()
                    elif event.key == "-":
                        self.post_message(ScoreTaskRequest(task_id, "down"))
                        event.stop()
            except Exception as e:
                log.error(f"Key handler error: {e}")

    @on(DataTable.RowSelected)
    def on_data_table_row_selected(self, event: DataTable.RowSelected) -> None:
        """Handle row selection to view task details."""
        task_id_key: RowKey | None = event.row_key
        if task_id_key is not None:
            task_id = str(task_id_key.value)
            log.info(f"Row selected, task ID: {task_id}")
            self.post_message(ViewTaskDetailsRequest(task_id))

    async def load_or_refresh_data(self) -> None:
        """Load or refresh the task list data."""
        table = self._datatable
        if not table:
            log.error("DataTable instance is None in load_or_refresh_data!")
            return

        log.info(f"TaskListWidget ({self.id}): Refreshing. Type='{self.task_type_filter}', Text='{self._text_filter}'")

        # Remember current selection
        current_row_id = self._get_current_cursor_row_id()

        # Clear table and prepare for new data
        table.loading = True
        table.clear()
        self._tasks = []
        self._invalidate_sort_cache()

        # Set up filters
        data_filters = {"text_filter": self._text_filter}
        if self.task_type_filter and self.task_type_filter != "all":
            data_filters["task_type"] = self.task_type_filter

        try:
            # Fetch tasks from data store
            fetched_tasks = await self.app.run_in_thread(self.app.datastore.get_tasks, **data_filters)
            log.info(f"Received {len(fetched_tasks)} tasks from DataStore.")
            self._tasks = fetched_tasks
        except Exception as e:
            log.error(f"Error getting tasks from datastore: {e}")
            self._tasks = []

        # Sort and display tasks
        self.sort_and_display_tasks()
        table.loading = False

        # Restore cursor position if possible
        self._restore_cursor_position(current_row_id)

    def sort_and_display_tasks(self) -> None:
        """Sort and display tasks based on current sort settings."""
        table = self._datatable
        if not table:
            return

        table.clear()
        sorted_tasks = self._tasks

        # Sort if a sort key is set
        if self.sort_key is not None:
            try:
                sorted_tasks = sorted(self._tasks, key=lambda task: self._get_sort_value(task), reverse=not self.sort_ascending)
            except Exception as e:
                log.error(f"Error sorting tasks: {e}")

        # Add rows for all tasks
        for task in sorted_tasks:
            try:
                self._add_row_for_task(table, task)
            except Exception as e:
                task_id = getattr(task, "id", "N/A")
                log.error(f"Error adding row for task {task_id}: {e}")

    def _add_row_for_task(self, table: DataTable, task: Task) -> None:
        """Add a row to the table for a task."""
        try:
            # Get task attributes
            status = getattr(task, "_status", "unknown")
            value = getattr(task, "value", 0.0)
            priority = getattr(task, "priority", 1.0)
            tag_names = getattr(task, "tag_names", [])
            task_id = getattr(task, "id", f"no-id-{id(task)}")

            # Create cell content
            status_cell = Text("●", style=f"bold {self._get_status_style(status)}")
            task_text = Text.from_markup(getattr(task, "text", ""))
            due_str = self._format_due_date(task)
            tag_str = self._create_tags_cell(tag_names)

            # Add the row
            table.add_row(
                status_cell,
                task_text,
                f"{value:.1f}",
                f"{priority:.1f}",
                due_str,
                tag_str,
                key=str(task_id),
            )
        except Exception as e:
            log.error(f"Error adding row for task: {e}")

    def _format_due_date(self, task: Task) -> str:
        """Format the due date for a task."""
        due_str = ""
        try:
            if isinstance(task, Todo) and task.due_date:
                due_str = task.due_date.strftime("%Y-%m-%d")
            elif isinstance(task, Daily) and task.next_due and task.next_due[0]:
                due_str = task.next_due[0].strftime("%Y-%m-%d")
        except Exception as e:
            log.error(f"Error formatting due date: {e}")
            due_str = "Invalid"
        return due_str

    def _create_tags_cell(self, tag_names: list[str]) -> Text:
        """Create a formatted text object for tags."""
        tag_text = Text()
        if not tag_names:
            return tag_text

        for i, tag in enumerate(tag_names):
            color = self.tag_colors.get(tag, "$accent")
            tag_text.append(tag, style=color)
            if i < len(tag_names) - 1:
                tag_text.append(", ")

        return tag_text

    def _get_status_style(self, status: str) -> str:
        """Get the style color for a task status."""
        return {
            "due": "$warning",
            "red": "$error",
            "done": "$success",
            "success": "$success",
            "grey": "$text-muted",
            "habit": "$secondary",
            "reward": "$warning",
            "unknown": "$text-disabled",
        }.get(status, "$text-muted")

    def _get_sort_value(self, task: Task) -> Any:
        """Get the sort value for a task, using cache if available."""
        cache_key = (task.id, self.sort_key)
        if cache_key in self._sort_value_cache:
            return self._sort_value_cache[cache_key]

        value = self._calculate_sort_value(task)
        self._sort_value_cache[cache_key] = value
        return value

    def _calculate_sort_value(self, task: Task) -> Any:
        """Calculate the sort value for a task based on the current sort key."""
        key_str = self.sort_key

        if key_str == "status":
            status_order = {"red": 0, "due": 1, "habit": 2, "unknown": 3, "grey": 4, "done": 5, "success": 5}
            status = getattr(task, "_status", "unknown")
            return (status_order.get(status, 99), status)

        elif key_str == "text":
            return getattr(task, "text", "").lower()

        elif key_str == "value":
            return getattr(task, "value", 0.0)

        elif key_str == "priority":
            return getattr(task, "priority", 1.0)

        elif key_str == "due":
            if isinstance(task, Todo) and task.due_date:
                return task.due_date
            if isinstance(task, Daily) and task.next_due:
                return task.next_due[0] if task.next_due else datetime.max.replace(tzinfo=timezone.utc)
            return datetime.max.replace(tzinfo=timezone.utc)

        elif key_str == "tags":
            tag_names = getattr(task, "tag_names", [])
            return tag_names[0].lower() if tag_names else ""

        else:
            return getattr(task, "text", "").lower()

    def _get_current_cursor_row_id(self) -> str | None:
        """Get the ID of the currently selected row."""
        table = self._datatable
        if not table:
            return None

        current_cursor_coordinate = table.cursor_coordinate
        if current_cursor_coordinate and table.is_valid_coordinate(current_cursor_coordinate):
            try:
                cell_key: CellKey = table.coordinate_to_cell_key(current_cursor_coordinate)
                return str(cell_key.row_key.value)
            except Exception as e:
                log.warning(f"Could not get row key: {e}")

        return None

    def _restore_cursor_position(self, row_id: str | None) -> None:
        """Restore cursor to previously selected row if possible."""
        table = self._datatable
        if not table or not table.row_count:
            return

        if row_id is not None:
            try:
                new_row_index = table.get_row_index(RowKey(row_id))
                table.move_cursor(row=new_row_index, animate=False)
                return
            except Exception:
                pass

        # Default to first row if can't restore
        table.move_cursor(row=0, animate=False)

    def _invalidate_sort_cache(self) -> None:
        """Clear the sort value cache."""
        self._sort_value_cache = {}
-e 
-------- END OF FILE ui/widgets/table_list_panel.py --------

-------- START OF FILE ui/widgets/tasks_view.py --------
-e 
-------- END OF FILE ui/widgets/tasks_view.py --------

-------- START OF FILE ui/widgets/__init__.py --------
-e 
-------- END OF FILE ui/widgets/__init__.py --------

-------- START OF FILE ui/__init__.py --------
-e 
-------- END OF FILE ui/__init__.py --------

-------- START OF FILE __init__.py --------
-e 
-------- END OF FILE __init__.py --------

-------- START OF FILE __main__.py --------
-e 
-------- END OF FILE __main__.py --------

