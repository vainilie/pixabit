This is a code for a Habitica TUI.
I'm using:
- English
- Pydantic 2.
- Typing for Python +3.10 (using | for union, optional, dict and list in lowercase)
- My personalized helpers (I'll add at the end): date, json, logger.
- Anchor comments (avoid SECTION if not closing each section)

I want to refactor them:
- I want a pythonic structure, to not have circular or innecessary proccesses. I want to use all Pydantic possibilities and integrate my own helpers if needed.
- I want to generate if possible all the content data within the classes, I mean, not to depend of others or, if I do, don't distinguish between the linked/processed/posprocessed.
- I want to store the specific quest details got in the data section in the party or users. My idea is to only use data for enriching it.
- English comments and headers. The same order for the arguments and properties within functions and variables and more.

Considerations:
- TagFactory and Tags: Tags is the simple version, TagFactory is my personalized and advanced version for the API.
- I haven't manually reviewed Challenge class. The idea is to make EVERYTHING PYDANTIC if possible.

Structure:
- Explain all suggestions for all files. Add headers for each file, then the suggestions or propossed changes, detail them and the refactored code.
- Remember ALWAYS my instructions: make it minimal, simple, pythonic, zen python, non-complex, clear and mantain my personalized fields. This isn't a generic habitica api tui, but mine with creative ideas.

-------- START OF FILE pixabit/models/challenge.py --------
# pixabit/models/challenge.py

# SECTION: MODULE DOCSTRING
"""Defines Pydantic models for Habitica Challenges.

Includes:
- `ChallengeLeader`: Represents nested leader information.
- `ChallengeGroup`: Represents nested group information.
- `Challenge`: Represents a single challenge entity with its metadata and associated tasks.
- `ChallengeList`: Processes raw challenge data, manages Challenge objects,
  and links associated Task objects.

Alternatives for Linking: Instead of passing TaskList to ChallengeList.__init__,
consider a separate function `link_challenges_and_tasks(challenges: list[Challenge], tasks: TaskList)`
that performs the linking. This slightly decouples the list classes.
"""

# SECTION: IMPORTS
from __future__ import annotations

import logging
from datetime import datetime, timezone
from typing import (  # Use standard List/Dict
    Any,
    Dict,
    List,
    Literal,
    Optional,
    cast,
)

# External Libs
import emoji_data_python
from pydantic import (  # Use | for Union
    BaseModel,
    ConfigDict,
    Field,
    FieldValidationInfo,  # For context in validators
    ValidationError,
    field_validator,
    model_validator,
)

# Local Imports
try:
    # Assuming Task models are in the same models directory
    from .task import Task, TaskList  # Keep TaskList import here
except ImportError:
    logging.warning("Using placeholder Task/TaskList in challenge.py.")

    # Define fallback types for type checking
    class Task:
        id: str | None = None
        challenge: Any = None
        type: str = "unknown"  # type: ignore

    class TaskList:  # type: ignore
        def __init__(self, tasks: list | None = None):
            self.tasks = tasks or []

        def __iter__(self):
            return iter(self.tasks)

        def __len__(self):
            return len(self.tasks)


# Use standard Python logger
logger = logging.getLogger(__name__)


# SECTION: PYDANTIC SUB-MODELS


# KLASS: ChallengeLeader
class ChallengeLeader(BaseModel):
    """Represents the leader info potentially nested within challenge data."""

    model_config = ConfigDict(extra="ignore", populate_by_name=True)

    id: str | None = Field(None, alias="_id", description="Leader's user ID.")
    # Profile sub-object is ignored by config, only extract name if present
    name: str | None = Field(
        None, description="Leader's display name (parsed)."
    )

    # Validator to handle potential 'profile' nesting for name
    @model_validator(mode="before")
    @classmethod
    def extract_profile_name(cls, data: Any) -> Any:
        if isinstance(data, dict):
            profile_data = data.get("profile")
            if isinstance(profile_data, dict):
                data["name"] = profile_data.get(
                    "name"
                )  # Extract name from profile
            # Allow 'name' at the top level too if profile doesn't exist
        return data

    # Validator to parse emoji in the name field AFTER extraction
    @field_validator("name", mode="before")
    @classmethod
    def parse_name_emoji(cls, value: Any) -> str | None:
        if isinstance(value, str):
            return emoji_data_python.replace_colons(value)
        return None


# KLASS: ChallengeGroup
class ChallengeGroup(BaseModel):
    """Represents the group info nested within challenge data."""

    model_config = ConfigDict(extra="ignore", populate_by_name=True)

    id: str | None = Field(None, alias="_id", description="Group ID.")
    name: str | None = Field(None, description="Group name (parsed).")
    # Use Literal for known group types if applicable, otherwise str
    type: Literal["party", "guild", "tavern"] | str | None = Field(
        None, description="Group type."
    )

    @field_validator("name", mode="before")
    @classmethod
    def parse_name_emoji(cls, value: Any) -> str | None:
        if isinstance(value, str):
            return emoji_data_python.replace_colons(value)
        return None


# SECTION: MAIN CHALLENGE MODEL


# KLASS: Challenge
class Challenge(BaseModel):
    """Represents a Habitica Challenge entity."""

    model_config = ConfigDict(extra="ignore", populate_by_name=True)

    # Core fields
    id: str = Field(
        ..., description="Unique challenge ID."
    )  # Made non-optional
    name: str = Field(
        "Unnamed Challenge", description="Challenge name (parsed)."
    )
    short_name: str | None = Field(
        None, alias="shortName", description="Short name (parsed)."
    )
    summary: str = Field(
        "", description="Summary text (parsed)."
    )  # Default to empty string
    description: str = Field(
        "", description="Full description (parsed)."
    )  # Default to empty string

    # Nested/Extracted Leader and Group info (set by validator)
    leader: ChallengeLeader | None = Field(
        None, description="Challenge leader details."
    )
    group: ChallengeGroup | None = Field(
        None, description="Associated group details."
    )

    # Other Attributes
    prize: int = Field(0, description="Gem prize for the winner.")
    member_count: int = Field(
        0, alias="memberCount", description="Number of participants."
    )
    official: bool = Field(
        False, description="Is this an official Habitica challenge?"
    )
    created_at: datetime | None = Field(
        None, alias="createdAt", description="Timestamp created (UTC)."
    )
    updated_at: datetime | None = Field(
        None, alias="updatedAt", description="Timestamp updated (UTC)."
    )
    broken: str | None = Field(
        None, description="Status if broken (e.g., 'CHALLENGE_DELETED')."
    )
    # Owned status might come from a different context, keep Optional
    owned: bool | None = Field(
        None, description="Is challenge owned by the fetching user?"
    )

    # Linked Tasks (populated externally AFTER challenge list is created)
    # Exclude from model serialization, manage externally
    tasks: list[Task] = Field(default_factory=list, exclude=True)

    # --- Validators ---

    # Validator to handle _id vs id and ensure ID exists
    @model_validator(mode="before")
    @classmethod
    def check_and_assign_id(cls, data: Any) -> Any:
        if isinstance(data, dict):
            if "_id" in data:
                data["id"] = data["_id"]  # Prefer 'id' field in the model
            elif "id" not in data:
                # Attempt to use name for logging if ID is missing
                name_for_log = data.get("name", "Unknown Challenge")
                logger.error(
                    f"Challenge data missing required 'id' or '_id': {name_for_log}"
                )
                # Pydantic will raise ValidationError later if 'id' is required (which it is now)
                # raise ValueError("Challenge data must contain 'id' or '_id'")
        return data

    # Consolidate emoji parsing for text fields
    @field_validator(
        "name", "short_name", "summary", "description", mode="before"
    )
    @classmethod
    def parse_text_emoji(cls, value: Any) -> str | None:
        """Parses text fields and replaces emoji shortcodes."""
        if isinstance(value, str):
            return emoji_data_python.replace_colons(value)
        # Return None for optional fields if input is None, else default (like "" for summary/desc)
        return None if value is None else ""

    # Consolidate datetime parsing
    @field_validator("created_at", "updated_at", mode="before")
    @classmethod
    def parse_datetime_utc(cls, value: Any) -> datetime | None:
        """Parses timestamp strings/datetimes into timezone-aware UTC datetime objects."""
        # Reuse function from date helper if available and imported
        # return convert_timestamp_to_utc(value)

        # Or keep inline logic:
        if isinstance(value, str):
            try:
                # Handle ISO 8601 format, ensuring UTC
                dt = datetime.fromisoformat(value.replace("Z", "+00:00"))
                return (
                    dt.astimezone(timezone.utc)
                    if dt.tzinfo
                    else dt.replace(tzinfo=timezone.utc)
                )
            except (ValueError, TypeError):
                logger.warning(f"Could not parse timestamp: {value}")
                return None
        elif isinstance(value, datetime):
            # Ensure timezone aware UTC
            if value.tzinfo is None:
                return value.replace(tzinfo=timezone.utc)
            return value.astimezone(timezone.utc)
        return None

    # Consolidate integer parsing
    @field_validator("prize", "member_count", mode="before")
    @classmethod
    def ensure_int(cls, value: Any) -> int:
        """Ensures numeric fields are integers, defaulting to 0 on error."""
        try:
            return int(value)
        except (ValueError, TypeError):
            return 0

    # --- Methods ---
    def add_tasks(self, tasks_to_add: list[Task]) -> None:
        """Adds Task objects associated with this challenge."""
        if not isinstance(tasks_to_add, list):
            return
        self.tasks.extend(
            task for task in tasks_to_add if isinstance(task, Task)
        )

    def get_tasks_by_type(self, task_type: str) -> list[Task]:
        """Returns linked tasks of a specific type."""
        return [task for task in self.tasks if task.type == task_type]

    def __repr__(self) -> str:
        """Concise representation."""
        status = f" (Broken: {self.broken})" if self.broken else ""
        owner_flag = " (Owned)" if self.owned else ""
        official_flag = " (Official)" if self.official else ""
        task_count = len(self.tasks)
        name_preview = self.name[:30].replace("\n", " ") + (
            "..." if len(self.name) > 30 else ""
        )
        return f"Challenge(id='{self.id}', name='{name_preview}', tasks={task_count}{status}{owner_flag}{official_flag})"


# SECTION: CHALLENGE LIST CONTAINER


# KLASS: ChallengeList
class ChallengeList:
    """Container for managing Challenge objects and linking tasks."""

    # FUNC: __init__
    def __init__(self, raw_challenge_list: list[dict[str, Any]]):
        """Initializes the ChallengeList by processing raw data.

        Args:
            raw_challenge_list: List of dictionaries (raw challenge data from API).
        """
        self.challenges: list[Challenge] = []
        self._process_list(raw_challenge_list)
        logger.info(f"Processed {len(self.challenges)} challenges.")

    # FUNC: _process_list
    def _process_list(self, raw_challenge_list: list[dict[str, Any]]) -> None:
        """Processes the raw list, creating Challenge Pydantic instances."""
        processed_challenges: list[Challenge] = []
        if not isinstance(raw_challenge_list, list):
            logger.error(
                f"ChallengeList input must be a list, got {type(raw_challenge_list)}."
            )
            self.challenges = []
            return

        for raw_challenge in raw_challenge_list:
            if not isinstance(raw_challenge, dict):
                logger.warning(
                    f"Skipping invalid non-dict entry in challenge list: {raw_challenge}"
                )
                continue
            try:
                challenge_instance = Challenge.model_validate(raw_challenge)
                # ID is now required by the model, validation handles missing ones
                processed_challenges.append(challenge_instance)
            except ValidationError as e:
                # Log details about the failing item
                failed_id = raw_challenge.get("id") or raw_challenge.get(
                    "_id", "N/A"
                )
                failed_name = raw_challenge.get("name", "N/A")
                logger.error(
                    f"Validation failed for challenge (ID: {failed_id}, Name: {failed_name}):\n{e}"
                )
            except Exception as e:
                failed_id = raw_challenge.get("id") or raw_challenge.get(
                    "_id", "N/A"
                )
                logger.error(
                    f"Unexpected error processing challenge data (ID: {failed_id}): {e}",
                    exc_info=True,
                )

        self.challenges = processed_challenges

    # FUNC: link_tasks (Moved logic here from __init__)
    def link_tasks(self, task_list: TaskList) -> int:
        """Links Task objects from a TaskList to the corresponding Challenge objects.

        Args:
            task_list: A TaskList instance containing processed Task objects.

        Returns:
            The number of tasks successfully linked.
        """
        if not isinstance(task_list, TaskList):
            logger.warning(
                f"link_tasks requires a TaskList object, got {type(task_list)}. Skipping."
            )
            return 0

        logger.info(
            f"Linking {len(task_list)} tasks to {len(self.challenges)} challenges..."
        )
        # Create a lookup dictionary for faster challenge access
        challenges_by_id: dict[str, Challenge] = {
            chal.id: chal for chal in self.challenges if chal.id
        }
        # Clear existing task links first
        for challenge in self.challenges:
            challenge.tasks = []

        linked_count = 0
        skipped_count = 0
        # Iterate through tasks from the provided TaskList instance
        for task in task_list:
            # Check if task object has necessary attributes
            if (
                not hasattr(task, "challenge")
                or not task.challenge
                or not hasattr(task.challenge, "id")
                or not task.challenge.id
            ):
                skipped_count += 1
                continue  # Skip tasks without valid challenge ID info

            challenge_id = task.challenge.id
            target_challenge = challenges_by_id.get(challenge_id)

            if target_challenge:
                target_challenge.add_tasks([task])  # Use add_tasks method
                linked_count += 1
            else:
                # Log if a task points to a challenge not in our list (maybe filtered out?)
                # logger.debug(f"Task {task.id} links to challenge {challenge_id}, but challenge not found in ChallengeList.")
                skipped_count += 1

        logger.info(
            f"Task linking complete. Linked: {linked_count}, Skipped/Not Found: {skipped_count}."
        )
        # Optional: Sort tasks within each challenge after linking
        # for challenge in self.challenges: challenge.tasks.sort(...)
        return linked_count

    # --- Access and Filtering Methods ---

    def __len__(self) -> int:
        return len(self.challenges)

    def __iter__(self) -> iter[Challenge]:
        return iter(self.challenges)

    def __getitem__(self, index: int | slice) -> Challenge | list[Challenge]:
        if isinstance(index, int):
            if not 0 <= index < len(self.challenges):
                raise IndexError("Challenge index out of range")
        elif isinstance(index, slice):
            # Handle slicing appropriately if needed
            pass  # Pydantic models in list handle slicing by default
        else:
            raise TypeError("Index must be an integer or slice")
        return self.challenges[index]

    def get_by_id(self, challenge_id: str) -> Challenge | None:
        """Finds a challenge by its ID."""
        return next((c for c in self.challenges if c.id == challenge_id), None)

    def filter_by_name(
        self, name_part: str, case_sensitive: bool = False
    ) -> list[Challenge]:
        """Filters challenges by name containing a substring."""
        if not case_sensitive:
            name_part_lower = name_part.lower()
            return [
                c for c in self.challenges if name_part_lower in c.name.lower()
            ]
        else:
            return [c for c in self.challenges if name_part in c.name]

    def filter_by_leader(self, leader_id: str) -> list[Challenge]:
        """Filters challenges by leader's user ID."""
        return [
            c for c in self.challenges if c.leader and c.leader.id == leader_id
        ]

    def filter_by_group(
        self, group_id: str | None = None, group_type: str | None = None
    ) -> list[Challenge]:
        """Filters challenges by group ID and/or group type."""
        filtered = self.challenges
        if group_id is not None:
            filtered = [
                c for c in filtered if c.group and c.group.id == group_id
            ]
        if group_type is not None:
            filtered = [
                c for c in filtered if c.group and c.group.type == group_type
            ]
        return filtered

    def filter_by_official(self, official: bool = True) -> list[Challenge]:
        """Filters for official or unofficial challenges."""
        return [c for c in self.challenges if c.official is official]

    def filter_broken(self, is_broken: bool = True) -> list[Challenge]:
        """Filters challenges based on whether they have a 'broken' status string."""
        return [
            c for c in self.challenges if (c.broken is not None) == is_broken
        ]

    def filter_owned(self, owned: bool = True) -> list[Challenge]:
        """Filters challenges based on the 'owned' flag (if available)."""
        # Only include challenges where 'owned' is not None for comparison
        return [
            c
            for c in self.challenges
            if c.owned is not None and c.owned is owned
        ]

    def filter_by_task_count(
        self, min_tasks: int = 1, max_tasks: int | None = None
    ) -> list[Challenge]:
        """Filters challenges by the number of linked tasks."""
        if max_tasks is None:
            return [c for c in self.challenges if len(c.tasks) >= min_tasks]
        else:
            return [
                c
                for c in self.challenges
                if min_tasks <= len(c.tasks) <= max_tasks
            ]

    def filter_containing_task_id(self, task_id: str) -> list[Challenge]:
        """Filters challenges that contain a specific task ID."""
        return [
            c
            for c in self.challenges
            if any(task.id == task_id for task in c.tasks)
        ]

    def __repr__(self) -> str:
        """Simple representation."""
        return f"ChallengeList(count={len(self.challenges)})"

-------- END OF FILE pixabit/models/challenge.py --------

-------- START OF FILE pixabit/models/data_manager.py --------
# pixabit/services/data_manager.py

"""Proporciona un gestor de datos centralizado para los objetos modelo de Habitica.

Este módulo define la clase DataManager que maneja la carga, el cacheo
y el acceso a los modelos de datos de Habitica como Usuario, Tareas y Lista de Etiquetas.
Orquesta la carga tanto de datos de usuario 'vivos' como de contenido de juego estático.
"""

from __future__ import annotations

import asyncio
import json
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import TYPE_CHECKING, Any, Dict, List, Optional, Union

# Importaciones de tu proyecto
from pixabit.api.client import HabiticaClient
from pixabit.helpers._logger import log

# Asumiendo que StaticContentManager, Quest y Gear están definidos en game_content.py
# Asegúrate de que estas importaciones sean correctas según tu estructura de archivos
from pixabit.models.game_content import Gear, Quest, StaticContentManager
from pixabit.models.party import Party
from pixabit.models.tag import Tag, TagList

# Asumiendo que TaskList, User, Daily, etc., están definidos en task.py y user.py
# Asegúrate de que estas importaciones sean correctas según tu estructura de archivos
from pixabit.models.task import Daily, Habit, Reward, Task, TaskList, Todo
from pixabit.models.user import User

# Configuración del cache
# Establece un tiempo de espera razonable para el cache de datos 'vivos' (4 minutos)
CACHE_TIMEOUT = timedelta(minutes=4)
# La duración del cache de contenido estático es manejada por StaticContentManager


class CustomJSONEncoder(json.JSONEncoder):
    """Codificador JSON personalizado para manejar tipos especiales como datetime y objetos modelo."""

    def default(self, obj: Any) -> Any:
        # Handle datetime objects
        if isinstance(obj, datetime):
            return obj.isoformat()

        # Handle Pydantic models or any object with model_dump
        if hasattr(obj, "model_dump"):
            # Usar el dump de Pydantic para una serialización adecuada incluyendo alias, etc.
            # mode='json' maneja automáticamente modelos anidados y tipos estándar
            return obj.model_dump(mode="json")

        # Handle objects with to_dict (fallback for non-Pydantic collections like TaskList if needed,
        # but TaskList should ideally have model_dump or similar now)
        if hasattr(obj, "to_dict"):
            return obj.to_dict()

        # Handle Task subclasses (like Habit) - This might be redundant if model_dump/to_dict is used
        # and can sometimes cause issues with private attributes. Prefer Pydantic's methods.
        # if hasattr(obj, "__dict__"):
        #     return obj.__dict__

        # Let the parent class handle the rest or raise TypeError
        return super().default(obj)


class DataManager:
    """Gestor centralizado para los modelos de datos de Habitica.

    Maneja la carga, el cacheo y el acceso a Usuario, Tareas, Lista de Etiquetas, Grupo (Party),
    y contenido de juego estático (Equipamiento, Misiones, Hechizos).
    Coordina las interacciones entre modelos y asegura que los datos correctos estén disponibles.
    """

    def __init__(self, api_client: HabiticaClient, cache_dir: Path | None = None):
        """Inicializa el DataManager.

        Args:
            api_client: El cliente HabiticaClient para las solicitudes a la API
            cache_dir: Directorio opcional para cachear datos en disco
        """
        self.api = api_client
        self.cache_dir = cache_dir

        # Configurar rutas de cache si se proporciona un directorio
        if cache_dir:
            self.cache_dir.mkdir(exist_ok=True, parents=True)

            # Crear subdirectorios para datos crudos y procesados
            self.raw_cache_dir = self.cache_dir / "raw"
            self.processed_cache_dir = self.cache_dir / "processed"
            # Directorio dedicado para el cache de contenido estático
            self.static_cache_dir = self.cache_dir / "static"

            self.raw_cache_dir.mkdir(exist_ok=True)
            self.processed_cache_dir.mkdir(exist_ok=True)
            self.static_cache_dir.mkdir(exist_ok=True)

        # --- Inicializa el Gestor de Contenido Estático ---
        # El StaticContentManager maneja la lógica de carga y cacheo del endpoint /content
        # Pásale el directorio donde debe almacenar sus archivos de cache
        self.static_content_manager = StaticContentManager(data_dir=self.static_cache_dir)  # Usa un subdirectorio dedicado para el cache estático
        # --- Fin Inicialización Gestor Contenido Estático ---

        # Inicializar contenedores para modelos de datos 'VIVOS'
        self._user: User | None = None
        self._tasks: TaskList | None = None
        self._tags: TagList | None = None
        self._party: Party | None = None

        # Inicializar contenedores para datos ESTÁTICOS
        self._static_gear_data: dict[str, Gear] | None = None
        self._static_quest_data: dict[str, Quest] | None = None
        # Añade otros tipos de datos estáticos si es necesario (hechizos, etc.)

        # Contenedores para datos crudos 'VIVOS' (opcional, principalmente para depuración/cacheo crudo)
        self._raw_user_data: Dict | None = None
        self._raw_tasks_data: List | None = None
        self._raw_tags_data: List | None = None
        self._raw_party_data: Dict | None = None

        # Rastrear los últimos tiempos de actualización para datos 'VIVOS'
        self._user_last_refresh: datetime | None = None
        self._tasks_last_refresh: datetime | None = None
        self._tags_last_refresh: datetime | None = None
        self._party_last_refresh: datetime | None = None

    @property
    def user(self) -> User | None:
        """Obtiene los datos de Usuario cacheado, retorna None si aún no se ha cargado."""
        return self._user

    @property
    def tasks(self) -> TaskList | None:
        """Obtiene la TaskList cacheada, retorna None si aún no se ha cargado."""
        return self._tasks

    @property
    def tags(self) -> TagList | None:
        """Obtiene la TagList cacheada, retorna None si aún no se ha cargado."""
        return self._tags

    @property
    def party(self) -> Party | None:
        """Obtiene los datos de Party cacheado, retorna None si aún no se ha cargado."""
        return self._party

    # --- Propiedades para Datos Estáticos ---
    @property
    def static_gear_data(self) -> dict[str, Gear] | None:
        """Obtiene los datos estáticos de equipamiento cacheado."""
        return self._static_gear_data

    @property
    def static_quest_data(self) -> dict[str, Quest] | None:
        """Obtiene los datos estáticos de misiones cacheado."""
        return self._static_quest_data

    # ... Añade propiedades para otros tipos de datos estáticos ...

    @property
    def raw_user_data(self) -> Dict | None:
        """Obtiene los datos crudos de usuario desde la API."""
        return self._raw_user_data

    @property
    def raw_tasks_data(self) -> List | None:
        """Obtiene los datos crudos de tareas desde la API."""
        return self._raw_tasks_data

    @property
    def raw_tags_data(self) -> List | None:
        """Obtiene los datos crudos de etiquetas desde la API."""
        return self._raw_tags_data

    @property
    def raw_party_data(self) -> Dict | None:
        """Obtiene los datos crudos de party desde la API."""
        return self._raw_party_data

    def _is_cache_stale(self, last_refresh: datetime | None) -> bool:
        """Verifica si el cache está obsoleto basándose en el último tiempo de actualización."""
        if last_refresh is None:
            return True
        return datetime.now() - last_refresh > CACHE_TIMEOUT

    # --- Métodos de Carga Asíncrona para Datos 'VIVOS' ---
    async def load_user(self, force_refresh: bool = False) -> User:
        """Carga o actualiza los datos de usuario."""
        if self._user is None or force_refresh or self._is_cache_stale(self._user_last_refresh):
            log.info("Cargando datos de usuario desde la API")
            try:
                self._raw_user_data = await self.api.get_user_data()
                # Procesar en modelo
                # Asumiendo que User.model_validate maneja modelos anidados como UserPartyInfo, UserStats, etc.
                self._user = User.model_validate(self._raw_user_data)
                self._user_last_refresh = datetime.now()

                # Cachear datos crudos en disco si cache_dir está configurado
                if self.cache_dir:
                    self._save_to_cache("user.json", self._raw_user_data, is_raw=True)
                    # Guardar datos procesados por separado (User.to_dict o model_dump)
                    self._save_processed_model("user.json", self._user)

            except Exception as e:
                log.error(f"Fallo al cargar datos de usuario: {e}", exc_info=True)
                # Decide cómo manejar el fallo: lanzar excepción, retornar None, cargar desde cache?
                # Por ahora, relanzamos para indicar un fallo crítico
                raise

        return self._user

    async def load_tasks(self, force_refresh: bool = False) -> TaskList:
        """Carga o actualiza los datos de tareas."""
        if self._tasks is None or force_refresh or self._is_cache_stale(self._tasks_last_refresh):
            log.info("Cargando datos de tareas desde la API")
            try:
                self._raw_tasks_data = await self.api.get_tasks()

                # Procesar en modelo
                # Asumiendo que TaskList.from_api_data maneja la validación y creación de subclases de tarea
                self._tasks = TaskList.from_api_data(self._raw_tasks_data)
                self._tasks_last_refresh = datetime.now()

                # Cachear datos crudos en disco si cache_dir está configurado
                if self.cache_dir:
                    self._save_to_cache("tasks.json", self._raw_tasks_data, is_raw=True)
                    # Guardar datos procesados por separado (TaskList.to_json o model_dump)
                    self._save_processed_model("tasks.json", self._tasks)

            except Exception as e:
                log.error(f"Fallo al cargar datos de tareas: {e}", exc_info=True)
                raise  # Relanzar

        return self._tasks

    async def load_tags(self, force_refresh: bool = False) -> TagList:
        """Carga o actualiza los datos de etiquetas."""
        if self._tags is None or force_refresh or self._is_cache_stale(self._tags_last_refresh):
            log.info("Cargando datos de etiquetas desde la API")
            try:
                self._raw_tags_data = await self.api.get_tags()

                # Procesar en modelo
                # Asumiendo que TagList.from_raw_data existe
                self._tags = TagList.from_raw_data(self._raw_tags_data)
                self._tags_last_refresh = datetime.now()

                # Cachear datos crudos en disco si cache_dir está configurado
                if self.cache_dir:
                    self._save_to_cache("tags.json", self._raw_tags_data, is_raw=True)
                    # Guardar datos procesados por separado (TagList.to_dict o model_dump)
                    self._save_processed_model("tags.json", self._tags)

            except Exception as e:
                log.error(f"Fallo al cargar datos de etiquetas: {e}", exc_info=True)
                raise  # Relanzar

        return self._tags

    async def load_party(self, force_refresh: bool = False) -> Party | None:
        """Carga o actualiza los datos de party."""
        if self._party is None or force_refresh or self._is_cache_stale(self._party_last_refresh):
            log.info("Cargando datos de party desde la API")

            try:
                # Cargar datos crudos desde la API
                self._raw_party_data = await self.api.get_party_data()

                # Procesar en modelo
                # Asumiendo que Party.model_validate maneja QuestInfo, QuestProgress anidados, etc.
                # Podrías necesitar pasar el current_user_id a Party.create_from_raw_data si se usa
                # self._party = Party.create_from_raw_data(self._raw_party_data, self._user.id if self._user else None)
                # Usando model_validate directamente asume que Party valida chat/miembros internamente o no necesita user_id
                self._party = Party.model_validate(self._raw_party_data)
                self._party_last_refresh = datetime.now()

                # Cachear datos crudos en disco si cache_dir está configurado
                if self.cache_dir:
                    self._save_to_cache("party.json", self._raw_party_data, is_raw=True)
                    # Guardar datos procesados por separado (Party.to_json or model_dump)
                    self._save_processed_model("party.json", self._party)

            except Exception as e:
                log.warning(f"Fallo al cargar datos de party: {e}")
                self._party = None  # Establecer a None si la carga falla
                self._raw_party_data = None  # Limpiar datos crudos en caso de fallo

        return self._party

    # --- Métodos de Carga Asíncrona para Datos ESTÁTICOS (Usando StaticContentManager) ---
    async def load_static_gear_data(self, force_refresh_content: bool = False) -> dict[str, Gear]:
        """Carga datos estáticos de equipamiento usando StaticContentManager."""
        log.info("Cargando datos estáticos de equipamiento...")
        # Delegate the loading and caching logic to the StaticContentManager
        # The get_gear() method in StaticContentManager already returns the dict[str, Gear]
        # Pass force_refresh_content to the StaticContentManager's load_content method if it supports it
        # Note: StaticContentManager.get_gear() itself might call load_content internally.
        # If StaticContentManager.get_gear doesn't accept force_refresh_content, remove it here.
        # Based on your StaticContentManager code, get_gear does NOT accept force_refresh_content.
        self._static_gear_data = await self.static_content_manager.get_gear()
        log.info(f"Datos estáticos de equipamiento cargados. {len(self._static_gear_data) if self._static_gear_data else 0} items.")
        return self._static_gear_data

    async def load_all_static_quest_data(self, force_refresh_content: bool = False) -> dict[str, Quest]:
        """Carga todos los datos estáticos de misiones usando StaticContentManager."""
        log.info("Cargando todos los datos estáticos de misiones...")
        # Delegate the loading and caching logic to the StaticContentManager
        # The get_quests() method in StaticContentManager should return the dict[str, Quest]
        # Pass force_refresh_content to the StaticContentManager's load_content method if it supports it
        # Based on your StaticContentManager code, get_quests does NOT accept force_refresh_content.
        self._static_quest_data = await self.static_content_manager.get_quests()
        log.info(f"Todos los datos estáticos de misiones cargados. {len(self._static_quest_data) if self._static_quest_data else 0} misiones.")
        return self._static_quest_data

    # --- Método de Orquestación: Cargar Todos los Datos ('Vivos' + Estáticos) ---
    async def load_all_data(self, force_refresh: bool = False) -> None:
        """Carga todos los datos ('vivos' y estáticos) a la vez."""
        log.info("Iniciando load_all_data...")
        # Use force_refresh for both live and static data loading
        force_refresh_content = force_refresh  # Decide if static content refresh is tied to live data refresh

        # Create a list of coroutines to run concurrently
        loading_tasks = [
            # Call the methods and pass their return values (coroutines) to the list
            self.load_user(force_refresh),
            self.load_tasks(force_refresh),
            self.load_tags(force_refresh),
            self.load_party(force_refresh),  # load_party can return None
            # Pass the force_refresh_content flag to the static loading methods
            # Note: If StaticContentManager.load_content() is the method that accepts
            # force_refresh, you might need to call that directly here or ensure
            # get_gear/get_quests pass it down. For now, calling the methods as they are defined.
            self.load_static_gear_data(force_refresh_content=force_refresh_content),  # This method accepts the flag
            self.load_all_static_quest_data(force_refresh_content=force_refresh_content),  # This method accepts the flag
            # Add other static data loading methods here
        ]

        # Ejecutar todas las tareas de carga concurrentemente y esperar su finalización
        # El operador * desempaqueta la lista de corutinas en argumentos individuales para gather
        await asyncio.gather(*loading_tasks)

        log.info("load_all_data finalizado.")
        # Note: El procesamiento ocurre en un paso separado después de la carga.

    # --- Método de Orquestación: Procesar Datos Cargados ---
    # Este método asume que load_all_data ha sido llamado y ha poblado
    # los atributos internos (_user, _tasks, _tags, _static_gear_data, _static_quest_data).
    async def process_loaded_data(self) -> None:
        """Orquesta el procesamiento de los modelos de datos cargados.

        Incluye establecer nombres de etiquetas en tareas, calcular estadísticas efectivas del usuario,
        y calcular daño de tareas diarias basándose en datos estáticos de misiones.
        """
        # Verificar si los datos necesarios están cargados
        if not all([self._user, self._tasks, self._tags, self._static_gear_data, self._static_quest_data]):
            log.warning("No se pueden procesar los datos cargados - no se han cargado todos los datos requeridos.")
            # Registrar qué datos faltan para una mejor depuración
            missing = []
            if not self._user:
                missing.append("User")
            if not self._tasks:
                missing.append("Tasks")
            if not self._tags:
                missing.append("Tags")
            if not self._static_gear_data:
                missing.append("Static Gear Data")
            if not self._static_quest_data:
                missing.append("Static Quest Data")
            log.warning(f"Datos faltantes: {', '.join(missing)}")
            return  # No se puede continuar sin datos esenciales

        log.info("Iniciando process_loaded_data...")

        # 1. Procesar Usuario: calcular estadísticas efectivas (requiere datos estáticos de equipamiento)
        # User.calculate_effective_stats ahora es SÍNCRONO y acepta gear_data
        # Pasa el diccionario de datos estáticos de equipamiento cargado
        self._user.calculate_effective_stats(gear_data=self._static_gear_data)
        log.debug("Estadísticas efectivas del usuario calculadas.")

        # 2. Procesar Tareas: asignar etiquetas, calcular estados y DAÑO (requiere usuario, etiquetas, datos estáticos de misiones)
        # TaskList.process_task_statuses es ASÍNCRONO y acepta user, tags_provider, all_quest_data
        # Pasa el usuario, las etiquetas y los diccionarios de datos estáticos de misiones cargados
        await self._tasks.process_task_statuses(
            user=self._user, tags_provider=self._tags, all_quest_data=self._static_quest_data  # Pasa los datos estáticos de misiones cargados
        )
        log.debug("Tareas procesadas (etiquetas, estados, daño).")

        # Añade otros pasos de procesamiento aquí si es necesario

        log.info("process_loaded_data finalizado.")

        # Opcional: Guardar datos procesados después de que el procesamiento se complete
        if self.cache_dir:
            self._save_processed_model("user_processed.json", self._user)
            self._save_processed_model("tasks_processed.json", self._tasks)
            # Los modelos Party y Tags podrían no necesitar reprocesamiento aquí a menos que su estructura cambie basándose en datos estáticos

    # --- Helpers para Guardar/Cargar Cache ---
    def _save_to_cache(self, filename: str, data: Any, is_raw: bool = True) -> None:
        """Guarda datos en el directorio de cache."""
        if not self.cache_dir:
            return

        try:
            # Elige el directorio apropiado basándose en el tipo de datos
            target_dir = self.raw_cache_dir if is_raw else self.processed_cache_dir
            cache_path = target_dir / filename

            # Usa CustomJSONEncoder para datos crudos o datos no manejados por model_dump_json
            # Para modelos Pydantic, _save_processed_model es preferible
            with open(cache_path, "w", encoding="utf-8") as f:
                json.dump(data, f, cls=CustomJSONEncoder, indent=4)
            log.debug(f"Guardado datos {'crudos' if is_raw else 'procesados'} en cache: {cache_path}")
        except Exception as e:
            log.error(f"Fallo al guardar en cache {filename}: {e}")

    def _save_processed_model(self, filename: str, model: Any) -> None:
        """Guarda un modelo procesado en el directorio de cache procesado.

        Usa las características de serialización de Pydantic para manejar la conversión a JSON correctamente.
        """
        if not self.cache_dir:
            return

        try:
            json_str = None
            data_to_serialize = None  # Variable para almacenar la estructura (dict/list) si no se genera JSON directo

            # Prefer Pydantic's model_dump_json for models
            if hasattr(model, "model_dump_json"):
                json_str = model.model_dump_json(indent=4)
                log.debug(f"Using model_dump_json for {type(model)}")

            # Handle custom collections like TaskList which might have to_json returning a list/dict
            # or Pydantic models with model_dump
            # Check for model_dump first as it's standard Pydantic for getting dict representation
            elif hasattr(model, "model_dump"):
                data_to_serialize = model.model_dump()
                log.debug(f"Using model_dump for {type(model)}")
            # Fallback to custom to_json/to_dict methods that return data structures
            elif hasattr(model, "to_json"):
                # Assume to_json returns a serializable structure (list/dict), not a string
                data_to_serialize = model.to_json()
                log.debug(f"Using to_json (returning data structure) for {type(model)}")
            elif hasattr(model, "to_dict"):
                # Assume to_dict returns a serializable structure (list/dict)
                data_to_serialize = model.to_dict()
                log.debug(f"Using to_dict (returning data structure) for {type(model)}")
            elif hasattr(model, "__dict__"):  # Fallback, less preferred, might expose private attributes
                data_to_serialize = model.__dict__
                log.debug(f"Using __dict__ fallback for {type(model)}")
            else:
                log.warning(f"Could not find serialization method for model {type(model)}.")
                return  # Cannot save if no serialization method is found

            # If json_str was not generated directly (e.g., by model_dump_json),
            # serialize the data_to_serialize structure using json.dumps
            if json_str is None and data_to_serialize is not None:
                json_str = json.dumps(data_to_serialize, cls=CustomJSONEncoder, indent=4)
                log.debug(f"Serialized data structure using json.dumps for {type(model)}")

            # Final check if serialization produced a string
            if not isinstance(json_str, str):
                log.error(f"Serialization failed to produce a string for model {type(model)}. Result type: {type(json_str)}")
                return

            # Save the generated JSON string
            cache_path = self.processed_cache_dir / filename
            with open(cache_path, "w", encoding="utf-8") as f:
                f.write(json_str)
            log.debug(f"Saved processed model to cache: {cache_path}")

        except Exception as e:
            log.error(f"Fallo al guardar modelo procesado {filename}: {e}", exc_info=True)

    def _load_from_cache(self, filename: str, is_raw: bool = True) -> Any | None:
        """Carga datos desde el directorio de cache."""
        if not self.cache_dir:
            return None

        # Elige el directorio apropiado basándose en el tipo de datos
        target_dir = self.raw_cache_dir if is_raw else self.processed_cache_dir
        cache_path = target_dir / filename

        if not cache_path.exists():
            return None

        try:
            with open(cache_path, encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            log.error(f"Fallo al cargar desde cache {filename}: {e}")
            return None


# Note: La función main de ejemplo y el bloque if __name__ == "__main__":
# deberían estar en tu archivo principal de la aplicación (ej: main.py o cli.py),
# no típicamente dentro de los archivos de definición de servicio/modelo.
# La función main de ejemplo proporcionada en el prompt del usuario para este archivo
# ha sido eliminada aquí para mantener el archivo enfocado en la clase DataManager en sí.

-------- END OF FILE pixabit/models/data_manager.py --------

-------- START OF FILE pixabit/models/game_content.py --------
# pixabit/models/game_content.py

# SECTION: MODULE DOCSTRING
"""Manages Habitica's static game content (spells, gear, quests, etc.).

Provides Pydantic models for content items and a manager class (`GameContentCache`)
for fetching, caching (raw and processed), and accessing this data efficiently.
"""

# SECTION: IMPORTS
from __future__ import annotations

import asyncio  # Needed for lock
import json
import time
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Any

# External Libs
from pydantic import (
    BaseModel,
    Field,
    model_validator,
)

from pixabit.api.client import HabiticaClient

# Local Imports (assuming helpers and api are accessible)
from pixabit.config import (
    HABITICA_DATA_PATH,
    HABITICA_DATA_PROCESSED,
    HABITICA_DATA_RAW,
)
from pixabit.helpers._json import load_json, save_json
from pixabit.helpers._logger import log

# SECTION: CONSTANTS & CONFIG


# SECTION: PYDANTIC MODELS FOR CONTENT ITEMS


# KLASS: Spell
class Spell(BaseModel):
    """Habitica spell/skill model."""

    model_config = {"extra": "ignore", "populate_by_name": True}

    key: str
    text: str
    notes: str = Field("", description="In-game description/notes.")  # Alias notes -> description? Keep notes.
    mana: float = Field(0.0, description="Mana cost.")
    target: str | None = Field(None, description="Target type (e.g., 'self', 'user', 'party', 'task')")
    klass: str | None = Field(
        None,
        description="Associated class ('wizard', 'healer', etc.) or 'special'.",
    )
    lvl: int = Field(1, description="Level required to learn/use.")

    # Optional fields found in content data
    immediateUse: bool | None = False
    purchaseType: str | None = None
    value: int | None = 0
    previousPurchase: bool | None = False
    bulk: bool | None = False
    level_required: int | None = Field(alias="lvl", default=1)
    silent: bool | None = False
    limited: bool | None = False

    model_config = {
        "extra": "ignore",
        "populate_by_name": True,
        "arbitrary_types_allowed": True,  # Permitir tipos arbitrarios
    }

    # Opcionalmente, agrega un método de validación para diagnosticar problemas
    @model_validator(mode="after")
    def validate_gear(self) -> Gear:
        return self


class BossModel(BaseModel):
    """Model for boss properties."""

    defense: int = Field(alias="def")
    hp: int
    name: str
    strength: float = Field(alias="str")


class DropModel(BaseModel):
    """Model for drop properties."""

    exp: int
    gp: int
    items: dict | list | None = None


class UnlockConditionModel(BaseModel):
    """Model for unlock condition properties."""

    condition: str
    text: str


class Quest(BaseModel):
    """Habitica quest model."""

    # Required fields
    category: str
    completion: str
    key: str
    notes: str
    text: str
    drop: DropModel

    # Optional fields
    boss: BossModel | None = None
    goldValue: int | None = None
    group: str | None = None
    unlockCondition: UnlockConditionModel | None = None

    # Additional fields from first schema that weren't in the detailed analysis
    addlNotes: str | None = None
    completionChat: str | None = None
    collect: dict[str, Any] | None = None
    colors: dict[str, Any] | None = None
    prereqQuests: list[str] | None = None
    prerequisite: dict[str, Any] | None = None
    previous: str | None = None
    previous1: str | None = None
    lvl: int | None = None
    value: int | None = None

    model_config = {
        "extra": "ignore",
        "populate_by_name": False,
        "arbitrary_types_allowed": True,  # Permitir tipos arbitrarios
    }

    # Opcionalmente, agrega un método de validación para diagnosticar problemas
    @model_validator(mode="after")
    def validate_gear(self) -> Gear:
        return self


class Gear(BaseModel):
    """Habitica gear model."""

    constitution: int = Field(alias="con")
    intelligence: int = Field(alias="int")
    perception: int = Field(alias="per")
    strength: int = Field(alias="str")
    sett: str = Field(alias="set")
    index: str
    key: str
    specialClass: str | None = Field(None, alias="specialClass")
    notes: str
    text: str
    value: int

    # optional
    gear_type: str | None = Field(alias="type")
    event: dict | None = None
    gearSet: str | None = None
    last: bool | None = False
    mistery: str | None = None
    season: str | None = None
    twoHanded: bool | None = False

    model_config = {
        "extra": "ignore",  # Ignorar campos extra
        "populate_by_name": True,  # Permitir completar por nombre
        "arbitrary_types_allowed": True,  # Permitir tipos arbitrarios
    }

    # Opcionalmente, agrega un método de validación para diagnosticar problemas
    @model_validator(mode="after")
    def validate_gear(self) -> Gear:
        return self


class GameContent(BaseModel):
    """Container for all game content we're interested in."""

    spells: dict[str, dict[str, Spell]]
    quests: dict[str, Quest]
    gear: dict[str, Gear]
    last_updated: datetime

    @classmethod
    def from_full_content(cls, content_data: dict[str, Any]) -> GameContent:
        """Create GameContent from the full Habitica content JSON."""
        # Process spells (grouped by class)
        processed_spells: dict[str, dict[str, Spell]] = {}
        for klass, spells_dict in content_data.get("spells", {}).items():
            if klass == "special":  # Optionally skip special spells
                continue

            processed_spells[klass] = {}
            for spell_key, spell_data in spells_dict.items():
                try:
                    processed_spells[klass][spell_key] = Spell(klass=klass, **spell_data)
                except Exception as e:
                    log.warning(f"Error processing spell {spell_key}: {e}")

        # Process quests (flat structure)
        processed_quests: dict[str, Quest] = {}
        for quest_key, quest_data in content_data.get("quests", {}).items():
            try:
                processed_quests[quest_key] = Quest(**quest_data)
            except Exception as e:
                log.warning(f"Error processing quest {quest_key}: {e}")

        # Process gear (flat structure)
        processed_gear: dict[str, Gear] = {}
        for gear_key, gear_data in content_data.get("gear", {}).get("flat", {}).items():
            try:
                processed_gear[gear_key] = Gear(**gear_data)
            except Exception as e:
                log.warning(f"Error processing gear {gear_key}: {e}")

        # Create the container with current timestamp
        return cls(
            spells=processed_spells,
            quests=processed_quests,
            gear=processed_gear,
            last_updated=datetime.now(),
        )


class StaticContentManager:
    """Manages Habitica game content data with efficient caching."""

    def __init__(
        self,
        data_dir: str | Path,
        full_content_filename: str = HABITICA_DATA_RAW,
        processed_content_filename: str = HABITICA_DATA_PROCESSED,
        cache_duration_days: int = 30,
    ):
        """Initialize the data manager.

        Args:
            data_dir: Directory for storing data files
            full_content_filename: Filename for full content JSON
            processed_content_filename: Filename for processed content JSON
            cache_duration_days: How often to refresh the content cache
        """
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True, parents=True)

        self.full_content_path = self.data_dir / full_content_filename
        self.processed_content_path = self.data_dir / processed_content_filename
        self.cache_duration = timedelta(days=cache_duration_days)

        self._content: GameContent | None = None

    async def load_content(self, force_refresh: bool = False) -> GameContent:
        """Load game content, using cached data if available and fresh."""
        # Check if we already have content loaded in memory
        if self._content and not force_refresh:
            return self._content

        # Try to load from processed cache first (if not forcing refresh)
        if not force_refresh and self.processed_content_path.exists():
            try:
                content_dict = json.loads(self.processed_content_path.read_text(encoding="utf-8"))
                last_updated_str = content_dict.get("last_updated")

                if last_updated_str:
                    last_updated = datetime.fromisoformat(last_updated_str)
                    if datetime.now() - last_updated < self.cache_duration:
                        log.info("Using cached processed content")
                        content_dict["last_updated"] = last_updated
                        self._content = GameContent.model_validate(content_dict)
                        return self._content
            except Exception as e:
                log.warning(f"Error loading processed content: {e}. Will try raw content.")

        # If we get here, try to load from raw content file
        if self.full_content_path.exists():
            try:
                log.info("Processing full content file")
                full_content = json.loads(self.full_content_path.read_text(encoding="utf-8"))
                self._content = GameContent.from_full_content(full_content)
                # Save processed content for future use
                self.save_processed_content()
                return self._content
            except Exception as e:
                log.warning(f"Error processing full content: {e}. Will fetch from API.")

        # If we get here, we need to fetch from API
        try:
            log.info("Fetching content from Habitica API")
            client = HabiticaClient()  # Assuming this is properly defined
            full_content = await client.get_content()

            # Note: this may need to be handled differently if not async

            # Save the raw content
            self.full_content_path.write_text(json.dumps(full_content, indent=2), encoding="utf-8")

            # Process and save
            self._content = GameContent.from_full_content(full_content)
            self.save_processed_content()
            return self._content
        except Exception as e:
            log.error(f"Failed to fetch content from API: {e}")
            raise RuntimeError("Could not load game content from any source") from e

    async def refresh_from_api(self) -> GameContent:
        """Force refresh content from Habitica API."""
        try:
            log.info("Forcing refresh from Habitica API")
            client = HabiticaClient()
            full_content = await client.get_content()

            # Save raw content
            self.full_content_path.write_text(json.dumps(full_content, indent=2), encoding="utf-8")

            # Process and cache
            self._content = GameContent.from_full_content(full_content)
            self.save_processed_content()
            return self._content
        except Exception as e:
            log.error(f"Failed to refresh content from API: {e}")
            raise

    def save_processed_content(self) -> None:
        """Save the processed content to a JSON file."""
        if not self._content:
            log.warning("No content to save")
            return

        # Convert to dict and ensure datetime is serializable
        content_dict = self._content.model_dump(by_alias=True)
        content_dict["last_updated"] = content_dict["last_updated"].isoformat()

        # Save to file
        self.processed_content_path.write_text(json.dumps(content_dict, indent=2), encoding="utf-8")

        log.info(f"Saved processed content to {self.processed_content_path}")

    async def get_spells(self, klass: str | None = None) -> dict[str | dict[str | Spell]] | dict[str | Spell]:
        """Get all spells or spells for a specific class."""
        content = await self.load_content()

        if klass:
            return content.spells.get(klass, {})
        return content.spells

    async def get_quests(self, key: str | None = None, category: str | None = None) -> dict[str, Quest]:
        """Get all quests or filter by category."""
        content = await self.load_content()
        result = content.quests

        if key:
            result = {k: q for k, q in result.items() if q.key == key}
        if category:
            result = {k: q for k, q in result.items() if q.category == category}
        return result

    async def get_gear(
        self,
        gear_type: str | None = None,
        klass: str | None = None,
        key: str | None = None,
    ) -> dict[str, Gear]:
        """Get gear items filtered by type and/or class."""
        content = await self.load_content()

        filtered_gear = content.gear

        if gear_type:
            filtered_gear = {k: g for k, g in filtered_gear.items() if g.gear_type == gear_type}

        if klass:
            filtered_gear = {k: g for k, g in filtered_gear.items() if g.klass == klass or g.klass is None}  # Include classless gear

        if key:
            filtered_gear = {k: g for k, g in filtered_gear.items() if g.key == key}  # Include classless gear

        return filtered_gear


# Example usage
async def main():
    # Initialize the data manager with a directory path

    data_manager = StaticContentManager(HABITICA_DATA_PATH)

    try:
        # Force fetch/refresh if you need to update from Habitica API
        # (This would be done separately, not shown here)
        api = HabiticaClient()
        raw = await api.get_content()
        # Load the content
        content = await data_manager.load_content()
        # Example: Get all warrior spells
        warrior_spells = await data_manager.get_spells("warrior")
        print(f"Found {len(warrior_spells)} warrior spells")

        # Example: Get pet quests
        pet_quests = await data_manager.get_quests(category="pet")
        print(f"Found {len(pet_quests)} pet quests")

        # Example: Get wizard armor
        wizard_armor = await data_manager.get_gear(gear_type="armor", klass="wizard")
        print(f"Found {len(wizard_armor)} wizard armor items")

    except Exception as e:
        log.error(f"Error in main: {e}")


if __name__ == "__main__":
    # Necesitamos ejecutar la función main asíncrona
    import asyncio

    asyncio.run(main())

-------- END OF FILE pixabit/models/game_content.py --------

-------- START OF FILE pixabit/models/message.py --------
# pixabit/models/message.py

# SECTION: MODULE DOCSTRING
"""Defines Pydantic models for Habitica messages (Inbox/PM and Group Chat).

Includes:
- `MessageSenderStyles`: Represents nested sender style information.
- `Message`: Represents an individual message entity.
- `MessageList`: Container class to manage a collection of Message objects,
  providing processing, sorting, filtering, and conversation grouping.
"""

# SECTION: IMPORTS
from __future__ import annotations

import logging
from collections import defaultdict
from datetime import datetime, timezone
from typing import (  # Use standard List/Dict
    Any,
    Dict,
    Iterator,
    List,
    Optional,
    cast,
)

# External Libs
import emoji_data_python
from pydantic import (  # Use | for Union
    BaseModel,
    ConfigDict,
    Field,
    FieldValidationInfo,
    ValidationError,
    ValidationInfo,  # Importar ValidationInfo para acceder al contexto
    field_validator,
    model_validator,
)

from pixabit.config import USER_ID

# Use standard Python logger
logger = logging.getLogger(__name__)


def determine_conversation_id(message: Message, current_user_id: str | None) -> str | None:
    """Calcula conversation ID based on processed Message object and current user context."""
    # Group chats are straightforward
    if message.group_id:
        return message.group_id

    # Private Message Logic - Requires current_user_id
    if not current_user_id:
        # Esto no debería pasar si MessageList validation requiere el contexto
        logger.debug(f"Cannot determine PM conversation ID for msg {message.id}: current_user_id not set in context.")
        return None

    sender = message.sender_id
    recipient = message.recipient_id  # ownerId for inbox owner

    if message.sent_by_me:  # rely on sent_by_me flag set during processing
        # I sent it. Conversation is with the recipient.
        if recipient and recipient != current_user_id:
            return recipient
        else:
            # If recipient is missing or is me on a sent message, can't determine the other party.
            logger.debug(f"Cannot determine recipient for sent message {message.id}. Recipient ID: {recipient}")
            # Usar un placeholder o None. None podría agruparlos como "Sin Conversación".
            # Un placeholder puede ser útil para debugging, pero None es más limpio para datos finales.
            return None  # Or f"unknown_recipient_{message.id[:6]}"
    elif sender and sender != current_user_id:
        # Someone else sent it to me (sender is not me). Conversation is with the sender.
        return sender
    elif sender == "system":
        return "system"  # Assign a specific ID for system messages
    else:
        # Unknown state: sender missing, or sender is me but sent_by_me is false (logic error?).
        logger.debug(f"Could not determine conversation partner for message {message.id} (Sender: {sender}, Recipient: {recipient}, SentByMe: {message.sent_by_me}).")
        return None  # Or f"unknown_state_{message.id[:6]}"


# SECTION: PYDANTIC SUB-MODELS


# KLASS: MessageSenderStyles
class MessageSenderStyles(BaseModel):
    """Represents nested user style information often found in message data."""

    # Allow other fields related to styles (backgrounds, hair, etc.)
    model_config = ConfigDict(populate_by_name=True)

    # We only explicitly define fields we might want direct access to.
    # Example: Extract class if it's nested under 'stats'
    klass: str | None = Field(None, alias="class", description="Sender's class, if available.")

    @model_validator(mode="before")
    @classmethod
    def extract_nested_class(cls, data: Any) -> Any:
        """Extracts 'class' from a nested 'stats' dictionary if present."""
        if isinstance(data, dict):
            stats_data = data.get("stats")
            if isinstance(stats_data, dict):
                # Assign to alias 'class' so Pydantic maps it to 'klass' field
                data["class"] = stats_data.get("class")
        return data


# SECTION: MAIN MESSAGE MODEL


# KLASS: Message
class Message(BaseModel):
    """Represents an individual message in Habitica (Inbox or Group Chat)."""

    # Allow extra fields, use aliases where API keys differ from desired field names
    model_config = ConfigDict(extra="allow", populate_by_name=True)

    # Core IDs & Context
    # Made 'id' required; validator handles _id mapping
    id: str = Field(..., description="Unique message document ID.")
    # Sender ID ('uuid'): 'system' for system messages, user UUID otherwise
    sender_id: str | None = Field(None, alias="uuid", description="UUID of the sender or 'system'.")
    # Group ID ('groupId'): 'party', guild ID, 'tavern', etc. None for PMs.
    group_id: str | None = Field(
        None,
        alias="groupId",
        description="ID of the group chat context, if any.",
    )
    # Recipient ID ('ownerId'): Primarily for inbox messages, user ID of the inbox owner.
    recipient_id: str | None = Field(
        None,
        alias="ownerId",
        description="User ID of the inbox owner (for PMs).",
    )

    # Sender Info (Partially from direct fields, partially nested)
    # 'user' field often holds display name in chat messages
    sender_display_name: str | None = Field(None, alias="user", description="Sender's display name (parsed).")
    # 'username' field often holds login name
    sender_username: str | None = Field(None, alias="username", description="Sender's login name.")
    # Nested style information
    sender_styles: MessageSenderStyles | None = Field(None, alias="userStyles", description="Sender's style info.")

    # Content & Timestamp
    text: str = Field("", description="Formatted message text (parsed).")
    unformatted_text: str | None = Field(None, alias="unformattedText", description="Raw source text (parsed).")
    # Timestamp can be number (ms) or ISO string
    # No need for explicit serializer here, model_dump(mode='json') handles datetime
    timestamp: datetime | None = Field(None, description="Timestamp message sent/received (UTC).")

    # Engagement & Flags
    # Store likes/flags as dict {user_id: bool/timestamp} - bool is simpler if only presence matters
    likes: dict[str, bool] = Field(
        default_factory=dict,
        description="Dictionary of user IDs who liked the message.",
    )
    flags: dict[str, bool] = Field(
        default_factory=dict,
        description="Dictionary of user IDs who flagged the message.",
    )
    flag_count: int = Field(0, alias="flagCount", description="Total number of flags.")

    # System Message Info
    # 'info' field contains data for system messages (spell casts, quest progress, etc.)
    info: dict[str, Any] | None = Field(None, description="Data for system messages.")

    # --- Fields calculated/set externally AFTER initial validation ---
    # These fields will be set by the MessageList validator
    # Keep exclude=True as they are derived state, not part of raw API message data
    sent_by_me: bool | None = Field(
        None,
        exclude=True,
        description="True if message was sent by the current user context.",
    )
    conversation_id: str | None = Field(
        None,
        exclude=True,
        description="Grouping ID (group_id or other user's ID in PMs).",
    )

    # --- Validators ---

    @model_validator(mode="before")
    @classmethod
    def handle_id_mapping(cls, data: Any) -> Any:
        """Map '_id' to 'id' if 'id' isn't already present."""
        if isinstance(data, dict):
            if "_id" in data and "id" not in data:
                data["id"] = data["_id"]
            # Ensure 'id' exists after mapping (as it's required)
            # Pydantic's core validation will enforce 'id' is not None after this.
            # No need to raise here, Pydantic handles missing required fields.
        return data

    # Consolidate text parsing
    @field_validator("text", "unformatted_text", "sender_display_name", mode="before")
    @classmethod
    def parse_text_emoji(cls, value: Any) -> str | None:
        """Parses text fields and replaces emoji shortcodes."""
        if isinstance(value, str):
            return emoji_data_python.replace_colons(value)
        # Return None for optional fields if input is None, empty string otherwise
        # Pydantic handles Optional[str] = None correctly.
        return None if value is None else ""

    # Robust timestamp parsing
    @field_validator("timestamp", mode="before")
    @classmethod
    def parse_timestamp_robust(cls, value: Any) -> datetime | None:
        """Parses ISO string or epoch milliseconds/seconds into UTC datetime."""
        # Reuse date helper if available
        # return convert_timestamp_to_utc(value)

        # Inline logic:
        dt: datetime | None = None
        if isinstance(value, str):
            try:
                dt = datetime.fromisoformat(value.replace("Z", "+00:00"))
            except ValueError:
                pass  # Ignore string parse errors, try number next
        elif isinstance(value, (int, float)):
            try:
                # Assume ms if large number, otherwise seconds
                ts_sec = value / 1000.0 if abs(value) > 2e9 else value
                dt = datetime.fromtimestamp(ts_sec, tz=timezone.utc)
            except (ValueError, OSError, TypeError):
                pass  # Ignore invalid numbers
        elif isinstance(value, datetime):
            dt = value  # Accept datetime objects directly

        if dt:
            # Ensure timezone aware UTC
            if dt.tzinfo is None:
                return dt.replace(tzinfo=timezone.utc)
            return dt.astimezone(timezone.utc)
        else:
            # Only log if the original value wasn't None
            if value is not None:
                # Log at debug level, as invalid timestamps might be somewhat expected
                logger.debug(f"Could not parse timestamp: {value!r} (type: {type(value).__name__}). Setting to None.")
            return None

    # Ensure integer for flag_count
    @field_validator("flag_count", mode="before")
    @classmethod
    def ensure_int(cls, value: Any) -> int:
        """Ensures flag_count is an integer, defaulting to 0."""
        if value is None:
            return 0
        try:
            return int(value)
        except (ValueError, TypeError):
            # Log at debug level, as invalid flag counts might occur
            logger.debug(f"Could not parse flag_count: {value!r} (type: {type(value).__name__}). Setting to 0.")
            return 0

    # --- Properties ---

    @property
    def is_system_message(self) -> bool:
        """Checks if this is likely a system message."""
        # Check sender_id or presence of 'info' field
        return self.sender_id == "system" or bool(self.info)

    @property
    def sender_class(self) -> str | None:
        """Extracts sender's class from sender_styles, if available."""
        return self.sender_styles.klass if self.sender_styles else None

    # --- Methods ---
    def __repr__(self) -> str:
        """Provides a developer-friendly string representation."""
        sender = "System" if self.is_system_message else (self.sender_username or self.sender_id or "Unknown")
        ts = self.timestamp.strftime("%Y-%m-%d %H:%M") if self.timestamp else "NoTime"
        # Use post-processed conversation_id if available
        conv = f" ({self.conversation_id})" if self.conversation_id else ""
        text_preview = self.text[:30].replace("\n", " ") + ("..." if len(self.text) > 30 else "")
        return f"Message(id={self.id}, from='{sender}', time='{ts}{conv}', text='{text_preview}')"

    # to_dict and to_json methods are not typically needed on individual BaseModel
    # as model_dump() and model_dump_json() provide this functionality.
    # Keeping the old model_dump might confuse, remove it.


# SECTION: MESSAGE LIST CONTAINER (Now a BaseModel)


# KLASS: MessageList
class MessageList(BaseModel):  # Inherit from BaseModel
    """BaseModel container for managing Message objects.

    Processes raw message data, calculates context-dependent fields (like
    conversation IDs based on the current user), sorts messages, and provides
    filtering methods.
    """

    # Configure model
    model_config = ConfigDict(
        extra="forbid",  # Forbid extra fields at this level unless allowed by specific fields
        arbitrary_types_allowed=False,  # Should not need arbitrary types now
    )

    # The main data field - a list of Message objects
    # Pydantic will validate that the input list contains items
    # that can be converted to Message objects.
    messages: list[Message] = Field(default_factory=list, description="List of Message objects.")

    # --- Validators to process and set derived fields ---
    # This validator runs BEFORE Pydantic validates the list content for 'messages'.
    # 'value' here is the raw input list of dictionaries for the 'messages' field.
    @model_validator(mode="before")
    @classmethod
    def process_and_sort_messages(cls, value: Any, info: ValidationInfo) -> Any:
        """Processes raw message dictionaries, validates them into Message models,
        sets context-dependent fields (sent_by_me, conversation_id), and sorts.
        Requires 'current_user_id' in info.context.
        """
        # If the input 'value' is already a dictionary with a 'messages' key,
        # it might be data already structured for the model.
        # Handle the case where Pydantic might pass {'messages': [...]} directly.
        if isinstance(value, dict) and "messages" in value and isinstance(value["messages"], list):
            # Data is already in the expected format for the model, proceed with list validation
            # Need to pass the list content and context to the list validation mechanism.
            # However, a 'before' validator should typically transform the *entire* input.
            # Let's assume the input is always the raw list when nested in Party.
            # If the input IS already {'messages': [...]}, this validator might not be needed
            # or needs different logic. Assuming input is the raw list for 'chat' from Party.
            logger.debug("Input to MessageList validator looks like already structured data. Skipping transformation.")
            # Return the data as-is, Pydantic's list validation will handle messages list
            # NOTE: If this branch is taken, derived fields (sent_by_me, conversation_id)
            # WILL NOT be set here. They would need a separate 'after' validator on the list itself.
            # Let's stick to the original intent: process the raw list input.
            pass  # Fall through to process the list

        if not isinstance(value, list):
            # Pydantic will handle the TypeError if the input isn't a list.
            # Returning value here lets Pydantic handle the type error correctly.
            # logger.warning(f"Expected a list for messages, got {type(value).__name__}")
            return value  # Let Pydantic handle validation failure for non-list

        current_user_id = USER_ID
        if current_user_id is None:
            # If current_user_id is mandatory for processing, log a warning or raise error
            if info.context.get("is_party_chat_validation") is not True:  # Only warn/error if not Party validation context
                logger.warning("'current_user_id' not found in context for MessageList validation, cannot set derived fields.")
                # If it's truly impossible to proceed without user_id, you would raise here:
                # raise ValueError("'current_user_id' must be provided in validation context.")
            # Continue processing, but derived fields will be None where user_id is needed

        processed_messages: list[Message] = []
        # Store messages that failed validation or processing
        failed_messages: list[Any] = []

        for item in value:
            try:
                # 1. Validate the raw item into a Message object
                # This relies on Message's own validators (like timestamp parsing)
                if isinstance(item, Message):
                    # If it's already a Message instance (e.g., from previous processing layer)
                    msg = item
                elif isinstance(item, dict):
                    # Validate raw dict into Message model
                    msg = Message.model_validate(item)
                else:
                    logger.warning(f"Skipping invalid message item type during MessageList processing: {type(item)}. Expected dict or Message.")
                    failed_messages.append(item)
                    continue  # Skip this item

                # 2. Post-processing: Calculate context-dependent fields using current_user_id
                if current_user_id:
                    # Calculate sent_by_me (Check sender_id first, then original 'sent' flag if available)
                    msg.sent_by_me = msg.sender_id == current_user_id
                    # Refined 'sent' flag logic: Only use if sender isn't known or contradicts
                    # Need access to original raw item here for 'sent' flag.
                    # This is a limitation of 'before' validator if you only receive validated items.
                    # If 'value' is always raw dicts, we can access the original item here.
                    # Assuming 'value' is the list of raw dicts:
                    if isinstance(item, dict):
                        original_sent_flag = item.get("sent", False)
                        # If we didn't identify by sender_id, fallback to original 'sent' flag
                        if not msg.sent_by_me and original_sent_flag is True:
                            msg.sent_by_me = True  # Override if original data says it was sent

                    # Determine conversation ID
                    # Pass required info: message object and current_user_id
                    msg.conversation_id = determine_conversation_id(msg, current_user_id)
                else:
                    # Cannot calculate context without user ID, fields remain None
                    msg.sent_by_me = None
                    # Fallback for conversation_id if user_id is missing (e.g., group ID only)
                    msg.conversation_id = msg.group_id  # Default to group_id if no user_id context

                processed_messages.append(msg)

            except ValidationError as e:
                # Log validation errors for individual messages
                item_id = getattr(item, "id", "unknown") if not isinstance(item, dict) else item.get("id") or item.get("_id", "unknown")
                logger.error(f"Validation failed for message ID {item_id} during MessageList processing: {e}")
                failed_messages.append(item)  # Store failed item
            except Exception as e:
                # Catch other unexpected errors during processing of a single message
                item_id = getattr(item, "id", "unknown") if not isinstance(item, dict) else item.get("id") or item.get("_id", "unknown")
                logger.error(
                    f"Error processing message {item_id} during MessageList processing: {e}",
                    exc_info=True,
                )
                failed_messages.append(item)  # Store failed item

        # Sort by timestamp AFTER processing
        # Use a default time for messages without a timestamp
        default_time = datetime.min.replace(tzinfo=timezone.utc)
        # messages might be empty, check before sorting
        if processed_messages:
            processed_messages.sort(key=lambda m: m.timestamp or default_time)
            logger.debug(f"Sorted {len(processed_messages)} messages by timestamp.")
        else:
            logger.debug("No messages to sort.")

        if failed_messages:
            logger.warning(f"Skipped {len(failed_messages)} messages due to errors.")
            # You might want to log or return the failed_messages list somewhere
            # info.context['failed_messages'] = failed_messages # Example: add to context

        # Return the list of validated and processed Message objects.
        # Pydantic will then assign this list to the 'messages' field.
        return {"messages": processed_messages}

    # --- Access and Filtering Methods ---
    # These methods operate on the validated 'self.messages' list

    def __len__(self) -> int:
        return len(self.messages)

    def __iter__(self) -> Iterator[Message]:
        return iter(self.messages)

    def __getitem__(self, index: int | slice) -> Message | list[Message]:
        return self.messages[index]

    def get_by_id(self, message_id: str) -> Message | None:
        """Finds a message by its unique ID."""
        return next((m for m in self.messages if m.id == message_id), None)

    def filter_by_sender(self, sender_id_or_name: str) -> list[Message]:
        """Returns messages sent by a specific user ID or username."""
        sender_id_or_name_lower = sender_id_or_name.lower()
        return [m for m in self.messages if m.sender_id == sender_id_or_name or (m.sender_username and m.sender_username.lower() == sender_id_or_name_lower)]

    def filter_by_conversation(self, conversation_id: str) -> list[Message]:
        """Returns messages belonging to a specific conversation ID (group or PM partner)."""
        # conversation_id is now a field on Message, set by the validator
        return [m for m in self.messages if m.conversation_id == conversation_id]

    def filter_by_group(self, group_id: str) -> list[Message]:
        """Returns messages belonging to a specific group ID (party, guild, tavern)."""
        return [m for m in self.messages if m.group_id == group_id]

    def filter_private_messages(self) -> list[Message]:
        """Returns messages likely Private Messages (no group_id)."""
        return [m for m in self.messages if m.group_id is None and not m.is_system_message]  # Exclude system messages too

    def filter_system_messages(self, include_system: bool = True) -> list[Message]:
        """Returns system messages or non-system messages."""
        return [m for m in self.messages if m.is_system_message is include_system]

    def filter_by_text(self, text_part: str, case_sensitive: bool = False) -> list[Message]:
        """Filters messages containing a specific text substring."""
        if not text_part:
            return self.messages  # Return all if text_part is empty

        if not case_sensitive:
            text_part_lower = text_part.lower()
            return [m for m in self.messages if text_part_lower in m.text.lower()]
        else:
            return [m for m in self.messages if text_part in m.text]

    def filter_by_date_range(self, start: datetime | None = None, end: datetime | None = None) -> list[Message]:
        """Filters messages within date/time range."""
        start_utc = start.astimezone(timezone.utc) if start and start.tzinfo else start.replace(tzinfo=timezone.utc) if start else None
        end_utc = end.astimezone(timezone.utc) if end and end.tzinfo else end.replace(tzinfo=timezone.utc) if end else None

        filtered = []
        for m in self.messages:
            if not m.timestamp:
                continue
            # Ensure message timestamp is timezone-aware for comparison
            msg_ts_utc = m.timestamp.astimezone(timezone.utc) if m.timestamp.tzinfo else m.timestamp.replace(tzinfo=timezone.utc)

            if start_utc and msg_ts_utc < start_utc:
                continue
            if end_utc and msg_ts_utc > end_utc:
                continue
            filtered.append(m)
        return filtered

    def filter_liked_by(self, user_id: str) -> list[Message]:
        """Returns messages liked by a specific user ID."""
        return [m for m in self.messages if user_id in m.likes]

    def filter_flagged(self, min_flags: int = 1) -> list[Message]:
        """Returns messages flagged at least `min_flags` times."""
        return [m for m in self.messages if m.flag_count >= min_flags]

    def get_conversations(self) -> dict[str, list[Message]]:
        """Groups messages by their calculated conversation ID."""
        grouped = defaultdict(list)
        for msg in self.messages:
            # Only group messages that have a determined conversation_id
            if msg.conversation_id:
                grouped[msg.conversation_id].append(msg)

        # Sort by most recent activity
        # Need to handle potential None timestamps for sorting
        sorted_ids = sorted(
            grouped.keys(),
            key=lambda cid: grouped[cid][-1].timestamp or datetime.min.replace(tzinfo=timezone.utc),  # Use min time for None timestamps
            reverse=True,
        )
        return {cid: grouped[cid] for cid in sorted_ids}

    # to_dict and to_json methods are redundant as model_dump/model_dump_json are available
    # Remove them to use standard Pydantic methods.
    # def to_dict(self) -> list[dict]: ... removed ...
    # def to_json(self) -> str: ... removed ...

    # __repr__ works the same
    def __repr__(self) -> str:
        """Simple representation."""
        # Use model_dump_json for a concise JSON representation if needed, or keep custom repr
        # return self.model_dump_json(indent=2) # Example of JSON repr
        return f"MessageList(count={len(self.messages)})"

-------- END OF FILE pixabit/models/message.py --------

-------- START OF FILE pixabit/models/party.py --------
# pixabit/models/party.py

# SECTION: MODULE DOCSTRING
"""Defines Pydantic models for Habitica Parties and related quest information.

Includes:
- `QuestProgress`: Holds progress data for an active quest.
- `QuestInfo`: Represents metadata about the party's current quest.
- `PartyMember`: Represents basic information about a party member.
- `Party`: Represents the main Party group object, potentially including members and chat.
"""

# SECTION: IMPORTS
from __future__ import annotations

import json
import logging
from datetime import datetime
from pathlib import Path
from typing import TYPE_CHECKING, Any

# External Libs
import emoji_data_python
from pydantic import (
    BaseModel,
    ConfigDict,
    Field,
    ValidationError,
    field_validator,
    model_validator,
)

from pixabit.api.client import HabiticaClient
from pixabit.config import HABITICA_DATA_PATH
from pixabit.helpers._json import load_json, save_json
from pixabit.helpers._logger import log
from pixabit.models.game_content import Quest, StaticContentManager
from pixabit.models.message import Message, MessageList

data_manager = StaticContentManager(HABITICA_DATA_PATH)
CACHE_DIR = Path("./pixabit_cache")  # Default cache dir fallback
if TYPE_CHECKING:
    pass


# SECTION: PYDANTIC SUB-MODELS
def encode_datetime_simple(obj):
    """Convert datetime object to ISO 8601 string using isoformat."""
    if isinstance(obj, datetime):
        return obj.isoformat()  # Usar isoformat estándar
    # Permite que otros errores de tipo se propaguen
    raise TypeError(f"Object of type {obj.__class__.__name__} is not JSON serializable")


# KLASS: QuestProgress
class QuestProgress(BaseModel):
    """Represents the progress within an active party quest."""

    model_config = ConfigDict(
        extra="ignore",
        populate_by_name=True,
        arbitrary_types_allowed=False,  # Prefer False unless strictly needed
    )

    up: float = Field(0.0, description="Boss damage dealt or positive habit progress.")
    down: float = Field(0.0, description="Damage taken or negative habit progress.")
    # Collection quest goal (e.g., {item_key: count_needed})
    collect: dict[str, int] = Field(default_factory=dict, description="Item collection goals.")
    # Use alias for collectedItems count
    collected_items: int = Field(0, alias="collectedItems", description="Items collected so far.")
    # Boss HP/Rage might be None if not applicable or not started
    hp: float | None = Field(None, description="Boss current HP.")
    rage: float | None = Field(None, description="Boss current Rage.")

    # Ensure numeric fields are correctly parsed as floats/ints
    @field_validator("up", "down", "hp", "rage", mode="before")
    @classmethod
    def ensure_float_or_none(cls, value: Any) -> float | None:
        """Ensures numeric progress fields are floats if present, else None."""
        if value is None:
            return None
        try:
            return float(value)
        except (ValueError, TypeError):
            log.debug(f"Could not parse quest progress value: {value}. Setting to 0.0")  # Cambiado a debug
            return 0.0

    @field_validator("collected_items", mode="before")
    @classmethod
    def ensure_int(cls, value: Any) -> int:
        """Ensures collected_items is an integer, defaulting to 0."""
        if value is None:
            return 0
        try:
            return int(value)
        except (ValueError, TypeError):
            log.debug(f"Could not parse collected_items value: {value}. Setting to 0.")  # Cambiado a debug
            return 0

    def __repr__(self) -> str:
        """Concise representation."""
        parts = []
        if self.hp is not None:
            parts.append(f"hp={self.hp:.1f}")
        if self.rage is not None:
            parts.append(f"rage={self.rage:.1f}")
        if self.up != 0.0:
            parts.append(f"up={self.up:.1f}")
        if self.down != 0.0:
            parts.append(f"down={self.down:.1f}")
        if self.collect:
            goal_str = ", ".join(f"{k}:{v}" for k, v in self.collect.items())
            parts.append(f"collect={self.collected_items}/[{goal_str}]")
        return f"QuestProgress({', '.join(parts)})"


# KLASS: QuestInfo
class QuestInfo(BaseModel):
    """Represents the information about the party's current quest."""

    model_config = ConfigDict(extra="ignore", populate_by_name=True)

    key: str | None = Field(None, description="Unique key for the quest (e.g., 'basilisk').")
    active: bool = Field(False, description="Is the quest active (invitation sent/accepted)?")
    # Use alias for RSVPNeeded
    rsvp_needed: bool = Field(
        False,
        alias="RSVPNeeded",
        description="Does leader need to accept invites?",
    )
    # Completion status (might be timestamp string, 'allGuilds', etc.)
    completed: str | None = Field(None, description="Completion status or timestamp.")
    # Use alias for leader ID
    leader_id: str | None = Field(None, alias="leader", description="User ID of quest leader/inviter.")
    # Member RSVP status {userId: bool}
    members: dict[str, bool | None] | None = Field(None, description="Member RSVP status.")
    # Additional quest-specific data
    extra: dict[str, Any] | None = Field(None, description="Extra quest data (e.g., webhook URLs).")

    # Nested progress model, defaults to empty QuestProgress
    progress: QuestProgress = Field(default_factory=QuestProgress)

    # Calculated property based on multiple fields
    @property
    def is_active_and_ongoing(self) -> bool:
        """Calculates if the quest is truly active AND not yet completed."""
        # A quest is ongoing if the 'active' flag is true AND the 'completed' field is missing/None/empty.
        return self.active and not self.completed

    def __repr__(self) -> str:
        """Concise representation."""
        status = "Ongoing" if self.is_active_and_ongoing else ("Completed" if self.completed else "Inactive")
        key_str = f"key='{self.key}'" if self.key else "NoKey"
        return f"QuestInfo({key_str}, status={status}, progress={self.progress})"


# KLASS: PartyMember (Basic Placeholder)
class PartyMember(BaseModel):
    """Represents a member within the party (basic info)."""

    # Needs to be populated from a separate endpoint or expanded user data
    model_config = ConfigDict(extra="ignore", arbitrary_types_allowed=False)
    id: str = Field(..., alias="_id")  # Assume member data has _id
    display_name: str | None = Field(None)  # Needs population from profile usually
    username: str | None = Field(None)
    level: int | None = Field(None)


# SECTION: MAIN PARTY MODEL


# KLASS: Party
class Party(BaseModel):
    """Represents a Habitica Party group object."""

    model_config = ConfigDict(
        extra="ignore",
        populate_by_name=True,
        # arbitrary_types_allowed=True, # Should not need this if MessageList is BaseModel
        arbitrary_types_allowed=False,  # Prefer False
    )

    # Core fields
    # ID might be missing in some contexts, allow None initially
    id: str = Field(..., description="Unique party ID.")  # Make ID required
    name: str = Field("Unnamed Party", description="Party name (parsed).")
    description: str | None = Field(None, description="Party description (parsed).")
    summary: str | None = Field(None, description="Party summary/tagline (parsed).")

    # Leader ID extracted by validator
    leader_id: str | None = Field(None, description="User ID of the party leader.")

    # Sorting info
    order: str | None = Field(None, description="Field used for sorting members (e.g., 'level').")
    order_ascending: bool | None = Field(None, alias="orderAscending", description="Sort direction.")

    # Nested QuestInfo model
    quest: QuestInfo = Field(default_factory=QuestInfo)

    # Nested MessageList BaseModel - Pydantic will validate this
    # Keep exclude=True if you don't want chat data in default dumps
    chat: MessageList | None = Field(None, description="Party chat messages.")

    # List of PartyMember BaseModels - Pydantic will validate list contents
    # Keep exclude=True if you don't want members data in default dumps
    member_count: int = Field(alias="memberCount")

    @model_validator(mode="before")
    @classmethod
    def handle_id_mapping(cls, data: Any) -> Any:
        """Map '_id' to 'id' if 'id' isn't already present."""
        if isinstance(data, dict):
            if "_id" in data and "id" not in data:
                data["id"] = data["_id"]
            # Ensure 'id' exists after mapping (as it's required)
            # Pydantic's core validation will enforce 'id' is not None after this.
        return data

    # Consolidate text parsing
    @field_validator("name", "description", "summary", mode="before")
    @classmethod
    def parse_text_emoji(cls, value: Any) -> str | None:
        """Parses text fields and replaces emoji shortcodes."""
        if isinstance(value, str):
            return emoji_data_python.replace_colons(value)
        # Return None for optional fields if input is None, handle default name below
        return None if value is None else ""

    # Ensure 'name' has a default if it ends up empty after parsing
    @model_validator(mode="after")
    def ensure_default_name(self) -> Party:
        """Ensure party has a valid name."""
        if not self.name:
            self.name = "Unnamed Party"
        return self

    # Extract leader ID
    @field_validator("leader_id", mode="before")
    @classmethod
    def extract_leader_id(cls, v: Any, info: Any) -> str | None:
        """Extracts leader ID from string or dict in raw data."""
        if isinstance(info.data, dict):
            leader_info = info.data.get("leader")  # Check original 'leader' key
            if isinstance(leader_info, str):
                return leader_info
            elif isinstance(leader_info, dict):
                # Check both possible ID keys within the leader object
                return leader_info.get("_id") or leader_info.get("id")
        # Return None if leader info isn't found or is in unexpected format
        return None

    # Normalize boolean for sort order
    @field_validator("order_ascending", mode="before")
    @classmethod
    def normalize_order_ascending(cls, v: Any) -> bool | None:
        """Normalizes 'orderAscending' from string 'true'/'false' or bool."""
        if isinstance(v, bool):
            return v
        if isinstance(v, str):
            return v.lower() == "true"
        return None  # Return None if not bool or recognized string

    # --- Methods ---
    # This factory method is crucial to pass context for MessageList validation
    @classmethod
    def create_from_raw_data(cls, raw_data: dict, current_user_id: str | None = None) -> Party:
        """Create a Party object from raw data, handling chat/members validation via Pydantic."""
        if not isinstance(raw_data, dict):
            raise ValueError(f"Expected dict, got {type(raw_data)}")

        # Pydantic will now validate 'chat' and 'members' automatically
        # We need to pass the current_user_id in the context for MessageList validation
        # Add a flag to context to indicate this is Party chat validation,
        # which might have specific requirements for user_id presence.
        context = {
            "current_user_id": current_user_id,
            "is_party_chat_validation": True,
        }

        try:
            # Use model_validate to process the raw data, including nested models
            # Pydantic will find the 'chat' key in raw_data and try to validate it
            # as a MessageList, passing the context down.
            # Pydantic will find the 'members' key and validate it as list[PartyMember].
            party_instance = cls.model_validate(raw_data, context=context)
            return party_instance
        except ValidationError as e:
            log.error(f"Validation failed while creating Party from raw data: {e}")
            raise  # Re-raise the specific Pydantic error
        except Exception as e:
            log.error(
                f"Unexpected error creating Party from raw data: {e}",
                exc_info=True,
            )
            raise  # Re-raise other exceptions

    def get_chat_messages(
        self,
    ) -> list[Message]:  # Return list of Message objects
        """Devuelve los mensajes del chat si existen."""
        # Access the validated messages list directly from the MessageList BaseModel
        if self.chat is None:
            return []
        return self.chat.messages  # chat is now a MessageList BaseModel instance

    def to_json(
        self,
        file_path: str | Path | None = None,
        include_chat: bool = False,
    ) -> str | None:
        """Serializes the Party to JSON with control over included fields.

        Args:
            file_path: Optional path to save JSON file
            include_chat: Whether to include chat data (defaults to False)

        Returns:
            JSON string if file_path is None, otherwise None
        """
        # Build the 'exclude' dictionary for model_dump.
        # We start with an empty dict and add entries for fields
        # where we want to OVERRIDE the default exclude=True behavior.
        exclude_override: dict[str, bool] = {}

        # If include_chat is True, we want to force 'chat' to be included

        # If include_members is True, we want to force 'members' to be included

        log.debug(f"to_json: Final model_dump exclude parameter: {exclude_override}")  # ADDED LOG

        try:
            # Use model_dump in 'json' mode. This mode automatically serializes
            # BaseModel, list[BaseModel], dict, datetime, UUID, etc. to JSON-compatible types.
            # Since MessageList is now a BaseModel, model_dump(mode='json') will call
            # message_list_instance.model_dump(mode='json'), which will call
            # message_instance.model_dump(mode='json') for each message.
            # This should handle datetime serialization automatically.
            json_data_dict = self.model_dump(mode="json", exclude=exclude_override, exclude_none=True)
            log.debug(f"to_json: model_dump resulted in dict with keys: {list(json_data_dict.keys())}")  # ADDED LOG
            if "chat" in json_data_dict:  # ADDED LOG
                log.debug(
                    f"to_json: 'chat' key IS present in model_dump output. It has {len(json_data_dict['chat']['messages']) if isinstance(json_data_dict.get('chat'), dict) and isinstance(json_data_dict['chat'].get('messages'), list) else 'an unexpected structure'} messages."
                )  # ADDED LOG
            else:  # ADDED LOG
                log.debug("to_json: 'chat' key is NOT present in model_dump output.")  # ADDED LOG

            # The previous KeyErrors related to json.dumps with default handler
            # should be resolved because model_dump(mode='json') produces
            # a structure that is already fully JSON serializable *before*
            # reaching json.dumps.
            json_str = json.dumps(json_data_dict, indent=2)

        except Exception as e:  # Catch any errors during dump or json.dumps
            log.error(f"Error during Party JSON serialization: {e}", exc_info=True)
            raise  # Re-raise the error

        # Save to file if specified
        if file_path:
            file_path = Path(file_path)
            file_path.parent.mkdir(parents=True, exist_ok=True)
            try:
                file_path.write_text(json_str, encoding="utf-8")
                log.info(f"Saved party data to {file_path}")
            except Exception as e:
                log.error(
                    f"Error saving Party JSON to file {file_path}: {e}",
                    exc_info=True,
                )
                raise  # Re-raise file writing error
            return None  # Return None when saving to file

        return json_str  # Return JSON string when not saving to file

    def active_quest_data(self, quest_data=StaticContentManager) -> dict[str, float]:
        """Calculates total effective stats including base, buffs, training, level bonus, and gear."""
        # Get gear data
        quest = {}
        active_quest_key = None

        # Acceso seguro a la clave de misión
        if self.party is not None and self.party.quest is not None:
            active_quest_key = self.party.quest.key

            try:
                quest = quest_data.get_quests(key=active_quest_key)
                log.debug(f"Fetched static details for quest key: {active_quest_key}")
                self.party.quest.details = quest

            except Exception as e:
                log.error(f"Error fetching static quest details for {active_quest_key}: {e}")
                quest = {}  # Asegurar que sea un dict vacío en caso de error

            return quest

    # __repr__ works the same
    def __repr__(self) -> str:
        """Concise representation."""
        quest_key = self.quest.key if self.quest else "N/A"
        quest_status = "Ongoing" if self.quest and self.quest.is_active_and_ongoing else ("Completed" if self.quest and self.quest.completed else "Inactive")
        quest_str = f"Quest(key='{quest_key}', status='{quest_status}')"

        # Acceso seguro a self.chat.messages.len
        chat_len = len(self.chat.messages) if self.chat and self.chat.messages else 0

        name_preview = self.name[:30].replace("\n", " ") + ("..." if len(self.name) > 30 else "")
        return f"Party(id='{self.id}', name='{name_preview}', members={self.member_count}, chat={chat_len}, {quest_str})"


# Resto del archivo (función main, if __name__ == "__main__":)


async def main():
    """Demo function to retrieve and process party data."""
    try:
        # Ensure cache directory exists
        cache_dir = Path(CACHE_DIR)
        cache_dir.mkdir(exist_ok=True, parents=True)

        # Get data from API
        api = HabiticaClient()
        # Assuming api.get_party_data() returns the raw dictionary structure
        raw_data = await api.get_party_data()

        # Save raw data (optional)
        raw_path = cache_dir / "party_raw.json"
        save_json(raw_data, raw_path)
        log.info(f"Saved raw party data to {raw_path}")

        # Usar el método factory para procesar todo correctamente
        current_user_id = "50f36c30-60c7-46f7-92d1-be0e7c7259d6"  # Replace with actual user ID context

        log.debug(f"Creating Party BaseModel from raw data with user ID: {current_user_id}")
        party_data = Party.create_from_raw_data(raw_data, current_user_id)
        log.info("Party BaseModel created successfully.")

        # Example of accessing data
        log.info(f"Party Name: {party_data.name}")
        log.info(f"Number of members: {party_data.member_count}")
        log.info(f"Number of chat messages: {len(party_data.get_chat_messages())}")  # Use the method
        log.info(f"Quest Status: {party_data.quest.is_active_and_ongoing}")

        # Save processed data - incluir chat y miembros
        processed_path = cache_dir / "processed_party.json"
        log.debug(f"Attempting to save processed party data (with chat and members) to {processed_path}")
        party_data.to_json(processed_path, include_chat=True)
        log.info(f"Processed party data (with chat) saved to {processed_path}")

        # También podemos guardar una versión más ligera sin chat
        light_path = cache_dir / "party_without_chat.json"
        log.debug(f"Attempting to save light party data (without chat) to {light_path}")
        party_data.to_json(light_path, include_chat=False)
        log.info(f"Light party data (without chat) saved to {light_path}")

        return party_data
    except Exception as e:
        # log.error(f"Error in main: {e}") # The error is already logged in the except block where it happens
        # Just print the error message or handle as needed
        log.error(f"Main execution failed: {e}")
        return None


if __name__ == "__main__":
    import asyncio

    # Configure basic logging for the main script
    logging.basicConfig(
        level=logging.DEBUG,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    )

    asyncio.run(main())

-------- END OF FILE pixabit/models/party.py --------

-------- START OF FILE pixabit/models/tag.py --------
# pixabit/models/tag.py

# SECTION: MODULE DOCSTRING
"""Defines Pydantic models for representing Habitica Tags."""

# SECTION: IMPORTS
from __future__ import annotations

import json
from pathlib import Path
from typing import Any, Literal

import emoji_data_python
from pydantic import BaseModel, Field, field_validator

from pixabit.api.client import HabiticaClient
from pixabit.helpers._json import load_json, save_json
from pixabit.helpers._logger import log

# Local Imports (assuming helpers and api are accessible)
try:
    from pixabit.config import (
        CACHE_DIR,  # Assuming a central config defines this
    )

except ImportError:

    CACHE_DIR = Path("./pixabit_cache")  # Default cache dir fallback
    log.warning("GameContent: Could not import helpers/api/config. Using fallbacks.")
CACHE_SUBDIR = "content"

# SECTION: TAG MODELS


# KLASS: Tag
class Tag(BaseModel):
    """Represents a Habitica Tag."""

    id: str
    name: str
    challenge: bool = False
    position: int | None = None

    model_config = {
        "extra": "ignore",  # Ignore extra fields
        "validate_assignment": True,  # Validate when attributes are assigned
    }

    # --- Validators ---
    @field_validator("name", mode="before")
    @classmethod
    def parse_name_emoji(cls, value: Any) -> str:
        """Parses tag name and replaces emoji shortcodes."""
        if isinstance(value, str):
            return emoji_data_python.replace_colons(value)
        log.warning(f"Received non-string value for tag name: {value!r}. Using empty string.")
        return ""  # Return empty string if name is not a string

    def __repr__(self) -> str:
        """Concise representation."""
        name_preview = self.name.replace("\n", " ")
        return f"Tag(id='{self.id}', name='{name_preview}')"


class TagManager:
    def create_tag(tags):
        tags: list[dict] = []
        for tag in tags:
            Tag(tag)
            tags.append(Tag(tag))


# KLASS: TagList
class TagList(BaseModel):
    """Container for managing a list of Tag objects."""

    tags: list[Tag] = Field(default_factory=list)

    @classmethod
    def from_raw_data(cls, raw_list: list[dict[str, Any]]) -> TagList:
        """Creates a TagList from raw dictionary data."""
        taglist = cls(tags=[Tag(**tag_data) for tag_data in raw_list])
        taglist.update_positions()
        return taglist

    def update_positions(self, force: bool = False) -> None:
        """Updates the position attribute for each tag based on its order in the list."""
        for i, tag in enumerate(self.tags):
            if force or tag.position is None:
                tag.position = i

    def add_tag(self, tag: Tag) -> None:
        """Adds a tag to the list and updates positions."""
        self.tags.append(tag)
        self.update_positions()

    def remove_tag(self, tag_id: str) -> bool:
        """Removes a tag by ID and updates positions."""
        before_len = len(self.tags)
        self.tags = [tag for tag in self.tags if tag.id != tag_id]
        if len(self.tags) < before_len:
            self.update_positions()
            return True
        return False

    def reorder_tags(self, tag_id: str, new_position: int) -> bool:
        """Moves a tag to a new position and updates all positions."""
        # Find the tag
        tag_to_move = self.get_by_id(tag_id)
        if not tag_to_move:
            return False

        # Remove and insert at new position
        self.tags.remove(tag_to_move)
        # Ensure position is within bounds
        new_position = max(0, min(new_position, len(self.tags)))
        self.tags.insert(new_position, tag_to_move)

        # Update all positions
        self.update_positions()
        return True

    def __len__(self) -> int:
        return len(self.tags)

    def __iter__(self):
        return iter(self.tags)

    def __getitem__(self, index: int | slice) -> Tag | list[Tag]:
        return self.tags[index]

    def get_by_id(self, tag_id: str) -> Tag | None:
        """Finds a tag by its ID."""
        return next((t for t in self.tags if t.id == tag_id), None)

    def get_user_tags(self) -> list[Tag]:
        """Returns only the user-created tags."""
        return [t for t in self.tags if t.challenge is False]

    def get_challenge_tags(self, challenge_id: str | None = None) -> list[Tag]:
        """Returns challenge tags, optionally filtered by a specific challenge ID."""
        return [t for t in self.tags if t.challenge is True]

    def save_to_json(self, file_path: str | Path) -> None:
        """Save the tag list to a JSON file."""
        if not self.tags:
            log.warning("No tags to save")
            return

        # Crear el directorio si no existe
        file_path = Path(file_path)
        file_path.parent.mkdir(exist_ok=True, parents=True)

        # Usar model_dump para convertir a diccionarios
        tags_data = [tag.model_dump() for tag in self.tags]

        # Guardar a archivo
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(tags_data, indent=2, fp=f)

        log.info(f"Saved {len(self.tags)} tags to {file_path}")

    def model_dump(self):
        """Dumps the model to a dictionary."""
        return {"tags": [tag.model_dump() for tag in self.tags]}

    def __repr__(self) -> str:
        user_count = len(self.get_user_tags())
        chal_count = len(self.get_challenge_tags())
        return f"TagList(user_tags={user_count}, challenge_tags={chal_count}, total={len(self.tags)})"


async def main():
    """Demo function to retrieve and process tags."""
    try:
        # Asegurar que el directorio de caché existe
        cache_dir = Path(CACHE_DIR) / CACHE_SUBDIR
        cache_dir.mkdir(exist_ok=True, parents=True)

        # Obtener tags desde la API
        api = HabiticaClient()
        raw_tags = await api.get_tags()

        # Guardar datos sin procesar
        raw_path = cache_dir / "tags.json"
        save_json(raw_tags, raw_path)

        # Procesar tags en modelos
        tag_list = TagList.from_raw_data(raw_tags)

        # Guardar tags procesados
        processed_path = cache_dir / "processed_tags.json"
        tag_list.save_to_json(processed_path)

        # Mostrar información
        print(f"Found {len(tag_list)} tags")
        print(f"User tags: {len(tag_list.get_user_tags())}")
        print(f"Challenge tags: {len(tag_list.get_challenge_tags())}")

        return tag_list
    except Exception as e:
        log.error(f"Error in main: {e}")
        return None


if __name__ == "__main__":
    import asyncio

    asyncio.run(main())

-------- END OF FILE pixabit/models/tag.py --------

-------- START OF FILE pixabit/models/tag_factory.py --------
# pixabit/models/tag.py

# SECTION: MODULE DOCSTRING
"""Defines Pydantic models for representing Habitica Tags."""

# SECTION: IMPORTS
from __future__ import annotations

# --- Python Standard Library Imports ---
import json
import logging
import re
from collections import defaultdict
from functools import lru_cache
from pathlib import Path
from typing import (
    Any,
    ClassVar,
    Dict,
    Iterable,
    Iterator,
    List,
    Literal,
    Optional,
    Union,
)

import emoji_data_python
from pydantic import (
    BaseModel,
    ConfigDict,
    Field,
    field_validator,
    model_validator,
)

# Use standard Python logger
logger = logging.getLogger(__name__)

import tomllib  # Python 3.11+, use tomli for older versions

# --- Third-Party Library Imports ---
from pydantic import TypeAdapter

from pixabit.helpers._logger import log

# SECTION: CONSTANTS

# Maps special symbols found in tag text to short attribute names
ATTRIBUTE_SYMBOL_MAP: dict[str, str] = {
    "🜄": "con",  # Water symbol maps to Constitution
    "🜂": "str",  # Fire symbol maps to Strength
    "🜁": "int",  # Air symbol maps to Intelligence
    "🜃": "per",  # Earth symbol maps to Perception
    "᛭": "legacy",  # Nordic cross symbol maps to Legacy
}

# Maps configuration keys to short attribute names
ATTRIBUTE_MAP: dict[str, str] = {
    "ATTR_TAG_STR_ID": "str",
    "ATTR_TAG_INT_ID": "int",
    "ATTR_TAG_CON_ID": "con",
    "ATTR_TAG_PER_ID": "per",
    "LEGACY_TAG_ID": "legacy",
    "CHALLENGE_TAG_ID": "challenge",
    "PERSONAL_TAG_ID": "personal",
}

# Precompile regex for efficiency
ATTRIBUTE_SYMBOL_REGEX = re.compile(
    f"([{''.join(ATTRIBUTE_SYMBOL_MAP.keys())}])"
)


# SECTION: BASE MODELS


class Tag(BaseModel):
    """Represents a basic tag with common attributes."""

    id: str
    text: str
    tag_type: str = "base"
    challenge: bool = False
    parent: str | None = None
    category: str | None = None
    attribute: str = "str"
    position: int | None = None

    model_config = {
        "extra": "ignore",  # Ignore extra fields
        "validate_assignment": True,  # Validate when attributes are assigned
    }

    @model_validator(mode="before")
    @classmethod
    def _prepend_category_to_text(
        cls, values: dict[str, Any]
    ) -> dict[str, Any]:
        """Prepend category to text if not already bracketed."""
        if "category" in values and "text" in values:
            if not values["text"].startswith("["):
                values["text"] = f"[{values['category']}] {values['text']}"
        return values

    @property
    def display_name(self) -> str:
        """Return a formatted display name for the tag."""
        return self.text

    def is_parent(self) -> bool:
        """Check if this is a parent tag."""
        return self.tag_type == "parent"

    def is_subtag(self) -> bool:
        """Check if this is a subtag."""
        return self.tag_type == "sub"

    def get_parent_id(self) -> str | None:
        """Get the parent ID for this tag."""
        return self.parent


class MomTag(Tag):
    """Represents a parent tag, typically corresponding to a core attribute."""

    tag_type: str = "parent"  # Override default value

    @property
    def base_name(self) -> str:
        """Returns the text of the parent tag."""
        return self.text


class SubTag(Tag):
    """Represents a subtag, usually associated with a parent MomTag."""

    tag_type: str = "sub"  # Override default value


# SECTION: TAG FACTORY


class TagFactory:
    """Factory for creating Tag, MomTag, or SubTag instances."""

    def __init__(self, config_path: str | Path):
        """Initialize factory with configuration from TOML file."""
        # Convert to Path if string
        if isinstance(config_path, str):
            config_path = Path(config_path)

        # Load configuration
        with config_path.open("rb") as f:
            data = tomllib.load(f)

        # Store mappings
        self.name_to_id: dict[str, str] = data.get("tags", {})

        # Create ID to attribute mapping
        self.id_to_attribute: dict[str, str] = {
            self.name_to_id.get(config_name): attr
            for config_name, attr in ATTRIBUTE_MAP.items()
            if config_name in self.name_to_id
        }

        # Reverse mapping
        self.id_to_name: dict[str, str] = {
            v: k for k, v in self.name_to_id.items()
        }

    def detect_type(self, tag_id: str, tag_text: str) -> tuple[str, str | None]:
        """Determine tag type and parent based on ID and text."""
        # Check if it's a parent tag
        if tag_id in self.id_to_attribute:
            return "mom", None

        # Check for attribute symbol in text
        match = ATTRIBUTE_SYMBOL_REGEX.search(tag_text)
        if match:
            symbol = match.group(1)
            attr = ATTRIBUTE_SYMBOL_MAP.get(symbol)
            if attr:
                # Find parent ID with matching attribute
                for parent_id, attr_name in self.id_to_attribute.items():
                    if attr_name == attr:
                        return "sub", parent_id

        # Default to base tag
        return "base", None

    def create_tag(self, data: dict, position: int | None = None) -> Tag:
        """Create appropriate Tag object from raw data."""
        # Extract basic data
        tag_id = data.get("id")
        data["text"] = data.get("text") or data.get("name", "")
        tag_text = data.get("text", "")

        # Add position if provided
        if position is not None:
            data["position"] = position

        # Determine tag type
        tag_type, parent_id = self.detect_type(tag_id, tag_text)

        # Set attribute if applicable
        attribute = self.id_to_attribute.get(tag_id)
        if attribute:
            data["attribute"] = attribute
            data["category"] = attribute

        # Create appropriate tag class
        if tag_type == "mom":
            return MomTag.model_validate(data)
        elif tag_type == "sub":
            return SubTag.model_validate({**data, "parent": parent_id})
        else:
            return Tag.model_validate(data)


# SECTION: TAG LIST


class TagList:
    """A list-like collection of Tag objects with additional utility methods."""

    def __init__(self, tags: list[Tag]):
        """Initialize with a list of tags."""
        self.tags = tags
        # Reset cache when creating new instance
        self.get_tag_by_id.cache_clear()

    @classmethod
    def from_raw_data(
        cls, raw_list: Iterable[dict], factory: TagFactory
    ) -> TagList:
        """Create TagList from raw data using a factory."""
        typed_tags = [
            factory.create_tag(item, i) for i, item in enumerate(raw_list)
        ]
        return cls(tags=typed_tags)

    @classmethod
    def from_json(cls, json_path: str | Path, factory: TagFactory) -> TagList:
        """Load TagList directly from a JSON file using a factory."""
        # Convert to Path if string
        if isinstance(json_path, str):
            json_path = Path(json_path)

        # Load JSON data
        with json_path.open(encoding="utf-8") as f:
            raw_data = json.load(f)

        # Validate data is a list
        if not isinstance(raw_data, list):
            raise TypeError(f"Expected list from JSON file {json_path}")

        # Create and return TagList
        return cls.from_raw_data(raw_data, factory)

    @classmethod
    def from_json_basic(cls, json_str: str) -> TagList:
        """Parse TagList directly from a JSON string using Pydantic."""
        parsed_tags = TypeAdapter(list[Tag]).validate_json(json_str)
        return cls(tags=parsed_tags)

    def as_dicts(self) -> list[dict]:
        """Convert tags to list of dictionaries."""
        return [tag.model_dump() for tag in self.tags]

    @property
    def parents(self) -> list[MomTag]:
        """Get all parent tags."""
        return [tag for tag in self.tags if isinstance(tag, MomTag)]

    def get_subtags_for_parent(self, parent_id: str) -> list[SubTag]:
        """Get all subtags for a specific parent ID."""
        return [
            tag
            for tag in self.tags
            if isinstance(tag, SubTag) and tag.parent == parent_id
        ]

    def get_subtags_by_parent_prefix(self, parent_text: str) -> list[SubTag]:
        """Find subtags whose text starts with given parent text."""
        parent_text = parent_text.strip().lower()
        return [
            tag
            for tag in self.tags
            if isinstance(tag, SubTag)
            and tag.text.lower().startswith(parent_text + " ")
        ]

    def group_by_parent(self) -> dict[str, list[SubTag]]:
        """Group subtags by their parent ID."""
        grouped = defaultdict(list)
        for tag in self.tags:
            if isinstance(tag, SubTag) and tag.parent:
                grouped[tag.parent].append(tag)
        return dict(grouped)

    @lru_cache(maxsize=128)
    def get_tag_by_id(self, tag_id: str) -> Tag | None:
        """Find a tag by ID (cached for performance)."""
        return next((t for t in self.tags if t.id == tag_id), None)

    def filter_by_challenge(self, challenge: bool) -> list[Tag]:
        """Filter tags by challenge flag."""
        return [tag for tag in self.tags if tag.challenge == challenge]

    def filter_by_text(self, keyword: str) -> list[Tag]:
        """Filter tags by text content."""
        keyword_lower = keyword.lower()
        return [tag for tag in self.tags if keyword_lower in tag.text.lower()]

    def filter_by_type(self, tag_type: str) -> list[Tag]:
        """Filter tags by type."""
        return [tag for tag in self.tags if tag.tag_type == tag_type]

    def filter_by_attribute(self, attribute: str) -> list[Tag]:
        """Filter tags by attribute."""
        return [tag for tag in self.tags if tag.attribute == attribute]

    def sorted_by_position(self) -> list[Tag]:
        """Return tags sorted by position."""
        return sorted(
            self.tags, key=lambda t: t.position if t.position is not None else 0
        )

    # List-like methods
    def __iter__(self) -> Iterator[Tag]:
        """Make TagList iterable."""
        return iter(self.tags)

    def __getitem__(self, index) -> Tag:
        """Allow index access."""
        return self.tags[index]

    def __len__(self) -> int:
        """Get number of tags."""
        return len(self.tags)

    def __repr__(self) -> str:
        """String representation."""
        return f"TagList(count={len(self.tags)})"


# SECTION: LOADING FUNCTION


def load_tags_from_json(
    json_path: str | Path, config_path: str | Path
) -> TagList:
    """Load and process tags from JSON file using configuration."""
    try:
        # Create factory
        factory = TagFactory(config_path=config_path)

        # Create TagList directly from JSON file
        return TagList.from_json(json_path, factory)

    except FileNotFoundError as e:
        log.error(f"Error: Required file not found: {e.filename}")
        raise
    except json.JSONDecodeError:
        log.error(f"Error: Could not decode JSON file: {json_path}")
        raise
    except tomllib.TOMLDecodeError:
        log.error(f"Error: Could not decode TOML config file: {config_path}")
        raise
    except (KeyError, TypeError) as e:
        log.error(f"Error processing configuration or data structure: {e}")
        raise


# SECTION: DEMO CONTEXT MANAGER

from contextlib import contextmanager


@contextmanager
def tag_loading_context(json_path: str | Path, config_path: str | Path):
    """Context manager for safely loading tags with error handling."""
    try:
        # Setup
        log.info(f"Loading tags from {json_path}...")

        # Create factory
        factory = TagFactory(config_path=config_path)

        # Load JSON data
        if isinstance(json_path, str):
            json_path = Path(json_path)

        with json_path.open(encoding="utf-8") as f:
            tag_data = json.load(f)

        if not isinstance(tag_data, list):
            raise TypeError("Expected a list from JSON file")

        # Yield data and factory
        yield tag_data, factory

    except FileNotFoundError as e:
        log.error(f"Error: Required file not found: {e.filename}")
        yield None, None
    except json.JSONDecodeError:
        log.error(f"Error: Could not decode JSON file: {json_path}")
        yield None, None
    except tomllib.TOMLDecodeError:
        log.error(f"Error: Could not decode TOML config file: {config_path}")
        yield None, None
    except Exception as e:
        log.error(f"An unexpected error occurred: {e}")
        yield None, None
    finally:
        log.success("Tag loading operation completed.")


# --- Main Execution Example ---

# Define file paths using pathlib
TAG_JSON_PATH = Path("tags.json")
TAG_CONFIG_PATH = Path("tags.toml")

# Example 1: Using direct function
try:
    # Load tags directly
    taglist = load_tags_from_json(
        json_path=TAG_JSON_PATH, config_path=TAG_CONFIG_PATH
    )
    print(f"--- Successfully loaded TagList with {len(taglist)} tags ---")

    # Using enhanced TagList properties and methods
    print("\n--- Parent Tags ---")
    for parent_tag in taglist.parents:
        print(f"{parent_tag.id}: {parent_tag.text}")
        # Get and show subtags for this parent
        subtags = taglist.get_subtags_for_parent(parent_tag.id)
        print(f"  Subtags: {len(subtags)}")
        for subtag in subtags[:3]:  # Show first 3 subtags as example
            print(f"    - {subtag.text}")

except Exception as e:
    print(f"Error loading tags: {e}")

# Example 2: Using context manager
print("\n--- Using Context Manager ---")
with tag_loading_context(TAG_JSON_PATH, TAG_CONFIG_PATH) as (data, factory):
    if data and factory:
        taglist = TagList.from_raw_data(data, factory)
        print(f"Successfully loaded {len(taglist)} tags")

        # Example of cached lookup
        tag_id = data[0]["id"]  # Get first tag ID
        tag = taglist.get_tag_by_id(tag_id)
        print(f"First tag: {tag.display_name}")
    else:
        print("Failed to load tag data")

-------- END OF FILE pixabit/models/tag_factory.py --------

-------- START OF FILE pixabit/models/task.py --------
# pixabit/models/task.py

# SECTION: MODULE DOCSTRING
"""Defines Pydantic models for representing Habitica Tasks (Habits, Dailies, Todos, Rewards).

Includes models for nested structures like ChecklistItem and ChallengeData, and provides
a TaskList container for managing collections of Task objects.
"""
# SECTION: IMPORTS
from __future__ import annotations

import json
import logging
import math
from collections import defaultdict
from datetime import datetime, timezone
from enum import Enum
from typing import TYPE_CHECKING, Any, Dict, List, Literal  # Removed Optional, Union

import emoji_data_python
from pydantic import BaseModel, ConfigDict, Field, ValidationError, computed_field, field_validator, model_validator

from pixabit.helpers._logger import log
from pixabit.helpers._md_to_rich import MarkdownRenderer
from pixabit.helpers._rich import Text
from pixabit.helpers.DateTimeHandler import DateTimeHandler

# Avoid circular imports - only import types when type checking
if TYPE_CHECKING:
    # Assuming Tags is also defined elsewhere and needed for type hinting
    from pixabit.models.tag import Tag, TagList
    from pixabit.models.user import User
from pixabit.api import HabiticaClient
from pixabit.helpers._json import save_json

md_renderer = MarkdownRenderer()  # Create one instance
from pathlib import Path

# Use standard Python logger
HABITICA_DATA_PATH = Path("./pixabit_cache")  # Default cache dir fallback
import pydantic

print(f"Pydantic version: {pydantic.__version__}")

# SECTION: NESTED DATA MODELS


# KLASS: ChecklistItem
class ChecklistItem(BaseModel):
    """Represents a single item within a task's checklist."""

    model_config = ConfigDict(extra="ignore", populate_by_name=True)

    id: str | None = Field(None, description="Checklist item ID.")
    text: str = Field("", description="Text content of the item (parsed).")
    completed: bool = Field(False, description="Whether the item is checked off.")
    progress: int = Field(0, description="Progress value for the item.")

    @field_validator("text", mode="before")
    @classmethod
    def parse_text_emoji(cls, value: Any) -> str:
        """Parse text and replace emoji shortcodes."""
        if isinstance(value, str):
            return emoji_data_python.replace_colons(value)
        return ""

    def __repr__(self) -> str:
        status = "[X]" if self.completed else "[ ]"
        text_preview = self.text[:30].replace("\n", " ") + ("..." if len(self.text) > 30 else "")
        return f"ChecklistItem(id={self.id}, status='{status}', text='{text_preview}')"


# KLASS: ChallengeLinkData
class ChallengeLinkData(BaseModel):
    """Represents the challenge link information within a task."""

    model_config = ConfigDict(extra="ignore", populate_by_name=True)

    task_id: str | None = Field(None, alias="taskId", description="Original task ID within the challenge.")
    challenge_id: str | None = Field(None, description="Challenge ID.", alias="id")
    broken_reason: str | None = Field(None, alias="broken", description="Reason if the challenge link is broken.")
    is_broken: bool = Field(False, description="True if the challenge link is broken.")
    broken_status: Literal["task", "challenge", "unknown"] | None = Field(None, description="Categorized reason for breakage.")
    short_name: str | None = Field(None, alias="shortName", description="Challenge short name (parsed).")

    @field_validator("short_name", mode="before")
    @classmethod
    def parse_name_emoji(cls, value: Any) -> str | None:
        """Parse short_name and replace emoji shortcodes."""
        if isinstance(value, str):
            return emoji_data_python.replace_colons(value)
        return None

    @model_validator(mode="before")
    @classmethod
    def process_broken_status(cls, data: Any) -> dict[str, Any]:
        """Sets is_broken and broken_status based on the raw 'broken' field."""
        if not isinstance(data, dict):
            return data
        values = data.copy()
        broken_raw = data.get("broken")
        is_broken_flag = bool(broken_raw)
        broken_status_val = None

        if is_broken_flag and isinstance(broken_raw, str):
            reason = broken_raw.upper()
            if reason in ("TASK_DELETED", "CHALLENGE_TASK_NOT_FOUND"):
                broken_status_val = "task"
            elif reason in (
                "CHALLENGE_DELETED",
                "UNSUBSCRIBED",
                "CHALLENGE_CLOSED",
            ):
                broken_status_val = "challenge"
            else:
                broken_status_val = "unknown"
                log.debug(f"Unknown 'broken' reason encountered: {broken_raw}")

        values["is_broken"] = is_broken_flag
        values["broken_status"] = broken_status_val
        return values

    def __repr__(self) -> str:
        status = f", broken='{self.broken_status}' ({self.broken_reason})" if self.is_broken else ""
        name = f", name='{self.short_name}'" if self.short_name else ""
        return f"ChallengeLinkData(challenge_id={self.challenge_id}{name}{status})"


# SECTION: BASE TASK MODEL


# KLASS: Task
class Task(BaseModel):
    """Base model representing common attributes of a Habitica Task."""

    model_config = ConfigDict(extra="allow", populate_by_name=True, validate_assignment=True)

    id: str = Field(..., alias="_id", description="Unique task ID.")
    alias: str | None = Field(None, description="User-defined task alias (slug).")
    user_id: str | None = Field(None, alias="userId", description="UUID of the task owner.")
    text: str = Field("", description="Main text content of the task (parsed).")
    notes: str = Field("", description="Additional notes/description (parsed).")
    task_type: Literal["habit", "daily", "todo", "reward"] = Field(None, description="Type of the task.", alias="type")
    tags_id: list[str] = Field(default_factory=list, alias="tags", description="List of associated tag UUIDs.")

    value: float = Field(default=0.0, description="Task value (influences gold/exp/damage).")
    priority: float = Field(default=1.0, description="Task priority (0.1 Trivial to 2.0 Hard).")
    attribute: Literal["str", "int", "con", "per"] = Field("str", description="Associated attribute for stat gains.")

    created_at: datetime | None = Field(None, alias="createdAt", description="Timestamp created (UTC).")
    updated_at: datetime | None = Field(None, alias="updatedAt", description="Timestamp last updated (UTC).")
    reminders: list[dict[str, Any]] = Field(default_factory=list, description="List of reminder objects.")

    challenge: ChallengeLinkData | None = Field(None, description="Challenge linkage information, if any.")

    # --- Fields calculated/populated externally AFTER validation ---
    position: int | None = Field(None, description="Calculated display position relative to type.")
    calculated_status: str = Field("unknown", description="Calculated display status string.")
    styled_value: str | None = Field("", description="Rich style name based on task value.")

    # --- Validators ---

    # Ensure ID exists after potential _id mapping
    @model_validator(mode="before")
    @classmethod
    def check_and_assign_id(cls, data: Any) -> Any:
        """Map '_id' to 'id' and ensure 'id' exists."""
        if isinstance(data, dict):
            if "_id" in data and "id" not in data:
                data["id"] = data["_id"]
            if "id" not in data:
                raise ValueError("Task data must contain 'id' or '_id'")
        return data

    # Consolidate text parsing
    @field_validator("text", "notes", mode="before")
    @classmethod
    def parse_text_emoji(cls, value: Any) -> str:
        """Parse text fields and replace emoji shortcodes."""
        if isinstance(value, str):
            return emoji_data_python.replace_colons(value)
        return ""  # Default to empty string

    # Consolidate datetime parsing
    @field_validator("created_at", "updated_at", mode="before")
    @classmethod
    def parse_datetime_utc(cls, value: Any) -> datetime | None:
        """Parses various timestamp formats into UTC datetime."""
        handler = DateTimeHandler(timestamp=value)  # Usa timestamp=value para aclarar
        return handler.utc_datetime

    @property
    def styled_text(self) -> Text:
        """Returns the task text rendered as Rich Text (handles Markdown)."""
        return md_renderer.markdown_to_rich_text(self.text or "")

    @property
    def styled_notes(self) -> Text:
        """Returns the task notes rendered as Rich Text (handles Markdown)."""
        return md_renderer.markdown_to_rich_text(self.notes or "")

    @property
    def status(self) -> str:
        """Returns the calculated status string."""
        return self.calculated_status

    @computed_field
    @property
    def tag_names(self) -> list[str]:
        """Returns the resolved tag names."""
        return getattr(self, "_tag_names", [])

    @tag_names.setter
    def tag_names(self, value: list[str]) -> None:
        """Sets the resolved tag names."""
        self._tag_names = value

    def set_tag_names_from_tags(self, tags_provider: TagList) -> None:
        """Resolves tag IDs to tag names using the provided Tags provider."""
        self.tag_names = []
        for tag_id in self.tags_id:
            tag = tags_provider.get_by_id(tag_id)
            if tag:
                self._tag_names.append(tag.name)
            else:
                self._tag_names.append(f"Unknown:{tag_id[:6]}")

    def calculate_checklist_progress(self, checklist: list[ChecklistItem]) -> float:
        """Calculates proportion (0.0-1.0) of checklist items done."""
        if not checklist or not isinstance(checklist, list):
            return 1.0  # Treat no checklist as fully "done" for mitigation
        try:
            completed = sum(1 for item in checklist if isinstance(item, ChecklistItem) and item.completed)
            total = len(checklist)
            return completed / total if total > 0 else 1.0
        except Exception as e:
            log.warning(f"Error calculating checklist progress: {e}")
            return 1.0

    # --- Methods ---
    def __repr__(self) -> str:
        """Concise developer representation."""
        text_preview = self.text[:30].replace("\n", " ") + ("..." if len(self.text) > 30 else "")
        return f"{self.__class__.__name__}(id='{self.id}', text='{text_preview}', type='{self.task_type}')"


# SECTION: TASK SUBCLASSES


# KLASS: Habit
class Habit(Task):
    """Represents a Habit task with up/down counters and frequency."""

    model_config = ConfigDict(extra="ignore", populate_by_name=True)
    task_type: Literal["habit"] = "habit"

    up: bool = Field(True, description="Does this habit have a positive scoring direction (+)?")
    down: bool = Field(True, description="Does this habit have a negative scoring direction (-)?")
    counter_up: int = Field(0, alias="counterUp", description="Current positive counter.")
    counter_down: int = Field(0, alias="counterDown", description="Current negative counter.")
    frequency: str = Field("daily", description="Frequency for reset (e.g., 'daily', 'weekly').")
    history: list[dict[str, Any]] = Field(default_factory=list, description="Scoring history.")

    @field_validator("counter_up", "counter_down", mode="before")
    @classmethod
    def ensure_int(cls, value: Any) -> int:
        """Ensures counters are integers, defaulting to 0."""
        try:
            return int(value)
        except (ValueError, TypeError):
            return 0

    def __repr__(self) -> str:
        """Concise representation for Habit."""
        up_str = f"⬆️{self.counter_up}" if self.up else ""
        down_str = f"⬇️{self.counter_down}" if self.down else ""
        sep = " / " if self.up and self.down else ""
        counters = f"{up_str}{sep}{down_str}" if up_str or down_str else "No Score Dirs"
        text_preview = self.text[:30].replace("\n", " ") + ("..." if len(self.text) > 30 else "")
        return f"Habit(id='{self.id}', text='{text_preview}', counters='{counters}')"


# KLASS: Todo
class Todo(Task):
    """Represents a To-Do task with completion status, due date, and checklist."""

    model_config = ConfigDict(extra="ignore", populate_by_name=True)

    task_type: Literal["todo"] = "todo"
    completed: bool = Field(False, description="Whether the To-Do is marked complete.")
    completed_date: datetime | None = Field(None, alias="dateCompleted", description="Timestamp completed (UTC).")
    due_date: datetime | None = Field(None, alias="date", description="Due date timestamp (UTC).")
    collapse_checklist: bool = Field(False, alias="collapseChecklist", description="UI hint to collapse checklist.")
    checklist: list[ChecklistItem] = Field(default_factory=list, description="Sub-items for the To-Do.")

    @field_validator("completed_date", "due_date", mode="before")
    @classmethod
    def parse_todo_datetime_utc(cls, value: Any) -> datetime | None:
        """Parse date fields into UTC datetime objects."""
        handler = DateTimeHandler(timestamp=value)  # Usa timestamp=value para aclarar
        return handler.utc_datetime

    @property
    def is_past_due(self) -> bool:
        """Checks if the To-Do is past its due date and not completed."""
        if self.completed or not self.due_date:
            return False
        return self.due_date < datetime.now(timezone.utc)

    @property
    def display_status(self) -> Literal["Complete", "Past Due", "Due", "No Due Date"]:
        """Returns a user-friendly status string based on completion and due date."""
        if self.completed:
            return "Complete"
        if not self.due_date:
            return "No Due Date"
        if self.is_past_due:
            return "Past Due"
        return "Due"

    def calculate_progress(self) -> float:
        """Calculate progress based on checklist items or completion."""
        if self.completed:
            return 1.0
        if not self.checklist:
            return 0.0
        return self.calculate_checklist_progress(self.checklist)

    def __repr__(self) -> str:
        """Concise representation for Todo."""
        due_str = f", due={self.due_date.strftime('%Y-%m-%d')}" if self.due_date else ""
        checklist_str = f", checklist={len(self.checklist)} items" if self.checklist else ""
        text_preview = self.text[:30].replace("\n", " ") + ("..." if len(self.text) > 30 else "")
        return f"Todo(id='{self.id}', status='{self.display_status}', text='{text_preview}'{due_str}{checklist_str})"


# KLASS: Daily
class Daily(Task):
    """Represents a Daily task with completion status, streak, schedule, and checklist."""

    model_config = ConfigDict(extra="ignore", populate_by_name=True)

    task_type: Literal["daily"] = "daily"
    completed: bool = Field(False, description="Whether the Daily is completed for the current period.")
    is_due: bool = Field(False, alias="isDue", description="Is the Daily currently due?")
    streak: int = Field(0, description="Current completion streak.")
    collapse_checklist: bool = Field(False, alias="collapseChecklist")
    checklist: list[ChecklistItem] = Field(default_factory=list)
    start_date: datetime | None = Field(None, alias="startDate", description="Date the daily started (UTC).")

    # --- Scheduling Fields ---
    frequency: Literal["daily", "weekly", "monthly", "yearly"] = Field("weekly", description="Repeat frequency.")
    every_x: int = Field(1, alias="everyX", description="Repeat interval (e.g., every 'X' weeks).")
    days_of_month: list[int] = Field(default_factory=list, alias="daysOfMonth")
    weeks_of_month: list[int] = Field(default_factory=list, alias="weeksOfMonth")
    repeat: dict[str, bool] = Field(default_factory=dict, description="Days of week to repeat on.")
    # --- End Scheduling ---

    yesterday_completed: bool = Field(False, alias="yesterDaily", description="Was completed yesterday?")
    next_due: list[datetime] = Field(default_factory=list, description="Upcoming due dates (UTC).")
    history: list[dict[str, Any]] = Field(default_factory=list)

    _calculated_user_damage: float | None = None
    _calculated_party_damage: float | None = None

    # --- Validators ---
    @field_validator("streak", "every_x", mode="before")
    @classmethod
    def ensure_int(cls, value: Any) -> int:
        """Ensures streak/every_x are integers."""
        try:
            return int(value)
        except (ValueError, TypeError):
            return 1 if value is None else 0

    @field_validator("start_date", mode="before")
    @classmethod
    def parse_start_date_utc(cls, value: Any) -> datetime | None:
        """Parse start_date into UTC datetime."""
        handler = DateTimeHandler(timestamp=value)  # Usa timestamp=value para aclarar
        return handler.utc_datetime

    @field_validator("next_due", mode="before")
    @classmethod
    def parse_next_due_list(cls, value: Any) -> list[datetime]:
        """Parse list of next due timestamps into UTC datetimes."""
        if not isinstance(value, list):
            return []
        parsed_dates: list[datetime] = []
        for ts in value:
            dt = DateTimeHandler(ts).format_utc  # Reuse base validator
            if dt:
                parsed_dates.append(dt)
        return parsed_dates

    # --- Properties for derived status ---
    @property
    def display_status(self) -> Literal["Complete", "Due", "Not Due"]:
        """Returns a user-friendly status string."""
        if self.completed:
            return "Complete"
        if self.is_due:
            return "Due"
        return "Not Due"

    @computed_field
    @property
    def user_damage(self) -> float | None:
        """Returns the calculated user damage, included in model_dump."""
        # Access the instance attribute populated in process_task_statuses
        return self._calculated_user_damage

    @computed_field
    @property
    def party_damage(self) -> float | None:
        """Returns the calculated party damage, included in model_dump."""
        # Access the instance attribute populated in process_task_statuses
        return self._calculated_party_damage

    def calculate_damage(self, user: User) -> tuple[float | None, float | None]:
        """Calculate potential damage to user and party if this daily is incomplete."""
        dmg_user = None
        dmg_party = None
        stealth = user.stealth

        if not self.is_due or self.completed or user.is_sleeping or stealth > 0:
            return dmg_user, dmg_party

        try:
            # Calculate using Habitica's damage formula
            value = self.value
            checklist_mitigation = 1.0 - self.calculate_checklist_progress(self.checklist)
            priority_value = self.priority

            # Value clamping
            clamped_value = max(-47.27, min(value, 21.27))
            base_delta = abs(math.pow(0.9747, clamped_value))
            effective_delta = base_delta * checklist_mitigation

            # User CON mitigation
            con_stat = user.stats.base_constitution  # Get CON stat safely
            con_mitigation = max(0.1, 1.0 - (con_stat / 250.0))

            # Priority mapping
            prio_map = {0.1: 0.1, 1.0: 1.0, 1.5: 1.5, 2.0: 2.0}
            priority_multiplier = prio_map.get(priority_value, 1.0)

            # Calculate user damage
            hp_mod = effective_delta * con_mitigation * priority_multiplier * 2.0
            dmg_user = round(hp_mod, 1)

            # Calculate party damage only if on boss quest
            if user.is_on_boss_quest and user.quest_data.get("boss_str", 0) > 0:
                boss_str = user.quest_data.get("boss_str", 0)
                boss_delta = effective_delta * priority_multiplier if priority_multiplier < 1.0 else effective_delta
                dmg_party = round(boss_delta * boss_str, 1)

            return dmg_user, dmg_party

        except Exception as e:
            log.error(f"Error calculating damage for daily {self.id}: {e}")
            return None, None

    def __repr__(self) -> str:
        """Concise representation for Daily."""
        streak_str = f" (Streak: {self.streak})" if self.streak > 0 else ""
        checklist_str = f", checklist={len(self.checklist)} items" if self.checklist else ""
        text_preview = self.text[:30].replace("\n", " ") + ("..." if len(self.text) > 30 else "")
        return f"Daily(id='{self.id}', status='{self.display_status}', text='{text_preview}'{streak_str}{checklist_str})"


# KLASS: Reward
class Reward(Task):
    """Represents a Reward task that users can purchase with gold."""

    model_config = ConfigDict(extra="ignore", populate_by_name=True)

    task_type: Literal["reward"] = "reward"
    value: float = Field(default=0.0, description="Gold cost of the reward.")

    def __repr__(self) -> str:
        """Concise representation for Reward."""
        cost_str = f" (Cost: {self.value:.0f} GP)" if self.value > 0 else " (Free)"
        text_preview = self.text[:30].replace("\n", " ") + ("..." if len(self.text) > 30 else "")
        return f"Reward(id='{self.id}', text='{text_preview}'{cost_str})"


# SECTION: TASK LIST CONTAINER


# KLASS: TaskList
class TaskList:
    """Container for managing a list of Task objects, providing filtering methods."""

    # FUNC: __init__
    def __init__(self, tasks: list[Task]):
        """Initializes the TaskList.

        Args:
            tasks: A list containing processed Task objects (Habit, Daily, Todo, Reward).
        """
        if not isinstance(tasks, list):
            raise TypeError("TaskList requires a list of Task objects.")
        self.tasks: list[Task] = tasks
        self._assign_relative_positions()

    # FUNC: _assign_relative_positions
    def _assign_relative_positions(self) -> None:
        """Assigns a 'position' attribute relative to other tasks of the same type."""
        type_position_counters: dict[str, int] = defaultdict(int)
        default_type_key = "unknown"
        for task in self.tasks:
            type_key = task.task_type if task.task_type else default_type_key
            task.position = type_position_counters[type_key]
            type_position_counters[type_key] += 1

    # --- Standard Container Methods ---
    def __len__(self) -> int:
        return len(self.tasks)

    def __iter__(self):
        return iter(self.tasks)

    def __getitem__(self, index: int | slice) -> Task | list[Task]:  # Updated Union to |
        return self.tasks[index]

    def __repr__(self) -> str:
        """Detailed representation showing counts per type."""
        counts = defaultdict(int)
        for task in self.tasks:
            counts[task.task_type or "unknown"] += 1
        count_str = ", ".join(f"{t}:{c}" for t, c in counts.items())
        return f"TaskList(count={len(self.tasks)}, types=[{count_str}])"

    # --- Access and Filtering ---

    def get_by_id(self, task_id: str) -> Task | None:
        """Finds a task by its unique ID."""
        return next((task for task in self.tasks if task.id == task_id), None)

    # Filter by type
    def filter_by_type(self, task_type: str) -> TaskList:
        """Returns a new TaskList containing only tasks of the specified type."""
        filtered_tasks = [task for task in self.tasks if task.task_type == task_type]
        return TaskList(filtered_tasks)

    def filter_by_status(self, status: str) -> TaskList:
        """Returns a new TaskList containing only tasks with the specified status."""
        filtered_tasks = [task for task in self.tasks if task.calculated_status == status]
        return TaskList(filtered_tasks)

    def filter_by_tag_id(self, tag_id: str) -> TaskList:
        """Returns a new TaskList containing tasks associated with the given tag ID."""
        filtered_tasks = [task for task in self.tasks if tag_id in task.tags_id]
        return TaskList(filtered_tasks)

    def filter_by_tag_name(self, tag_name: str, case_sensitive: bool = False) -> TaskList:
        """Returns a new TaskList containing tasks associated with the given tag name."""
        # Match function based on case sensitivity
        if case_sensitive:
            match_func = lambda tn: tag_name in tn
        else:
            tag_name_lower = tag_name.lower()
            match_func = lambda tn: tag_name_lower in tn.lower()

        filtered_tasks = [task for task in self.tasks if any(match_func(tn) for tn in task.tag_names)]
        return TaskList(filtered_tasks)

    def filter_by_text(self, text_part: str, case_sensitive: bool = False) -> TaskList:
        """Returns a new TaskList containing tasks whose text includes the substring."""
        if not case_sensitive:
            text_part_lower = text_part.lower()
            filtered = [t for t in self.tasks if text_part_lower in t.text.lower()]
        else:
            filtered = [t for t in self.tasks if text_part in t.text]
        return TaskList(filtered)

    def process_task_statuses(self, user: User, tags_provider: TagList) -> None:
        """Process all tasks to update their status, tag names, and damage calculations."""
        for task in self.tasks:
            # Set tag names
            # The TYPE_CHECKING block now includes `from pixabit.models.tags import Tags`
            # to provide the type hint for `tags_provider`.
            if TYPE_CHECKING:
                assert isinstance(tags_provider, TagList)
            task.set_tag_names_from_tags(tags_provider)

            # Calculate value color based on task value
            task.styled_value = self._calculate_value_color(task.value)

            # Type-specific processing
            if isinstance(task, Daily):
                # Set calculated status
                if task.is_due:
                    task.calculated_status = "complete" if task.completed else "due"
                else:
                    task.calculated_status = "not_due"
                # --- Calculate and store damage ---
                # Call the existing calculate_damage method
                dmg_user, dmg_party = task.calculate_damage(user)
                # Store the results in the instance attributes
                task._calculated_user_damage = dmg_user
                task._calculated_party_damage = dmg_party
                # --- End Damage Calculation ---

            elif isinstance(task, Todo):
                # Set calculated status based on completion and due date
                task.calculated_status = task.display_status.lower().replace(" ", "_")

            elif isinstance(task, Habit):
                # Set calculated status based on habit direction
                directions = []
                if task.up:
                    directions.append("up")
                if task.down:
                    directions.append("down")
                task.calculated_status = "_".join(directions) if directions else "neutral"

            elif isinstance(task, Reward):
                # Rewards just have a simple status
                task.calculated_status = "available"

    @staticmethod
    def _calculate_value_color(value: float) -> str:
        """Calculate the color style based on task value."""
        if value < -20:
            return "danger"
        elif value < -10:
            return "warning"
        elif value < -1:
            return "weak"
        elif value > 10:
            return "excellent"
        elif value > 5:
            return "good"
        else:
            return "neutral"

    @classmethod
    def from_api_data(cls, data: list[dict[str, Any]]) -> TaskList:
        """Create a TaskList from API data, constructing the appropriate type for each task."""
        processed_tasks = []

        for task_data in data:
            task_type = task_data.get("type")

            try:
                if task_type == "habit":
                    task = Habit.model_validate(task_data)
                elif task_type == "daily":
                    task = Daily.model_validate(task_data)
                elif task_type == "todo":
                    task = Todo.model_validate(task_data)
                elif task_type == "reward":
                    task = Reward.model_validate(task_data)
                else:
                    task = Task.model_validate(task_data)
                processed_tasks.append(task)

            except ValidationError as e:
                # Attempt to get ID from data for logging
                task_id_for_log = task_data.get("id") or task_data.get("_id") or "unknown"
                log.error(f"Failed to validate task {task_id_for_log}: {e}")
                print(f"Failed data keys: {task_data.keys()}")
                print(f"Error details: {e.errors()}")

            except Exception as e:
                task_id_for_log = task_data.get("id") or task_data.get("_id") or "unknown"
                log.error(f"An unexpected error occurred processing task {task_id_for_log}: {e}")

        return cls(processed_tasks)

    # Add more filters as needed (priority, attribute, challenge, etc.)
    def to_json(self) -> list[dict]:
        """Convert TaskList to JSON-serializable dictionary list."""
        json_list = []
        for task in self.tasks:
            # Primero convierte el modelo Pydantic a un diccionario
            task_dict = task.model_dump(mode="json")
            json_list.append(task_dict)
        return json_list


async def main():
    cache_dir = Path(HABITICA_DATA_PATH)
    cache_dir.mkdir(exist_ok=True, parents=True)
    api = HabiticaClient()
    raw_data = await api.get_tasks()

    task_list = TaskList.from_api_data(raw_data)
    print("Datos de usuario cargados y validados correctamente.")

    # Calculate effective stats

    # Save raw data (optional)
    new_path = cache_dir / "task_proccessed.json"
    raw_path = cache_dir / "task_raw.json"
    save_json(raw_data, raw_path)
    log.info(f"Saved raw user data to {raw_path}")
    save_json(task_list.to_json(), new_path)


if __name__ == "__main__":
    import asyncio

    # Configure basic logging for the main script
    logging.basicConfig(
        level=logging.DEBUG,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    )

    asyncio.run(main())

-------- END OF FILE pixabit/models/task.py --------

-------- START OF FILE pixabit/models/test.py --------
import asyncio
from pathlib import Path

from pixabit.api.client import HabiticaClient
from pixabit.models.data_manager import DataManager


async def main():
    # Crear una instancia del cliente de API
    api_client = HabiticaClient()

    # Crear una instancia del DataManager con una ruta para caché
    cache_dir = Path("habitica_cache").expanduser()
    data_manager = DataManager(api_client, cache_dir=cache_dir)

    # Cargar todos los datos
    await data_manager.load_all_data()

    # Ahora puedes acceder a los datos procesados
    user = data_manager.user
    tasks = data_manager.tasks
    tags = data_manager.tags
    party = data_manager.party

    # También puedes acceder a los datos crudos
    raw_user_data = data_manager.raw_user_data
    raw_tasks_data = data_manager.raw_tasks_data

    print(f"Usuario: {user.profile.name}")
    print(f"Tareas pendientes: {len(tasks)}")

    # Los datos han sido guardados automáticamente en:
    # - ~/.cache/pixabit/raw/ (datos crudos)
    # - ~/.cache/pixabit/processed/ (datos procesados)


# Ejecutar la función asíncrona
asyncio.run(main())

-------- END OF FILE pixabit/models/test.py --------

-------- START OF FILE pixabit/models/user.py --------
# pixabit/models/user.py

# SECTION: MODULE DOCSTRING
"""Defines Pydantic models for the Habitica User object and its nested structures."""

# SECTION: IMPORTS
from __future__ import annotations  # Enable postponed evaluation of annotations

import json
import logging
import math
from datetime import datetime
from pathlib import Path
from typing import Any

import emoji_data_python
from pydantic import (
    BaseModel,
    ConfigDict,
    Field,
    ValidationError,
    field_validator,
    model_validator,
)

from pixabit.api.client import HabiticaClient
from pixabit.config import HABITICA_DATA_PATH
from pixabit.helpers._json import save_json
from pixabit.helpers._logger import log
from pixabit.models.game_content import Gear, StaticContentManager
from pixabit.models.message import Message, MessageList
from pixabit.models.party import QuestInfo, QuestProgress
from pixabit.models.tag import Tag, TagList

# --- User Subcomponent Models ---


class UserProfile(BaseModel):
    """Represents user profile information like display name and blurb."""

    model_config = ConfigDict(extra="ignore")
    name: str | None = Field(None, description="User's display name (emoji shortcodes replaced).")
    blurb: str | None = Field(
        None,
        description="User's profile description (emoji shortcodes replaced).",
    )

    @field_validator("name", "blurb", mode="before")
    @classmethod
    def parse_emoji_shortcodes(cls, value: str | None) -> str | None:
        if isinstance(value, str):
            return emoji_data_python.replace_colons(value)
        return None


class UserAuth(BaseModel):
    """Represents user authentication details, extracting username."""

    model_config = ConfigDict(extra="ignore")
    username: str | None = Field(None, description="Username from local auth.")
    created_at: datetime | None = Field(
        None,
        alias="created",
        description="Timestamp when user account was created.",
    )
    updated_at: datetime | None = Field(
        None,
        alias="updated",
        description="Timestamp when user account was last updated.",
    )
    logged_in_at: datetime | None = Field(
        None,
        alias="loggedin",
        description="Timestamp when user last logged in.",
    )

    @model_validator(mode="before")
    @classmethod
    def extract_nested_fields(cls, data: Any) -> dict[str, Any]:
        if not isinstance(data, dict):
            return {}
        values = data.copy()
        local_auth = data.get("local")
        values["username"] = local_auth.get("username") if isinstance(local_auth, dict) else None

        timestamps = data.get("timestamps", {})
        if isinstance(timestamps, dict):
            values["created"] = timestamps.get("created")
            values["updated"] = timestamps.get("updated")
            values["loggedin"] = timestamps.get("loggedin")
        else:
            values["created"] = values["updated"] = values["loggedin"] = None
        return values

    @field_validator("created_at", "updated_at", "logged_in_at", mode="before")
    @classmethod
    def parse_datetime_utc(cls, value: Any) -> datetime | None:
        if isinstance(value, str):
            try:
                return datetime.fromisoformat(value.replace("Z", "+00:00"))
            except (ValueError, TypeError):
                return None
        elif isinstance(value, datetime):
            return value
        return None


class UserPreferences(BaseModel):
    """User-specific preferences and settings."""

    model_config = ConfigDict(extra="ignore")
    sleep: bool = Field(False, description="Whether user is resting in the Inn.")
    day_start: int = Field(
        default=0,
        alias="dayStart",
        description="User's preferred day start hour (0–23).",
    )
    timezone_offset: int | None = Field(
        None,
        alias="timezoneOffset",
        description="User's current timezone offset from UTC in minutes.",
    )
    timezone_offset_at_last_cron: int | None = Field(
        None,
        alias="timezoneOffsetAtLastCron",
        description="User's timezone offset at the time of the last cron.",
    )

    @field_validator("day_start", mode="before")
    @classmethod
    def parse_day_start(cls, value: Any) -> int:
        try:
            ds = int(value)
            return max(0, min(23, ds))
        except (ValueError, TypeError):
            return 0


class Buffs(BaseModel):
    """Temporary stat increases/decreases from spells, food, etc."""

    model_config = ConfigDict(extra="allow")

    # Using explicit stat names that don't conflict with Python types
    constitution: int = Field(default=0, alias="con")
    intelligence: int = Field(default=0, alias="int")
    perception: int = Field(default=0, alias="per")
    strength: int = Field(default=0, alias="str")
    stealth: int = Field(default=0)

    @model_validator(mode="before")
    @classmethod
    def extract_nested_fields(cls, data: Any) -> dict[str, Any]:
        if not isinstance(data, dict):
            return {}
        values = data.copy()
        buffs = data.get("buffs")
        if isinstance(buffs, dict):
            values["con"] = int(buffs.get("con", 0))
            values["int"] = int(buffs.get("int", 0))
            values["per"] = int(buffs.get("per", 0))
            values["str"] = int(buffs.get("str", 0))
            values["stealth"] = int(buffs.get("stealth", 0))
        return values


class Training(BaseModel):
    """Permanent stat increases from leveling/resets."""

    model_config = ConfigDict(extra="allow")

    # Using explicit stat names that don't conflict with Python types
    constitution: float = Field(default=0.0, alias="con")
    intelligence: float = Field(default=0.0, alias="int")
    perception: float = Field(default=0.0, alias="per")
    strength: float = Field(default=0.0, alias="str")

    @model_validator(mode="before")
    @classmethod
    def extract_nested_fields(cls, data: Any) -> dict[str, Any]:
        if not isinstance(data, dict):
            return {}

        values = data.copy()
        training_data = data.get("training")
        if isinstance(training_data, dict):
            values["con"] = float(training_data.get("con", 0.0))
            values["int"] = float(training_data.get("int", 0.0))
            values["per"] = float(training_data.get("per", 0.0))
            values["str"] = float(training_data.get("str", 0.0))
        return values


class EquippedGear(BaseModel):
    """Represents the gear currently equipped by the user."""

    model_config = ConfigDict(extra="allow")
    weapon: str | None = None
    armor: str | None = None
    head: str | None = None
    shield: str | None = None

    back: str | None = None
    headAccessory: str | None = None

    eyewear: str | None = None
    body: str | None = None

    def get_equipped_items(self) -> list[str]:
        """Returns a list of equipped item keys."""
        return [
            item_key
            for item_key in [
                self.weapon,
                self.armor,
                self.head,
                self.shield,
                self.back,
                self.headAccessory,
                self.eyewear,
                self.body,
            ]
            if item_key is not None
        ]

    def calculate_total_bonus(self, user_class: str | None, gear_data: StaticContentManager) -> dict[str, float]:
        """Calculates the total stat bonus from equipped gear, including class bonus.

        Args:
            user_class: User's character class (warrior, mage, etc.)
            gear_data: Dictionary with gear stats information

        Returns:
            Dictionary with total bonuses for each stat
        """
        # Using short stat names for the result dict because these aren't variable names
        total_bonus = {"str": 0.0, "con": 0.0, "int": 0.0, "per": 0.0}

        for gear_key in self.get_equipped_items():
            item_data = gear_data.get_gear(key=gear_key)
            if item_data:
                print(item_data)
                # 1.5x multiplier if item class matches user class
                class_match_bonus = 1.5 if item_data.specialClass == user_class else 1.0

                # Map our stat names to the original API stat names
                stat_mapping = {
                    "str": "strength",
                    "con": "constitution",
                    "int": "intelligence",
                    "per": "perception",
                }

                # Add each stat bonus with appropriate class multiplier
                for short_name, full_name in stat_mapping.items():
                    # Use API name when getting values from gear data

                    stat_value = getattr(item_data, full_name, 0)
                    if isinstance(stat_value, (int, float)):
                        total_bonus[short_name] += stat_value * class_match_bonus

        return total_bonus


class UserStats(BaseModel):
    """Represents the user's core numerical stats and attributes."""

    model_config = ConfigDict(extra="ignore")

    hp: float = Field(default=50, description="Current health points.")
    mp: float = Field(default=0, description="Current mana points.")
    exp: float = Field(default=0, description="Current experience points.")
    gp: float = Field(default=0, description="Current gold points.")
    lvl: int = Field(default=1, description="User's current level.")
    klass: str | None = Field(default=None, alias="class", description="User's selected class.")

    # Base stats - using explicit names that don't conflict with Python types
    base_constitution: int = Field(default=0, alias="con", description="Base Constitution attribute.")
    base_intelligence: int = Field(default=0, alias="int", description="Base Intelligence attribute.")
    base_perception: int = Field(default=0, alias="per", description="Base Perception attribute.")
    base_strength: int = Field(default=0, alias="str", description="Base Strength attribute.")

    max_hp: int = Field(default=50, alias="maxHealth", description="Base maximum health.")
    max_mp: int = Field(default=10, alias="maxMP", description="Base maximum mana.")
    exp_to_next_level: int = Field(
        default=0,
        alias="toNextLevel",
        description="Experience points needed for the next level.",
    )

    buffs: Buffs = Field(default_factory=Buffs)
    training: Training = Field(default_factory=Training)

    @model_validator(mode="before")
    @classmethod
    def parse_stats_data(cls, data: Any) -> dict[str, Any]:
        if not isinstance(data, dict):
            return {}
        values = data.copy()

        # Process buffs and training
        values["buffs"] = Buffs.model_validate(data.get("buffs", data)).model_dump(by_alias=True)
        values["training"] = Training.model_validate(data.get("training", data)).model_dump(by_alias=True)

        # Process base stats
        values["con"] = int(data.get("con", 0))
        values["int"] = int(data.get("int", 0))
        values["per"] = int(data.get("per", 0))
        values["str"] = int(data.get("str", 0))

        # Process other stats
        values["hp"] = data.get("hp", 50)
        values["mp"] = data.get("mp", 0)
        values["exp"] = data.get("exp", 0)
        values["gp"] = data.get("gp", 0)
        values["lvl"] = int(data.get("lvl", 1))
        values["maxHealth"] = int(data.get("maxHealth", 50))
        values["maxMP"] = int(data.get("maxMP", 10))
        values["toNextLevel"] = int(data.get("toNextLevel", 0))
        return values

    def stats_before_gear(self) -> dict[str, float]:
        """Returns stats including base, buffs, training + level_bonus as float values."""
        level_bonus = min(50.0, math.floor(self.lvl / 2.0))

        # Using short stat names in the result dict because these aren't variable names
        return {
            "con": float(self.base_constitution) + float(self.buffs.constitution) + level_bonus,
            "int": float(self.base_intelligence) + float(self.buffs.intelligence) + level_bonus,
            "per": float(self.base_perception) + float(self.buffs.perception) + level_bonus,
            "str": float(self.base_strength) + float(self.buffs.strength) + level_bonus,
        }


# --- Other User Subcomponents ---
class UserItems(BaseModel):
    """Holds user's inventory: gear, items, pets, mounts etc."""

    model_config = ConfigDict(extra="ignore")
    gear_equipped: EquippedGear = Field(default_factory=EquippedGear, alias="equipped")
    gear_costume: EquippedGear = Field(default_factory=EquippedGear, alias="costume")

    @model_validator(mode="before")
    @classmethod
    def structure_gear_data(cls, data: Any) -> dict[str, Any]:
        if not isinstance(data, dict):
            return {}
        values = data.copy()
        gear_data = data.get("gear", {})
        if isinstance(gear_data, dict):
            values["equipped"] = gear_data.get("equipped", {})
            values["costume"] = gear_data.get("costume", {})
            values["owned"] = gear_data.get("owned", {})
        else:
            values["equipped"], values["costume"], values["owned"] = {}, {}, {}

        return values


class UserAchievements(BaseModel):
    """Holds user's achievements progress."""

    model_config = ConfigDict(extra="ignore")
    challenges: list[str] = Field(default_factory=list, description="List of challenge IDs completed.")
    quests: dict[str, int] = Field(default_factory=dict, description="{quest_key: completion_count}.")
    perfect_days: int = Field(default=0, alias="perfect", description="Count of perfect days.")
    streak: int = Field(default=0, description="Max consecutive perfect days streak.")
    login_incentives: int = Field(
        default=0,
        alias="loginIncentives",
        description="Count of login incentives claimed.",
    )
    other_achievements: dict[str, Any] = Field(default_factory=dict, description="Other earned achievements.")

    @model_validator(mode="before")
    @classmethod
    def structure_achievements(cls, data: Any) -> dict[str, Any]:
        if not isinstance(data, dict):
            return {}
        values = {}
        known_keys = {
            "challenges",
            "quests",
            "perfect",
            "streak",
            "loginIncentives",
            "ultimateGearSets",
        }

        values["challenges"] = data.get("challenges", [])
        values["quests"] = data.get("quests", {})
        values["perfect"] = int(data.get("perfect", 0))
        values["streak"] = int(data.get("streak", 0))
        values["loginIncentives"] = int(data.get("loginIncentives", 0))
        values["other_achievements"] = {k: v for k, v in data.items() if k not in known_keys}
        return values


class UserPartyInfo(BaseModel):
    """Holds information about the user's party membership and active quest."""

    model_config = ConfigDict(extra="ignore")  # Keep ignore to ignore other potential party fields

    party_id: str | None = Field(
        None,
        alias="_id",  # Alias for the party ID field
        description="The unique ID of the party the user is in.",
    )

    # Add the quest field. It will contain QuestInfo, which in turn contains QuestProgress.
    # It should be optional (None) because a user might not be in a party or might not have a quest.
    quest: QuestInfo | None = Field(
        None,
        description="Information about the user's active party quest.",
    )


class UserInboxInfo(BaseModel):
    """Holds information about the user's inbox status."""

    model_config = ConfigDict(extra="ignore")
    new_messages: int = Field(0, alias="newMessages", description="Count of unread private messages.")
    has_gift: bool = Field(False, alias="hasNew", description="Flag indicating a new gift message.")
    opt_out: bool = Field(
        False,
        alias="optOut",
        description="Whether the user has opted out of receiving new PMs.",
    )
    blocks: list[str] = Field(
        default_factory=list,
        description="List of user IDs blocked by this user.",
    )
    messages: MessageList | None = None

    @field_validator("messages", mode="before")
    @classmethod
    def transform_messages_dict_to_list(cls, value: Any) -> Any:
        """Transforma un diccionario de mensajes en una lista para MessageList."""
        if isinstance(value, dict) and not isinstance(value, list):
            # Si es un diccionario pero no una lista, convertir valores a lista
            return list(value.values())
        return value


# --- Main User Model ---
class User(BaseModel):
    """Represents the complete Habitica User object, aggregating data from the '/user' endpoint."""

    model_config = ConfigDict(extra="ignore", arbitrary_types_allowed=True)
    id: str = Field(..., alias="_id")
    balance: float = Field(default=0.0)
    needs_cron: bool = Field(default=False, alias="needsCron")
    last_cron: datetime | None = Field(default=None, alias="lastCron")
    login_incentives: int = Field(default=0, alias="loginIncentives")

    profile: UserProfile = Field(default_factory=UserProfile)
    auth: UserAuth = Field(default_factory=UserAuth)
    preferences: UserPreferences = Field(default_factory=UserPreferences)
    items: UserItems = Field(default_factory=UserItems)
    achievements: UserAchievements = Field(default_factory=UserAchievements)
    party: UserPartyInfo = Field(default_factory=UserPartyInfo)
    inbox: UserInboxInfo = Field(default_factory=UserInboxInfo)
    stats: UserStats = Field(default_factory=UserStats)
    # Added field to store calculated stats
    calculated_stats: dict[str, Any] = Field(default_factory=dict)
    tags: TagList | None = Field(default_factory=TagList)

    @field_validator("last_cron", mode="before")
    @classmethod
    def parse_last_cron(cls, value: Any) -> datetime | None:
        if isinstance(value, str):
            try:
                return datetime.fromisoformat(value.replace("Z", "+00:00"))
            except (ValueError, TypeError):
                return None
        elif isinstance(value, datetime):
            return value
        return None

    @field_validator("tags", mode="before")
    @classmethod
    def tags_to_taglist(cls, value: Any) -> Any:
        """Transforma un diccionario de mensajes en una lista para MessageList."""
        if isinstance(value, list):
            return TagList.from_raw_data(value)

    @model_validator(mode="before")
    @classmethod
    def check_and_assign_id(cls, data: Any) -> Any:
        if isinstance(data, dict):
            if "_id" not in data and "id" in data:
                data["_id"] = data["id"]
            elif "_id" not in data and "id" not in data:
                raise ValueError("User data must contain 'id' or '_id'")
        return data

    # --- Convenience Properties ---
    @property
    def username(self) -> str | None:
        return self.auth.username

    @property
    def display_name(self) -> str | None:
        return self.profile.name

    @property
    def level(self) -> int:
        return self.stats.lvl

    @property
    def klass(self) -> str | None:
        return self.stats.klass

    @property
    def hp(self) -> float:
        return self.stats.hp

    @property
    def mp(self) -> float:
        return self.stats.mp

    @property
    def gp(self) -> float:
        return self.stats.gp

    @property
    def exp(self) -> float:
        return self.stats.exp

    @property
    def party_id(self) -> str | None:
        return self.party.party_id

    @property
    def is_sleeping(self) -> bool:
        return self.preferences.sleep

    @property
    def gems(self) -> int:
        return int(self.balance * 4) if self.balance > 0 else 0

    @property
    def exp_to_next_level(self) -> int:
        return self.stats.exp_to_next_level

    @property
    def stealth(self) -> int:
        """Returns the user's stealth buff value for easier access."""
        # Access the nested value through the defined models
        return self.stats.buffs.stealth

    @property
    def is_on_boss_quest(self) -> int:
        """Returns the user's stealth buff value for easier access."""
        # Access the nested value through the defined models
        return self.quest_info.is_active_and_ongoing

    @property
    def effective_stats(self) -> dict[str, float]:
        """Returns the pre-calculated effective stats."""
        return self.calculated_stats.get("effective_stats", {})

    def get_chat_messages(
        self,
    ) -> list[Message]:  # Return list of Message objects
        """Devuelve los mensajes del chat si existen."""
        # Access the validated messages list directly from the MessageList BaseModel
        if self.inbox.messages is None:
            return []
        return self.inbox.messages  # chat is now a MessageList BaseModel instance

    # --- Methods for Calculated Values ---
    def calculate_effective_stats(self, gear_data: StaticContentManager) -> dict[str, float]:
        """Calculates total effective stats including base, buffs, training, level bonus, and gear."""
        # Get stats before gear
        stats_before_gear = self.stats.stats_before_gear()
        # Calculate gear bonuses
        gear_bonus = self.items.gear_equipped.calculate_total_bonus(self.klass, gear_data)
        # Combine stats with gear bonuses
        # Keep using short stat names (str, int, etc.) in the result dictionary
        effective_stats = {stat: stats_before_gear.get(stat, 0.0) + gear_bonus.get(stat, 0.0) for stat in ["str", "con", "int", "per"]}

        # Store the calculated stats
        self.calculated_stats["effective_stats"] = effective_stats
        self.calculated_stats["gems"] = self.gems

        return effective_stats

    def to_dict(self, include_calculated_stats: bool = True) -> dict[str, Any]:
        """Converts the user to a dictionary, optionally including calculated stats.

        Args:
            include_calculated_stats: If True, includes calculated stats

        Returns:
            Dictionary with user data
        """
        data = self.model_dump(by_alias=True, mode="json")  # mode:json

        # Add calculated stats to output if requested
        if include_calculated_stats:
            data["calculated"] = self.calculated_stats

        return data

    def to_json(self, include_calculated_stats: bool = True, **kwargs) -> str:
        """Converts the user to a JSON string, optionally including calculated stats.

        Args:
            include_calculated_stats: If True, includes calculated stats
            **kwargs: Additional arguments for json.dumps

        Returns:
            JSON string with user data
        """
        data = self.to_dict(include_calculated_stats=include_calculated_stats)
        return json.dumps(data, **kwargs)

    @classmethod
    def create_from_raw_data(cls, raw_data: dict) -> User:
        """Create a User object from raw data and precompute calculated stats."""
        if not isinstance(raw_data, dict):
            raise ValueError(f"Expected dict, got {type(raw_data)}")

        try:
            user_instance = cls.model_validate(raw_data)
            return user_instance
        except ValidationError as e:
            log.error(f"Validation failed while creating User from raw data: {e}")
            raise
        except Exception as e:
            log.error(
                f"Unexpected error creating User from raw data: {e}",
                exc_info=True,
            )
            raise


async def main():
    try:
        cache_dir = Path(HABITICA_DATA_PATH)
        cache_dir.mkdir(exist_ok=True, parents=True)
        api = HabiticaClient()
        raw_data = await api.get_user_data()

        user = User.create_from_raw_data(raw_data)
        print("Datos de usuario cargados y validados correctamente.")

        # Calculate effective stats
        await user.calculate_effective_stats()

        # Save raw data (optional)
        raw_path = cache_dir / "user_raw.json"
        save_json(raw_data, raw_path)
        log.info(f"Saved raw user data to {raw_path}")

        print("\n--- Datos básicos ---")
        print(f"ID: {user.id}")
        print(f"Usuario: {user.username}")
        print(f"Nombre: {user.display_name}")
        print(f"Nivel: {user.level}")
        print(f"Clase: {user.klass}")

        print("\n--- Estadísticas actuales ---")
        print(f"HP: {user.hp}/{user.stats.max_hp}")
        print(f"MP: {user.mp}/{user.stats.max_mp}")
        print(f"Oro: {user.gp}")
        print(f"Exp: {user.exp}/{user.exp_to_next_level}")

        print("\n--- Estadísticas efectivas ---")
        for stat, value in user.effective_stats.items():
            print(f"{stat.upper()}: {value:.1f}")

        # Exportar a JSON
        export_path = cache_dir / "user_processed.json"
        user_json_data = await user.to_json(indent=2)

        with open(export_path, "w", encoding="utf-8") as f:
            f.write(user_json_data)

        print(f"\nDatos exportados a {export_path}")

    except Exception as e:
        print("\n--- Error ---")
        print(f"Error: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    import asyncio

    asyncio.run(main())

-------- END OF FILE pixabit/models/user.py --------

-------- START OF FILE pixabit/models/user_gemini.py --------
# pixabit/models/user.py

# SECTION: MODULE DOCSTRING
"""Defines Pydantic models for the Habitica User object and its nested structures."""

# SECTION: IMPORTS
from __future__ import annotations  # Enable postponed evaluation of annotations

import json  # Keep for example usage
import logging
import math
from datetime import datetime, timezone  # Ensure timezone is imported
from typing import (  # Use standard List/Dict
    Any,
    Dict,
    List,
    Literal,
    Optional,
    cast,
)

# External Libs
import emoji_data_python
from pydantic import (  # Use | for Union
    BaseModel,
    ConfigDict,
    Field,
    FieldValidationInfo,
    ValidationError,
    field_validator,
    model_validator,
)

# Use standard Python logger
logger = logging.getLogger(__name__)

# Mock Data (Keep for example usage or tests)
# This should ideally come from GameContentCache in a real scenario
ALL_GEAR_CONTENT: dict[str, dict[str, Any]] = {
    "weapon_warrior_1": {
        "text": "Warrior Sword L1",
        "str": 3,
        "klass": "warrior",
        "type": "weapon",
    },
    "armor_warrior_1": {
        "text": "Warrior Armor L1",
        "con": 2,
        "klass": "warrior",
        "type": "armor",
    },
    "head_warrior_1": {
        "text": "Warrior Helm L1",
        "int": 1,
        "klass": "warrior",
        "type": "head",
    },
    "shield_warrior_1": {
        "text": "Warrior Shield L1",
        "per": 2,
        "klass": "warrior",
        "type": "shield",
    },
    "weapon_base_0": {
        "text": "Training Sword",
        "str": 0,
        "klass": "base",
        "type": "weapon",
    },
    # Add more gear as needed for testing calculations
}

# SECTION: USER SUBCOMPONENT MODELS


# KLASS: UserProfile
class UserProfile(BaseModel):
    """Represents user profile information."""

    model_config = ConfigDict(extra="ignore")  # Ignore extra profile fields

    name: str = Field(
        "", description="User's display name (parsed)."
    )  # Default to empty string
    blurb: str = Field(
        "", description="User's profile description (parsed)."
    )  # Default to empty string

    @field_validator("name", "blurb", mode="before")
    @classmethod
    def parse_emoji_shortcodes(cls, value: Any) -> str:
        """Parse text and replace emoji shortcodes."""
        if isinstance(value, str):
            return emoji_data_python.replace_colons(value)
        return ""  # Return empty string if not a string


# KLASS: UserAuthTimestamps (Nested model for timestamps)
class UserAuthTimestamps(BaseModel):
    model_config = ConfigDict(extra="ignore")
    created: datetime | None = None
    updated: datetime | None = None
    loggedin: datetime | None = None

    # Validator for all timestamp fields
    @field_validator("created", "updated", "loggedin", mode="before")
    @classmethod
    def parse_datetime_utc(cls, value: Any) -> datetime | None:
        """Parses timestamp strings/datetimes into UTC datetime objects."""
        # Reuse date helper if available
        # return convert_timestamp_to_utc(value)
        # Inline logic:
        dt: datetime | None = None
        if isinstance(value, str):
            try:
                dt = datetime.fromisoformat(value.replace("Z", "+00:00"))
            except ValueError:
                pass
        elif isinstance(value, datetime):
            dt = value

        if dt:
            if dt.tzinfo is None:
                return dt.replace(tzinfo=timezone.utc)
            return dt.astimezone(timezone.utc)
        else:
            if value is not None:
                logger.warning(f"Could not parse timestamp: {value!r}")
            return None


# KLASS: UserAuthLocal (Nested model for local auth)
class UserAuthLocal(BaseModel):
    model_config = ConfigDict(extra="ignore")
    username: str | None = None
    email: str | None = None  # Include email if needed


# KLASS: UserAuth
class UserAuth(BaseModel):
    """Represents user authentication details."""

    model_config = ConfigDict(extra="ignore", populate_by_name=True)

    # Nested models for cleaner structure
    local: UserAuthLocal | None = Field(default_factory=UserAuthLocal)
    timestamps: UserAuthTimestamps | None = Field(
        default_factory=UserAuthTimestamps
    )

    # Convenience properties to access nested fields
    @property
    def username(self) -> str | None:
        return self.local.username if self.local else None

    @property
    def created_at(self) -> datetime | None:
        return self.timestamps.created if self.timestamps else None

    @property
    def updated_at(self) -> datetime | None:
        return self.timestamps.updated if self.timestamps else None

    @property
    def logged_in_at(self) -> datetime | None:
        return self.timestamps.loggedin if self.timestamps else None


# KLASS: UserPreferences
class UserPreferences(BaseModel):
    """User-specific preferences and settings."""

    model_config = ConfigDict(extra="ignore", populate_by_name=True)

    sleep: bool = Field(
        False, description="Whether user is resting in the Inn."
    )
    day_start: int = Field(
        0, alias="dayStart", description="User's day start hour (0-23)."
    )
    timezone_offset: int | None = Field(
        None,
        alias="timezoneOffset",
        description="Current offset from UTC (minutes).",
    )
    timezone_offset_at_last_cron: int | None = Field(
        None,
        alias="timezoneOffsetAtLastCron",
        description="Offset at last cron.",
    )
    # Add other common preferences if needed: disableClasses, background, etc.

    @field_validator("day_start", mode="before")
    @classmethod
    def parse_day_start(cls, value: Any) -> int:
        """Validate and parse day start hour."""
        try:
            ds = int(value)
            # Clamp value between 0 and 23
            return max(0, min(23, ds))
        except (ValueError, TypeError):
            logger.warning(
                f"Invalid dayStart value '{value}'. Using default 0."
            )
            return 0


# KLASS: Buffs
class Buffs(BaseModel):
    """Temporary stat increases/decreases."""

    # Allow extra fields for buffs not explicitly modeled (e.g., seafoam)
    model_config = ConfigDict(extra="allow", populate_by_name=True)

    con: float = 0.0  # Buffs can be floats from gear sets etc.
    int_: float = Field(0.0, alias="int")
    per: float = 0.0
    str_: float = Field(0.0, alias="str")
    stealth: int = 0  # Stealth is usually integer addition

    @field_validator("con", "int_", "per", "str_", mode="before")
    @classmethod
    def ensure_float(cls, value: Any) -> float:
        try:
            return float(value)
        except (ValueError, TypeError):
            return 0.0

    @field_validator("stealth", mode="before")
    @classmethod
    def ensure_int(cls, value: Any) -> int:
        try:
            return int(value)
        except (ValueError, TypeError):
            return 0


# KLASS: Training
class Training(BaseModel):
    """Permanent stat increases from leveling/resets."""

    model_config = ConfigDict(extra="ignore", populate_by_name=True)

    con: float = 0.0  # Training points can be fractional
    int_: float = Field(0.0, alias="int")
    per: float = 0.0
    str_: float = Field(0.0, alias="str")

    @field_validator("con", "int_", "per", "str_", mode="before")
    @classmethod
    def ensure_float(cls, value: Any) -> float:
        try:
            return float(value)
        except (ValueError, TypeError):
            return 0.0


# KLASS: EquippedGear
class EquippedGear(BaseModel):
    """Represents the gear currently equipped by the user."""

    model_config = ConfigDict(
        extra="allow"
    )  # Allow other slots if API adds them

    # Explicitly define known gear slots
    weapon: str | None = Field(None, description="Key of equipped weapon.")
    armor: str | None = Field(None, description="Key of equipped body armor.")
    head: str | None = Field(None, description="Key of equipped head gear.")
    shield: str | None = Field(
        None, description="Key of equipped shield/off-hand."
    )
    head_accessory: str | None = Field(
        None,
        alias="headAccessory",
        description="Key of equipped head accessory.",
    )
    eyewear: str | None = Field(None, description="Key of equipped eyewear.")
    body: str | None = Field(
        None, description="Key of equipped body wear (over armor)."
    )
    back: str | None = Field(None, description="Key of equipped back item.")

    # --- Methods ---
    def get_equipped_item_keys(self) -> list[str]:
        """Returns a list of non-None gear keys currently equipped."""
        return [
            key
            for key in self.model_dump(exclude_none=True).values()
            if isinstance(key, str)
        ]

    def calculate_total_bonus(
        self,
        user_class: str | None,
        gear_content_lookup: dict[str, dict[str, Any]],
    ) -> dict[str, float]:
        """Calculates the total stat bonus from equipped gear.

        Args:
            user_class: The user's class ('warrior', 'rogue', 'healer', 'mage').
            gear_content_lookup: A dictionary mapping gear keys to their stats
                                 (ideally from GameContentCache.get_gear()).

        Returns:
            A dictionary with total float bonuses for 'str', 'con', 'int', 'per'.
        """
        total_bonus: dict[str, float] = {
            "str": 0.0,
            "con": 0.0,
            "int": 0.0,
            "per": 0.0,
        }
        stats_to_check = list(total_bonus.keys())

        for gear_key in self.get_equipped_item_keys():
            item_data = gear_content_lookup.get(gear_key)
            if isinstance(item_data, dict):
                # Check item class vs user class for bonus multiplier
                item_class = item_data.get("klass")
                # Class bonus applies if item class matches user class, OR if item is 'base' class
                class_match_bonus_multiplier = (
                    1.5
                    if (
                        item_class
                        and (item_class == user_class or item_class == "base")
                    )
                    else 1.0
                )

                for stat_key in stats_to_check:
                    # Look up stat value ('str', 'int', 'con', 'per') in item data
                    # Gear models now nest stats, adjust lookup
                    item_stats_obj = item_data.get(
                        "stats", {}
                    )  # Assuming Gear model populates this
                    stat_value = float(
                        item_stats_obj.get(stat_key, 0.0)
                    )  # Use dict.get for safety

                    total_bonus[stat_key] += (
                        stat_value * class_match_bonus_multiplier
                    )

        return total_bonus


# KLASS: UserItems
class UserItems(BaseModel):
    """Holds user's inventory: gear, items, pets, mounts etc."""

    model_config = ConfigDict(extra="ignore", populate_by_name=True)

    # Nested models for equipped/costume gear
    gear_equipped: EquippedGear = Field(
        default_factory=EquippedGear, alias="equipped"
    )
    gear_costume: EquippedGear = Field(
        default_factory=EquippedGear, alias="costume"
    )
    # Owned gear: {gear_key: bool/timestamp} - bool is simpler
    gear_owned: dict[str, bool] = Field(default_factory=dict, alias="owned")

    # Consumables / Collectibles
    eggs: dict[str, int] = Field(default_factory=dict)
    food: dict[str, int] = Field(default_factory=dict)
    hatching_potions: dict[str, int] = Field(
        default_factory=dict, alias="hatchingPotions"
    )

    # Companions
    # Pets: {Pet-SpeciesKey: feed_count} (e.g., {"BearCub-Base": 5})
    pets: dict[str, int] = Field(default_factory=dict)
    # Mounts: {Mount-SpeciesKey: True} (e.g., {"BearCub-Base": True})
    mounts: dict[str, bool] = Field(default_factory=dict)

    # Special Items & Quest Scrolls
    # Special items: Orb, Rebirth items, Cards, Gear Sets etc.
    special: dict[str, Any] = Field(default_factory=dict)
    # Quests: {quest_key: count}
    quests: dict[str, int] = Field(default_factory=dict)

    # Validator to structure gear data correctly before nested models are parsed
    @model_validator(mode="before")
    @classmethod
    def structure_gear_data(cls, data: Any) -> dict[str, Any]:
        """Ensures gear data (equipped, costume, owned) is correctly nested."""
        if not isinstance(data, dict):
            return data  # Or {}
        values = data.copy()  # Work on a copy
        gear_data = data.get("gear", {})  # Get the top-level 'gear' dict
        if isinstance(gear_data, dict):
            # Assign sub-dicts to the keys our model expects (using aliases)
            values["equipped"] = gear_data.get("equipped", {})
            values["costume"] = gear_data.get("costume", {})
            values["owned"] = gear_data.get("owned", {})
        else:
            # Ensure defaults if 'gear' key is missing or invalid
            values["equipped"], values["costume"], values["owned"] = {}, {}, {}
        # No need to assign other fields like eggs, food here - Pydantic handles them
        return values


# KLASS: UserAchievements
class UserAchievements(BaseModel):
    """Holds user's achievements progress."""

    model_config = ConfigDict(extra="ignore", populate_by_name=True)

    # List of challenge IDs completed
    challenges: list[str] = Field(default_factory=list)
    # Quests completed: {quest_key: completion_count}
    quests: dict[str, int] = Field(default_factory=dict)
    # Perfect day count
    perfect_days: int = Field(0, alias="perfect")
    # Max consecutive perfect days streak
    streak: int = Field(0)
    # Count of login incentives claimed
    # Note: This might also be at the top level of User data
    login_incentives_achieved: int = Field(0, alias="loginIncentives")
    # Specific named achievements (e.g., ultimate gear sets) might be modeled explicitly if needed
    ultimate_gear_sets: dict[str, bool] = Field(
        default_factory=dict, alias="ultimateGearSets"
    )
    # Store other achievements found directly under 'achievements'
    other_achievements: dict[str, Any] = Field(default_factory=dict)

    @model_validator(mode="before")
    @classmethod
    def structure_achievements(cls, data: Any) -> dict[str, Any]:
        """Separates known achievement fields from others."""
        if not isinstance(data, dict):
            return data  # Or {}
        values = {}
        known_keys = {
            "challenges",
            "quests",
            "perfect",
            "streak",
            "loginIncentives",
            "ultimateGearSets",
        }
        # Assign known fields using aliases
        values["challenges"] = data.get("challenges", [])
        values["quests"] = data.get("quests", {})
        values["perfect"] = data.get("perfect", 0)
        values["streak"] = data.get("streak", 0)
        values["loginIncentives"] = data.get("loginIncentives", 0)
        values["ultimateGearSets"] = data.get("ultimateGearSets", {})
        # Collect remaining keys into other_achievements
        values["other_achievements"] = {
            k: v for k, v in data.items() if k not in known_keys
        }
        return values

    # Ensure counts are integers
    @field_validator(
        "perfect_days", "streak", "login_incentives_achieved", mode="before"
    )
    @classmethod
    def ensure_int(cls, value: Any) -> int:
        try:
            return int(value)
        except (ValueError, TypeError):
            return 0


# KLASS: UserPartyInfo
class UserPartyInfo(BaseModel):
    """Holds information about the user's party membership."""

    model_config = ConfigDict(extra="ignore", populate_by_name=True)
    # Party ID might be missing if user is not in a party
    party_id: str | None = Field(
        None, alias="_id", description="The UUID of the user's current party."
    )
    # Potential other fields: quest info (handled in Party model usually), etc.


# KLASS: UserInboxInfo
class UserInboxInfo(BaseModel):
    """Holds information about the user's inbox status."""

    model_config = ConfigDict(extra="ignore", populate_by_name=True)

    # Count of unread PMs
    new_messages: int = Field(0, alias="newMessages")
    # Flag for new gift messages (usually triggers notification)
    has_gift: bool = Field(False, alias="hasNew")
    # Has user opted out of receiving new PMs?
    opt_out: bool = Field(False, alias="optOut")
    # List of user UUIDs blocked by this user
    blocks: list[str] = Field(default_factory=list)

    @field_validator("new_messages", mode="before")
    @classmethod
    def ensure_int(cls, value: Any) -> int:
        try:
            return int(value)
        except (ValueError, TypeError):
            return 0


# KLASS: UserStatsCalculations (Helper for stat calculations)
class UserStatsCalculations:
    """Provides methods to calculate derived stats based on base values, buffs, gear."""

    @staticmethod
    def _get_level_bonus(level: int) -> float:
        """Calculate stat bonus based on user level."""
        return min(50.0, math.floor(level / 2.0))

    @classmethod
    def calculate_stats_before_gear(cls, stats: UserStats) -> dict[str, float]:
        """Calculates stats including base, buffs, training + level bonus (result is float)."""
        level_bonus = cls._get_level_bonus(stats.lvl)
        # Access base stats, buffs, training via the UserStats nested models
        return {
            "con": float(stats.base_con + stats.buffs.con)
            + stats.training.con
            + level_bonus,
            "int": float(stats.base_int_ + stats.buffs.int_)
            + stats.training.int_
            + level_bonus,
            "per": float(stats.base_per + stats.buffs.per)
            + stats.training.per
            + level_bonus,
            "str": float(stats.base_str_ + stats.buffs.str_)
            + stats.training.str_
            + level_bonus,
        }

    @classmethod
    def calculate_effective_stats(
        cls,
        stats: UserStats,
        items: UserItems,
        gear_content_lookup: dict[str, dict[str, Any]],
    ) -> dict[str, float]:
        """Calculates total effective stats including gear bonuses."""
        stats_before_gear = cls.calculate_stats_before_gear(stats)
        # Pass user class from stats model
        gear_bonus = items.gear_equipped.calculate_total_bonus(
            stats.klass, gear_content_lookup
        )

        effective_stats = {
            stat: stats_before_gear.get(stat, 0.0) + gear_bonus.get(stat, 0.0)
            for stat in ["str", "con", "int", "per"]
        }
        return effective_stats

    @classmethod
    def calculate_max_hp(cls, stats: UserStats, effective_con: float) -> float:
        """Calculates max HP based on base max HP and effective CON."""
        # effective_con should include base, buffs, training, level, and gear
        return float(stats.max_hp_base + math.floor(effective_con * 2.0))

    @classmethod
    def calculate_max_mp(cls, stats: UserStats, effective_int: float) -> float:
        """Calculates max MP based on base max MP and effective INT."""
        # effective_int should include base, buffs, training, level, and gear
        return float(stats.max_mp_base + math.floor(effective_int * 2.0))


# KLASS: UserStats (Main Stats Container)
class UserStats(BaseModel):
    """Represents the user's core numerical stats and attributes."""

    model_config = ConfigDict(extra="ignore", populate_by_name=True)

    # --- Core Resources ---
    hp: float = Field(
        default=50.0, description="Current health points."
    )  # Allow float
    mp: float = Field(
        default=0.0, description="Current mana points."
    )  # Allow float
    exp: float = Field(
        default=0.0, description="Current experience points."
    )  # Allow float
    gp: float = Field(
        default=0.0, description="Current gold points."
    )  # Allow float
    lvl: int = Field(default=1, description="User's current level.")
    # Use alias 'class' to map to 'klass' field name
    klass: Literal["warrior", "rogue", "healer", "wizard"] | None = Field(
        None, alias="class", description="User's selected class."
    )

    # --- Base Attributes & Max Values ---
    # Renamed fields clashing with built-ins, using aliases
    base_con: int = Field(
        0, alias="con", description="Base Constitution allocated points."
    )
    base_int_: int = Field(
        0, alias="int", description="Base Intelligence allocated points."
    )
    base_per: int = Field(
        0, alias="per", description="Base Perception allocated points."
    )
    base_str_: int = Field(
        0, alias="str", description="Base Strength allocated points."
    )
    # Base max values (before CON/INT bonuses)
    max_hp_base: int = Field(
        50, alias="maxHealth", description="Base maximum health."
    )
    max_mp_base: int = Field(
        10, alias="maxMP", description="Base maximum mana."
    )
    # EXP needed for next level
    exp_to_next_level: int = Field(
        0, alias="toNextLevel", description="EXP needed for next level."
    )

    # --- Modifiers ---
    buffs: Buffs = Field(default_factory=Buffs)
    training: Training = Field(default_factory=Training)

    # --- Validators for core resources/level ---
    @field_validator("hp", "mp", "exp", "gp", mode="before")
    @classmethod
    def ensure_float(cls, value: Any) -> float:
        try:
            return float(value)
        except (ValueError, TypeError):
            return 0.0

    @field_validator(
        "lvl", "max_hp_base", "max_mp_base", "exp_to_next_level", mode="before"
    )
    @classmethod
    def ensure_int(cls, value: Any) -> int:
        try:
            return int(value)
        except (ValueError, TypeError):
            return 0

    # --- Convenience properties using UserStatsCalculations ---
    # These now require passing self (the UserStats instance) to the calculation methods

    @property
    def stats_before_gear(self) -> dict[str, float]:
        """Calculated stats before gear bonuses are applied."""
        return UserStatsCalculations.calculate_stats_before_gear(self)

    # Note: Calculating effective stats and max HP/MP *requires* gear info (from UserItems)
    # These cannot be properties of UserStats alone. They belong on the main User model.


# SECTION: MAIN USER MODEL


# KLASS: User
class User(BaseModel):
    """Represents the complete Habitica User object."""

    # Use aliases, ignore extra fields from API
    model_config = ConfigDict(extra="ignore", populate_by_name=True)

    # --- Top-Level Fields ---
    id: str = Field(..., alias="_id", description="Unique User UUID.")
    # Balance is gems/4
    balance: float = Field(default=0.0, description="User's gem balance / 4.")
    needs_cron: bool = Field(
        False,
        alias="needsCron",
        description="Flag indicating cron needs to run.",
    )
    last_cron: datetime | None = Field(
        None,
        alias="lastCron",
        description="Timestamp of the last cron run (UTC).",
    )
    # Login incentives count at top level too
    login_incentives: int = Field(
        0, alias="loginIncentives", description="Current login incentive count."
    )

    # --- Nested Subcomponent Models ---
    profile: UserProfile = Field(default_factory=UserProfile)
    auth: UserAuth = Field(default_factory=UserAuth)
    preferences: UserPreferences = Field(default_factory=UserPreferences)
    items: UserItems = Field(default_factory=UserItems)
    achievements: UserAchievements = Field(default_factory=UserAchievements)
    party: UserPartyInfo = Field(default_factory=UserPartyInfo)
    inbox: UserInboxInfo = Field(default_factory=UserInboxInfo)
    stats: UserStats = Field(default_factory=UserStats)

    # --- Validators for Top-Level Fields ---
    @field_validator("last_cron", mode="before")
    @classmethod
    def parse_last_cron_utc(cls, value: Any) -> datetime | None:
        """Parse lastCron timestamp into UTC datetime."""
        # Reuse date helper or inline logic
        dt: datetime | None = None
        if isinstance(value, str):
            try:
                dt = datetime.fromisoformat(value.replace("Z", "+00:00"))
            except ValueError:
                pass
        elif isinstance(value, datetime):
            dt = value

        if dt:
            if dt.tzinfo is None:
                return dt.replace(tzinfo=timezone.utc)
            return dt.astimezone(timezone.utc)
        return None

    @field_validator("balance", mode="before")
    @classmethod
    def ensure_float_balance(cls, value: Any) -> float:
        try:
            return float(value)
        except (ValueError, TypeError):
            return 0.0

    @field_validator("login_incentives", mode="before")
    @classmethod
    def ensure_int_login(cls, value: Any) -> int:
        try:
            return int(value)
        except (ValueError, TypeError):
            return 0

    # --- Convenience Properties (Accessing nested data) ---
    @property
    def username(self) -> str | None:
        return self.auth.username

    @property
    def display_name(self) -> str:
        return self.profile.name  # Defaulted to "" if None

    @property
    def level(self) -> int:
        return self.stats.lvl

    @property
    def klass(self) -> str | None:
        return self.stats.klass

    @property
    def hp(self) -> float:
        return self.stats.hp

    @property
    def mp(self) -> float:
        return self.stats.mp

    @property
    def gp(self) -> float:
        return self.stats.gp

    @property
    def exp(self) -> float:
        return self.stats.exp

    @property
    def party_id(self) -> str | None:
        return self.party.party_id

    @property
    def is_sleeping(self) -> bool:
        return self.preferences.sleep

    @property
    def exp_to_next_level(self) -> int:
        return self.stats.exp_to_next_level

    @property
    def gems(self) -> int:
        """Calculated gem count from balance."""
        return int(self.balance * 4) if self.balance > 0 else 0

    # --- Methods for Calculated Values (Requiring Multiple Subcomponents) ---

    def calculate_effective_stats(
        self, gear_content_lookup: dict[str, dict[str, Any]] | None = None
    ) -> dict[str, float]:
        """Calculates total effective stats including gear bonuses.

        Args:
            gear_content_lookup: Optional dictionary mapping gear keys to stats.
                                 If None, uses mock ALL_GEAR_CONTENT.

        Returns:
            Dictionary with effective 'str', 'con', 'int', 'per' values.
        """
        lookup = (
            gear_content_lookup
            if gear_content_lookup is not None
            else ALL_GEAR_CONTENT
        )
        # Pass the necessary sub-models to the calculation helper
        return UserStatsCalculations.calculate_effective_stats(
            self.stats, self.items, lookup
        )

    @property
    def max_hp(self) -> float:
        """Calculated maximum HP including gear and all bonuses."""
        # Pass self.stats and the calculated effective CON
        effective_con = self.calculate_effective_stats().get("con", 0.0)
        return UserStatsCalculations.calculate_max_hp(self.stats, effective_con)

    @property
    def max_mp(self) -> float:
        """Calculated maximum MP including gear and all bonuses."""
        # Pass self.stats and the calculated effective INT
        effective_int = self.calculate_effective_stats().get("int", 0.0)
        return UserStatsCalculations.calculate_max_mp(self.stats, effective_int)


# SECTION: EXAMPLE USAGE
if __name__ == "__main__":
    USER_DATA_JSON_PATH = "user.json"  # Example file path
    raw_user_data = None
    user: User | None = None

    try:
        print("--- User Model Example ---")
        print(f"Loading user data from: {USER_DATA_JSON_PATH}...")
        # Load raw data from file
        with open(USER_DATA_JSON_PATH, encoding="utf-8") as f:
            raw_user_data = json.load(f)
        print("...JSON data loaded successfully.")

        print("Validating loaded data against User model...")
        # Validate and create User instance
        user = User.model_validate(
            raw_user_data
        )  # Use model_validate for Pydantic v2+
        print("...User data validated successfully.")

        # Access data using properties and methods
        print("\n--- Accessing data using the model ---")
        print(f"User ID: {user.id}")
        print(f"Username: {user.username}")
        print(f"Display Name: {user.display_name}")
        print(f"Level: {user.level}")
        print(f"Class: {user.klass}")
        print(f"Is Sleeping: {user.is_sleeping}")
        print(
            f"HP: {user.hp:.1f} / {user.max_hp:.1f}"
        )  # Access calculated max HP
        print(
            f"MP: {user.mp:.1f} / {user.max_mp:.1f}"
        )  # Access calculated max MP
        print(f"GP: {user.gp:.2f}")
        print(f"Gems: {user.gems}")
        print(f"EXP: {user.exp:.0f} / {user.exp_to_next_level}")
        print(f"Party ID: {user.party_id}")
        print(f"Created At: {user.auth.created_at}")  # Access via auth property

        print("\n--- Buffs & Training ---")
        print(f"Buffs: {user.stats.buffs}")  # Access nested model
        print(f"Training: {user.stats.training}")

        print("\n--- Equipped Gear ---")
        equipped_keys = user.items.gear_equipped.get_equipped_item_keys()
        print(f"Equipped Keys: {equipped_keys}")
        # Calculate gear bonus using mock content
        gear_bonus = user.items.gear_equipped.calculate_total_bonus(
            user.klass, ALL_GEAR_CONTENT
        )
        print(f"Calculated Gear Bonus: {gear_bonus}")

        print("\n--- Effective Stats (including gear) ---")
        # Calculate effective stats using mock content
        effective_stats = user.calculate_effective_stats(ALL_GEAR_CONTENT)
        print(f"Effective Stats: {effective_stats}")

        print("\n--- Sample Inventory ---")
        print(
            f"Eggs: {dict(list(user.items.eggs.items())[:3])}..."
        )  # Show first 3 eggs
        print(
            f"Pets: {dict(list(user.items.pets.items())[:3])}..."
        )  # Show first 3 pets

        print("\n--- Achievements ---")
        print(f"Perfect Days: {user.achievements.perfect_days}")
        print(f"Max Streak: {user.achievements.streak}")
        print(f"Completed Quests Count: {len(user.achievements.quests)}")

    except FileNotFoundError:
        print(
            f"\nFATAL ERROR: User data file not found at '{USER_DATA_JSON_PATH}'"
        )
    except json.JSONDecodeError as e:
        print(
            f"\nFATAL ERROR: Could not decode JSON from '{USER_DATA_JSON_PATH}'. Details: {e}"
        )
    except ValidationError as e:
        print(
            f"\nVALIDATION ERROR: Loaded data failed validation against the User model.\n{e}"
        )
    except Exception as e:
        logger.exception(
            "An unexpected error occurred during User model example execution."
        )
        print(f"\nUNEXPECTED ERROR: {e}")

-------- END OF FILE pixabit/models/user_gemini.py --------

-------- START OF FILE pixabit/models/__init__.py --------
# # pixabit/models/__init__.py

# """Pixabit Data Models Package.

# Exports Pydantic models representing core Habitica entities.
# """

# from .challenge import Challenge, ChallengeGroup, ChallengeLeader, ChallengeList
# from .game_content import (  # Assuming game_content.py is here now
#     GameContentCache,
#     Gear,
#     Mount,
#     Pet,
#     ProcessedGameContent,
#     Quest,
#     Spell,
# )
# from .message import Message, MessageList, MessageSenderStyles
# from .party import (  # Added PartyMember
#     Party,
#     PartyMember,
#     QuestInfo,
#     QuestProgress,
# )
# from .tag import (  # Use refined Tag models
#     Tag,
#     TagList,
# )
# from .task import (
#     ChallengeLinkData,  # Renamed from ChallengeData in task.py
#     ChecklistItem,
#     Daily,
#     Habit,
#     Reward,
#     Task,
#     TaskList,
#     Todo,
# )
# from .user import (
#     Buffs,
#     EquippedGear,
#     Training,
#     User,
#     UserAchievements,
#     UserAuth,
#     UserInboxInfo,
#     UserItems,
#     UserPartyInfo,
#     UserPreferences,
#     UserProfile,
#     UserStats,
# )

# __all__ = [
#     # Challenge Models
#     "Challenge",
#     "ChallengeGroup",
#     "ChallengeLeader",
#     "ChallengeList",
#     # Game Content Models & Cache Manager
#     "ProcessedGameContent",
#     "GameContentCache",
#     "Spell",
#     "Quest",
#     "QuestBoss",  # Export sub-models if needed directly
#     "QuestDrop",
#     "QuestCollect",
#     "Gear",
#     "GearStats",
#     "GearEvent",
#     "Pet",
#     "Mount",
#     # Message Models
#     "Message",
#     "MessageList",
#     "MessageSenderStyles",
#     # Party Models
#     "Party",
#     "PartyMember",
#     "QuestInfo",
#     "QuestProgress",
#     # Tag Models
#     "BaseTag",
#     "UserTag",
#     "ChallengeTag",
#     "Tag",  # Type Alias
#     "TagList",
#     # Task Models
#     "Task",
#     "Habit",
#     "Daily",
#     "Todo",
#     "Reward",
#     "TaskList",
#     "ChecklistItem",
#     "ChallengeLinkData",  # Renamed
#     # User Models
#     "User",
#     "UserProfile",
#     "UserAuth",
#     "UserAuthLocal",  # Export sub-models if needed
#     "UserAuthTimestamps",
#     "UserPreferences",
#     "UserItems",
#     "EquippedGear",
#     "UserAchievements",
#     "UserPartyInfo",
#     "UserInboxInfo",
#     "UserStats",
#     "Buffs",
#     "Training",
# ]

-------- END OF FILE pixabit/models/__init__.py --------

----------START OF FILE HELPER DateTimeHandler.py -------------
from datetime import datetime, timedelta, timezone
from typing import Any, Literal  # Eliminados Optional y Union

import dateutil.parser
from dateutil.tz import tzlocal
from pydantic import BaseModel, ConfigDict, Field, field_validator, model_validator


class DateTimeHandler(BaseModel):
    """Handles date/time operations with consistent timezone handling and formatting.

    Supports multiple timestamp formats including:
    - ISO 8601 strings (e.g. '2017-01-12T19:03:33.485Z')
    - Unix timestamps in milliseconds (e.g. 1484257319183)
    - Unix timestamps in seconds (e.g. 1484257319)
    - datetime objects
    """

    model_config = ConfigDict(arbitrary_types_allowed=True)  # Reemplaza class Config

    timestamp: str | datetime | int | float | None = None  # Union como |
    utc_datetime: datetime | None = None
    local_datetime: datetime | None = None
    local_timezone: timezone = Field(default_factory=tzlocal)

    @field_validator("utc_datetime", mode="before")  # Reemplaza validator
    @classmethod
    def parse_timestamp(cls, v: Any, info) -> datetime | None:
        """Parse timestamp into UTC datetime if not already set."""
        if v is not None:
            return v

        # Necesitamos obtener timestamp del diccionario de datos
        values = info.data
        timestamp = values.get("timestamp")
        if timestamp is None:
            return None

        try:
            if isinstance(timestamp, (int, float)):
                # Check if timestamp is in milliseconds (13 digits typically)
                if timestamp > 1_000_000_000_000:
                    timestamp = timestamp / 1000  # Convert ms to seconds
                return datetime.fromtimestamp(timestamp, tz=timezone.utc)
            elif isinstance(timestamp, str):
                dt = dateutil.parser.isoparse(timestamp)
                if dt.tzinfo is None:
                    dt = dt.replace(tzinfo=timezone.utc)
                return dt.astimezone(timezone.utc)
            elif isinstance(timestamp, datetime):
                if timestamp.tzinfo is None:
                    return timestamp.replace(tzinfo=timezone.utc)
                return timestamp.astimezone(timezone.utc)
        except Exception as e:
            raise ValueError(f"Invalid timestamp: {e}")

        return None

    @field_validator("local_datetime", mode="before")  # Reemplaza validator
    @classmethod
    def convert_to_local(cls, v: Any, info) -> datetime | None:
        """Convert UTC datetime to local timezone."""
        if v is not None:
            return v

        values = info.data
        utc_dt = values.get("utc_datetime")
        if utc_dt is None:
            return None

        local_tz = values.get("local_timezone", tzlocal())
        return utc_dt.astimezone(local_tz).replace(microsecond=0)

    @classmethod
    def from_iso(cls, iso_timestamp: str):
        """Create from ISO 8601 timestamp."""
        return cls(timestamp=iso_timestamp)

    @classmethod
    def from_unix_ms(cls, unix_ms: int):
        """Create from Unix timestamp in milliseconds."""
        return cls(timestamp=unix_ms)

    @classmethod
    def from_unix_seconds(cls, unix_seconds: float):
        """Create from Unix timestamp in seconds."""
        return cls(timestamp=unix_seconds)

    @classmethod
    def now(cls):
        """Create with current time."""
        return cls(timestamp=datetime.now(timezone.utc))

    def is_past(self) -> bool | None:
        """Check if datetime is in the past."""
        if self.utc_datetime is None:
            return None
        return self.utc_datetime < datetime.now(timezone.utc)

    def format_time_difference(self) -> str:
        """Format time difference between datetime and now."""
        if self.local_datetime is None:
            return "N/A"

        now_local = datetime.now(self.local_timezone).replace(microsecond=0)
        delta = self.local_datetime - now_local

        return self._format_timedelta(delta)

    def _format_timedelta(self, delta: timedelta) -> str:
        """Format a timedelta into a human-readable string."""
        total_seconds_float = delta.total_seconds()

        if abs(total_seconds_float) < 1:
            return "now"

        is_past = total_seconds_float < 0
        abs_delta = abs(delta)
        total_abs_seconds = int(abs_delta.total_seconds())

        days = total_abs_seconds // (24 * 3600)
        seconds_within_day = total_abs_seconds % (24 * 3600)

        hours, remainder = divmod(seconds_within_day, 3600)
        minutes, seconds = divmod(remainder, 60)

        parts = []
        if days > 0:
            parts.append(f"{days}d")

        parts.append(f"{hours:02}:{minutes:02}:{seconds:02}")

        time_str = " ".join(parts)

        if is_past:
            return f"{time_str} ago"
        else:
            return f"in {time_str}"

    def format_local(self, format_str: str = "%Y-%m-%d %H:%M:%S") -> str:
        """Format local time with specified format string."""
        if self.local_datetime is None:
            return "N/A"
        return self.local_datetime.strftime(format_str)

    def format_utc(self, format_str: str = "%Y-%m-%d %H:%M:%S") -> str:
        """Format UTC time with specified format string."""
        if self.utc_datetime is None:
            return "N/A"
        return self.utc_datetime.strftime(format_str)

    def format_with_diff(self) -> str:
        """Format date with time difference."""
        if self.local_datetime is None:
            return "N/A"
        diff = self.format_time_difference()
        return f"{self.format_local()} ({diff})"

    def to_iso(self) -> str | None:
        """Convert to ISO 8601 format."""
        if self.utc_datetime is None:
            return None
        return self.utc_datetime.isoformat()

    def to_unix_ms(self) -> int | None:
        """Convert to Unix timestamp in milliseconds."""
        if self.utc_datetime is None:
            return None
        return int(self.utc_datetime.timestamp() * 1000)

    def to_unix_seconds(self) -> float | None:
        """Convert to Unix timestamp in seconds."""
        if self.utc_datetime is None:
            return None
        return self.utc_datetime.timestamp()

------------ END OF FILE DATETIMEHANDLER.PY -----------
------------ START OF HELPER FILE LOGGER.PY ------------

"""Log helper for Textual."""

import logging
import sys
from enum import Enum
from logging import Logger
from logging.handlers import RotatingFileHandler
from pathlib import Path

from ._rich import RichHandler, console

# --- Constants ---
LOG_FILENAME = "app.log"
LOG_FORMAT_FILE = (
    "%(asctime)s - %(filename)s:%(lineno)d - %(levelname)s - %(message)s"
)
SUCCESS_LEVEL_NUM = 25

# --- Custom Log Level ---
logging.addLevelName(SUCCESS_LEVEL_NUM, "SUCCESS")


def success(self, message, *args, **kws):
    if self.isEnabledFor(SUCCESS_LEVEL_NUM):
        self._log(SUCCESS_LEVEL_NUM, message, args, **kws)


# Add the 'success' method to the Logger class
Logger.success = success


def setup_logging(
    log_level: int = logging.DEBUG,
    logger_name: str = "Pixabit",
    log_dir: Path = None,
) -> logging.Logger:
    """Configures logging with file and rich console handlers."""
    # Create log directory if needed
    if log_dir:
        log_dir.mkdir(parents=True, exist_ok=True)
        log_path = log_dir / LOG_FILENAME
    else:
        log_path = LOG_FILENAME

    # Get the logger
    log = logging.getLogger(logger_name)
    log.setLevel(log_level)

    # Clear existing handlers to prevent duplicates
    if log.hasHandlers():
        log.handlers.clear()

    # Create Rich handler for standard output
    rich_handler = RichHandler(
        console=console,
        rich_tracebacks=True,
        markup=True,
        show_time=False,
        show_path=False,
        enable_link_path=True,
    )
    rich_handler.setLevel(logging.INFO)
    log.addHandler(rich_handler)

    # Create error handler for warnings and above
    error_handler = RichHandler(
        console=console,
        rich_tracebacks=True,
        markup=True,
        show_time=False,
        show_path=False,
        level=logging.WARNING,
    )
    log.addHandler(error_handler)

    # Create file handler
    file_handler = RotatingFileHandler(
        log_path, maxBytes=10 * 1024 * 1024, backupCount=5, encoding="utf-8"
    )
    file_handler.setLevel(logging.DEBUG)
    file_formatter = logging.Formatter(LOG_FORMAT_FILE)
    file_handler.setFormatter(file_formatter)
    log.addHandler(file_handler)

    # Add custom level names to Rich formats
    logging._levelToName[SUCCESS_LEVEL_NUM] = "SUCCESS"

    # Forward Textual logs to our handler
    try:
        textual_logger = logging.getLogger("textual")
        textual_logger.setLevel(log_level)
        textual_logger.handlers.clear()
        textual_logger.addHandler(rich_handler)
    except Exception:
        pass

    return log


# Create logger instance
log = setup_logging(log_level=logging.DEBUG, logger_name="Pixabit")
# --- Singleton Access ---
_log_instance: Logger | None = None


def get_logger() -> Logger:
    global _log_instance
    if _log_instance is None:
        _log_instance = setup_logging(
            log_level=logging.DEBUG, logger_name="Pixabit"
        )
    return _log_instance


# Example usage in a Textual app:
# from textual.app import App
#
# class MyApp(App):
#     def on_mount(self):
#         log.info("Application started")
#         log.success("Setup complete!")
#
#     def on_error(self, event):
#         log.error(f"Error occurred: {event.error}")



------------------- END OF FILE LOGGER --------


----------START OF HELPER FILE JSON.PY ---------
# pixabit/helpers/_json.py

# SECTION: MODULE DOCSTRING
"""Provides utility functions for saving and loading Python data to/from JSON files.

Includes pretty printing, UTF-8 encoding, directory creation, and error handling.
Uses the application's configured logger. Supports saving/loading Pydantic models.
"""

# SECTION: IMPORTS
import json
from pathlib import Path
from typing import Any, Type, TypeVar, cast

# Use | for Union
from pydantic import BaseModel

# Assume logger and pydantic helpers are one level up
from ._logger import log
from ._pydantic import (  # Assuming this provides PYDANTIC_V2 and validation methods
    PYDANTIC_V2,
    ValidationError,
)

# SECTION: TYPE VARIABLES
T_BaseModel = TypeVar("T_BaseModel", bound=BaseModel)
JSONSerializable = dict[str, Any] | list[Any]
LoadResult = JSONSerializable | None


# SECTION: FUNCTIONS


# FUNC: resolve_path
def _resolve_path(
    filepath: str | Path, folder: str | Path | None = None
) -> Path:
    """Helper function to resolve the final file path."""
    if folder is not None:
        folder_path = Path(folder).resolve()
        # Extract just the filename from filepath if folder is given
        filename = Path(filepath).name
        return folder_path / filename
    else:
        return Path(filepath).resolve()


# FUNC: save_json
def save_json(
    data: JSONSerializable,
    filepath: str | Path,
    folder: str | Path | None = None,
    indent: int = 4,
    ensure_ascii: bool = False,
) -> bool:
    """Saves Python data (dict or list) to a JSON file with pretty printing.

    Ensures the output directory exists. Handles potential JSON serialization
    errors and file I/O errors, logging messages.

    Args:
        data: The Python dictionary or list to save.
        filepath: The full path (including filename and .json extension) for
                  the output file, or just the filename if folder is specified.
        folder: Optional folder path where the file should be saved.
                If provided, filepath will be treated as just the filename.
        indent: JSON indentation level.
        ensure_ascii: If True, escape non-ASCII characters.

    Returns:
        True if saving was successful, False otherwise.
    """
    output_path = _resolve_path(filepath, folder)
    log.debug(f"Attempting to save JSON to: '{output_path}'")

    try:
        # Create parent directory(ies) if they don't exist
        output_path.parent.mkdir(parents=True, exist_ok=True)

        # Write the file with UTF-8 encoding and specified indentation
        with output_path.open("w", encoding="utf-8") as f:
            json.dump(data, f, indent=indent, ensure_ascii=ensure_ascii)

        log.info(f"Successfully saved JSON data to: '{output_path}'")
        return True

    except TypeError as e:
        log.error(
            f"Data structure not JSON serializable for '{output_path}'. Error: {e}"
        )
        return False
    except OSError as e:
        log.error(f"Could not write file '{output_path}'. Error: {e}")
        return False
    except Exception as e:
        log.exception(  # Log full traceback for unexpected errors
            f"An unexpected error occurred saving to '{output_path}': {e}"
        )
        return False


# FUNC: load_json
def load_json(
    filepath: str | Path,
    folder: str | Path | None = None,
) -> LoadResult:
    """Loads data from a JSON file.

    Args:
        filepath: The path to the JSON file, or just the filename if folder is specified.
        folder: Optional folder path where the file is located.
                If provided, filepath will be treated as just the filename.

    Returns:
        The loaded Python dictionary or list, or None if the file doesn't exist,
        cannot be read, or contains invalid JSON.
    """
    input_path = _resolve_path(filepath, folder)

    if not input_path.is_file():
        log.debug(f"JSON file not found: '{input_path}'")
        return None

    log.debug(f"Attempting to load JSON from: '{input_path}'")
    try:
        with input_path.open("r", encoding="utf-8") as f:
            data = json.load(f)

        # Basic validation of loaded data type
        if isinstance(data, (dict, list)):
            log.debug(f"Successfully loaded JSON data from: '{input_path}'")
            return data
        else:
            log.warning(
                f"Invalid data type ({type(data).__name__}) in JSON file: '{input_path}'. Expected dict or list."
            )
            return None

    except (OSError, json.JSONDecodeError) as e:
        log.warning(
            f"Failed to load or parse JSON file '{input_path}'. Error: {e}"
        )
        return None
    except Exception as e:
        log.exception(  # Log full traceback for unexpected errors
            f"An unexpected error occurred loading '{input_path}': {e}"
        )
        return None


# FUNC: save_pydantic_model
def save_pydantic_model(
    model: BaseModel,
    filepath: str | Path,
    folder: str | Path | None = None,
    exclude_none: bool = True,
    indent: int = 4,
) -> bool:
    """Saves a Pydantic model to a JSON file.

    Args:
        model: The Pydantic model instance to save.
        filepath: The path where the JSON file will be saved, or just the filename
                  if folder is specified.
        folder: Optional folder path where the file should be saved.
        exclude_none: Whether to exclude fields with None values from the output.
        indent: JSON indentation level.

    Returns:
        True if saving was successful, False otherwise.
    """
    if not isinstance(model, BaseModel):
        log.error(
            "Invalid input: 'model' must be a Pydantic BaseModel instance."
        )
        return False
    try:
        # Use model_dump for Pydantic V2+, dict for V1
        if PYDANTIC_V2:
            # Use model_dump() for V2
            data = model.model_dump(mode="json", exclude_none=exclude_none)
        else:
            # Use dict() for V1
            data = model.dict(exclude_none=exclude_none)  # type: ignore[attr-defined]

        # Ensure data is dict or list (should be dict from model)
        if not isinstance(data, (dict, list)):
            log.error(
                f"Pydantic model dump resulted in unexpected type: {type(data).__name__}"
            )
            return False

        return save_json(
            cast(JSONSerializable, data), filepath, folder=folder, indent=indent
        )

    except Exception as e:
        log.exception(f"Failed to convert Pydantic model to JSON or save: {e}")
        return False


# FUNC: load_pydantic_model
def load_pydantic_model(
    model_class: Type[T_BaseModel],
    filepath: str | Path,
    folder: str | Path | None = None,
) -> T_BaseModel | None:
    """Loads a JSON file into a Pydantic model instance.

    Args:
        model_class: The Pydantic model class (e.g., User, Task).
        filepath: The path to the JSON file, or just the filename if folder is specified.
        folder: Optional folder path where the file is located.

    Returns:
        An instance of the model_class populated with data if successful, None otherwise.
    """
    data = load_json(filepath, folder=folder)
    if data is None:
        # load_json already logged the reason (not found or parse error)
        return None

    if not isinstance(data, dict):
        input_path = _resolve_path(filepath, folder)
        log.warning(
            f"JSON data loaded from '{input_path}' is not a dictionary, cannot parse into {model_class.__name__}."
        )
        return None

    try:
        # Use model_validate for Pydantic V2+, parse_obj for V1
        if PYDANTIC_V2:
            instance = model_class.model_validate(data)
        else:
            instance = model_class.parse_obj(data)  # type: ignore[attr-defined]

        log.debug(
            f"Successfully parsed JSON into Pydantic model {model_class.__name__}"
        )
        return instance

    except ValidationError as e:
        input_path = _resolve_path(filepath, folder)
        log.error(
            f"Failed Pydantic validation for {model_class.__name__} from '{input_path}':\n{e}"
        )
        return None
    except Exception as e:
        input_path = _resolve_path(filepath, folder)
        log.exception(
            f"An unexpected error occurred parsing JSON into Pydantic model {model_class.__name__} from '{input_path}': {e}"
        )
        return None

----------- END OF FILE JSON --------------
------------ START OF FILE CONFIG------------------
# pixabit/config.py

from pathlib import Path

HABITICA_DATA_PATH: Path = Path("./habitica_cache")
DEFAULT_CACHE_DURATION_DAYS = 7
HABITICA_DATA_PATH.mkdir(parents=True, exist_ok=True)
USER_ID = "USERID"

-------------END OF FILE CONFIG -----------

HABITICA API FILE ALREADY HAVE ACCESS TO THE AUTH CREDENTIALS

# KLASS: HabiticaConfig
class HabiticaConfig(BaseSettings):
    """Pydantic settings model for Habitica API configuration."""

    habitica_user_id: str = Field(..., description="Habitica User ID")
    habitica_api_token: SecretStr = Field(..., description="Habitica API Token")
    habitica_base_url: str = Field(DEFAULT_BASE_URL, description="Habitica API Base URL")

    model_config = SettingsConfigDict(
        env_file=".env",
        env_file_encoding="utf-8",
        case_sensitive=False,
        extra="ignore",
    )
